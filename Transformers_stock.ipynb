{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aplicação de LSTM e Transformers para ações\n",
        "\n",
        "Carlos Monteiro\n"
      ],
      "metadata": {
        "id": "CohKnq3oREfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from math import pi\n",
        "\n",
        "\n",
        "\n",
        "from __future__ import annotations\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n"
      ],
      "metadata": {
        "id": "hRTloyRMIxoP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preparação dos dados"
      ],
      "metadata": {
        "id": "xkqIuI-4RHw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Gerar Dados"
      ],
      "metadata": {
        "id": "vkn72a5PTLK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ticker = \"PETR4.SA\"\n",
        "end_date = datetime.today()\n",
        "start_date = end_date - timedelta(days=20*365)\n",
        "\n",
        "\n",
        "df = yf.download(\n",
        "    ticker,\n",
        "    start=start_date.strftime('%Y-%m-%d'),\n",
        "    end=end_date.strftime('%Y-%m-%d'),\n",
        "    interval=\"1d\",\n",
        "    auto_adjust=False,   # Mantém OHLC originais\n",
        "    progress=False\n",
        ")\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "KHDD7Owzn4XA",
        "outputId": "caf400ad-e46a-4a17-92fd-93a943196ea2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Price       Adj Close      Close       High        Low       Open    Volume\n",
              "Ticker       PETR4.SA   PETR4.SA   PETR4.SA   PETR4.SA   PETR4.SA  PETR4.SA\n",
              "Date                                                                       \n",
              "2006-01-09   4.027726  20.420000  20.650000  20.180000  20.344999   8782200\n",
              "2006-01-10   4.034630  20.455000  20.625000  20.225000  20.455000  13541600\n",
              "2006-01-11   4.171713  21.150000  21.225000  20.605000  20.625000  15117800\n",
              "2006-01-12   4.206234  21.325001  21.795000  20.950001  21.145000  16237800\n",
              "2006-01-13   4.201303  21.299999  21.620001  21.080000  21.424999   7932600\n",
              "...               ...        ...        ...        ...        ...       ...\n",
              "2025-12-23  30.309999  30.309999  30.570000  30.200001  30.549999  35703900\n",
              "2025-12-26  30.410000  30.410000  30.430000  30.120001  30.309999  20178600\n",
              "2025-12-29  30.730000  30.730000  30.809999  30.520000  30.559999  20588100\n",
              "2025-12-30  30.820000  30.820000  30.959999  30.650000  30.799999  16880600\n",
              "2026-01-02  30.709999  30.709999  30.959999  30.360001  30.959999  21625200\n",
              "\n",
              "[4970 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-53d70241-640a-4738-b0a8-3660b7181ab5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>Price</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ticker</th>\n",
              "      <th>PETR4.SA</th>\n",
              "      <th>PETR4.SA</th>\n",
              "      <th>PETR4.SA</th>\n",
              "      <th>PETR4.SA</th>\n",
              "      <th>PETR4.SA</th>\n",
              "      <th>PETR4.SA</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2006-01-09</th>\n",
              "      <td>4.027726</td>\n",
              "      <td>20.420000</td>\n",
              "      <td>20.650000</td>\n",
              "      <td>20.180000</td>\n",
              "      <td>20.344999</td>\n",
              "      <td>8782200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-01-10</th>\n",
              "      <td>4.034630</td>\n",
              "      <td>20.455000</td>\n",
              "      <td>20.625000</td>\n",
              "      <td>20.225000</td>\n",
              "      <td>20.455000</td>\n",
              "      <td>13541600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-01-11</th>\n",
              "      <td>4.171713</td>\n",
              "      <td>21.150000</td>\n",
              "      <td>21.225000</td>\n",
              "      <td>20.605000</td>\n",
              "      <td>20.625000</td>\n",
              "      <td>15117800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-01-12</th>\n",
              "      <td>4.206234</td>\n",
              "      <td>21.325001</td>\n",
              "      <td>21.795000</td>\n",
              "      <td>20.950001</td>\n",
              "      <td>21.145000</td>\n",
              "      <td>16237800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-01-13</th>\n",
              "      <td>4.201303</td>\n",
              "      <td>21.299999</td>\n",
              "      <td>21.620001</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>21.424999</td>\n",
              "      <td>7932600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-12-23</th>\n",
              "      <td>30.309999</td>\n",
              "      <td>30.309999</td>\n",
              "      <td>30.570000</td>\n",
              "      <td>30.200001</td>\n",
              "      <td>30.549999</td>\n",
              "      <td>35703900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-12-26</th>\n",
              "      <td>30.410000</td>\n",
              "      <td>30.410000</td>\n",
              "      <td>30.430000</td>\n",
              "      <td>30.120001</td>\n",
              "      <td>30.309999</td>\n",
              "      <td>20178600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-12-29</th>\n",
              "      <td>30.730000</td>\n",
              "      <td>30.730000</td>\n",
              "      <td>30.809999</td>\n",
              "      <td>30.520000</td>\n",
              "      <td>30.559999</td>\n",
              "      <td>20588100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-12-30</th>\n",
              "      <td>30.820000</td>\n",
              "      <td>30.820000</td>\n",
              "      <td>30.959999</td>\n",
              "      <td>30.650000</td>\n",
              "      <td>30.799999</td>\n",
              "      <td>16880600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2026-01-02</th>\n",
              "      <td>30.709999</td>\n",
              "      <td>30.709999</td>\n",
              "      <td>30.959999</td>\n",
              "      <td>30.360001</td>\n",
              "      <td>30.959999</td>\n",
              "      <td>21625200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4970 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53d70241-640a-4738-b0a8-3660b7181ab5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-53d70241-640a-4738-b0a8-3660b7181ab5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-53d70241-640a-4738-b0a8-3660b7181ab5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6b9c9e69-5e7e-46a9-be4a-15987b61c20c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6b9c9e69-5e7e-46a9-be4a-15987b61c20c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6b9c9e69-5e7e-46a9-be4a-15987b61c20c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_a993de59-aab3-4d10-93d5-0d75dcf2dcb7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a993de59-aab3-4d10-93d5-0d75dcf2dcb7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4970,\n  \"fields\": [\n    {\n      \"column\": [\n        \"Date\",\n        \"\"\n      ],\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2006-01-09 00:00:00\",\n        \"max\": \"2026-01-02 00:00:00\",\n        \"num_unique_values\": 4970,\n        \"samples\": [\n          \"2009-06-12 00:00:00\",\n          \"2020-09-23 00:00:00\",\n          \"2024-07-15 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Adj Close\",\n        \"PETR4.SA\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.201296698151959,\n        \"min\": 1.1193222999572754,\n        \"max\": 34.5811882019043,\n        \"num_unique_values\": 4224,\n        \"samples\": [\n          30.772764205932617,\n          3.208723783493042,\n          4.333376407623291\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Close\",\n        \"PETR4.SA\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.529842899066324,\n        \"min\": 4.199999809265137,\n        \"max\": 52.5099983215332,\n        \"num_unique_values\": 2574,\n        \"samples\": [\n          18.149999618530273,\n          6.789999961853027,\n          39.77000045776367\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"High\",\n        \"PETR4.SA\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.609163667338967,\n        \"min\": 4.269999980926514,\n        \"max\": 53.68000030517578,\n        \"num_unique_values\": 2628,\n        \"samples\": [\n          21.25,\n          27.30500030517578,\n          25.90999984741211\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Low\",\n        \"PETR4.SA\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.443331482023357,\n        \"min\": 4.119999885559082,\n        \"max\": 51.95000076293945,\n        \"num_unique_values\": 2608,\n        \"samples\": [\n          10.109999656677246,\n          14.600000381469727,\n          23.434999465942383\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Open\",\n        \"PETR4.SA\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.529982926254936,\n        \"min\": 4.199999809265137,\n        \"max\": 52.58000183105469,\n        \"num_unique_values\": 2504,\n        \"samples\": [\n          15.869999885559082,\n          25.4950008392334,\n          7.019999980926514\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Volume\",\n        \"PETR4.SA\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 72402963,\n        \"min\": 0,\n        \"max\": 1336049152,\n        \"num_unique_values\": 4871,\n        \"samples\": [\n          15777400,\n          12667200,\n          136878100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e04dc88",
        "outputId": "e18a4053-7fe7-4746-941e-e9036aa4aab5"
      },
      "source": [
        "df.columns = [col[0] for col in df.columns]\n",
        "df.columns"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['Adj Close'])"
      ],
      "metadata": {
        "id": "3doC_NhCyWWe"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "f8d3d78a",
        "outputId": "58a97d4c-3bab-48ed-860d-16d7d77d773b"
      },
      "source": [
        "df['y'] = (df['Close'].diff(periods=-1))*-1\n",
        "display(df.head())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                Close       High        Low       Open    Volume         y\n",
              "Date                                                                      \n",
              "2006-01-09  20.420000  20.650000  20.180000  20.344999   8782200  0.035000\n",
              "2006-01-10  20.455000  20.625000  20.225000  20.455000  13541600  0.695000\n",
              "2006-01-11  21.150000  21.225000  20.605000  20.625000  15117800  0.175001\n",
              "2006-01-12  21.325001  21.795000  20.950001  21.145000  16237800 -0.025002\n",
              "2006-01-13  21.299999  21.620001  21.080000  21.424999   7932600  0.425001"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65121ca8-5cd0-4c5c-bd47-3b502949e348\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2006-01-09</th>\n",
              "      <td>20.420000</td>\n",
              "      <td>20.650000</td>\n",
              "      <td>20.180000</td>\n",
              "      <td>20.344999</td>\n",
              "      <td>8782200</td>\n",
              "      <td>0.035000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-01-10</th>\n",
              "      <td>20.455000</td>\n",
              "      <td>20.625000</td>\n",
              "      <td>20.225000</td>\n",
              "      <td>20.455000</td>\n",
              "      <td>13541600</td>\n",
              "      <td>0.695000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-01-11</th>\n",
              "      <td>21.150000</td>\n",
              "      <td>21.225000</td>\n",
              "      <td>20.605000</td>\n",
              "      <td>20.625000</td>\n",
              "      <td>15117800</td>\n",
              "      <td>0.175001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-01-12</th>\n",
              "      <td>21.325001</td>\n",
              "      <td>21.795000</td>\n",
              "      <td>20.950001</td>\n",
              "      <td>21.145000</td>\n",
              "      <td>16237800</td>\n",
              "      <td>-0.025002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-01-13</th>\n",
              "      <td>21.299999</td>\n",
              "      <td>21.620001</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>21.424999</td>\n",
              "      <td>7932600</td>\n",
              "      <td>0.425001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65121ca8-5cd0-4c5c-bd47-3b502949e348')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-65121ca8-5cd0-4c5c-bd47-3b502949e348 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-65121ca8-5cd0-4c5c-bd47-3b502949e348');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ec881067-69ec-4627-bf1e-f1552374fe65\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec881067-69ec-4627-bf1e-f1552374fe65')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ec881067-69ec-4627-bf1e-f1552374fe65 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2006-01-09 00:00:00\",\n        \"max\": \"2006-01-13 00:00:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2006-01-10 00:00:00\",\n          \"2006-01-13 00:00:00\",\n          \"2006-01-11 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.45471140984172836,\n        \"min\": 20.420000076293945,\n        \"max\": 21.325000762939453,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          20.454999923706055,\n          21.299999237060547,\n          21.149999618530273\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5391500870733488,\n        \"min\": 20.625,\n        \"max\": 21.795000076293945,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          20.625,\n          21.6200008392334,\n          21.225000381469727\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.40915458543815464,\n        \"min\": 20.18000030517578,\n        \"max\": 21.079999923706055,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          20.225000381469727,\n          21.079999923706055,\n          20.604999542236328\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.46538157396314633,\n        \"min\": 20.344999313354492,\n        \"max\": 21.424999237060547,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          20.454999923706055,\n          21.424999237060547,\n          20.625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3756142,\n        \"min\": 7932600,\n        \"max\": 16237800,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          13541600,\n          7932600,\n          15117800\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2981278481678457,\n        \"min\": -0.02500152587890625,\n        \"max\": 0.6949996948242188,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6949996948242188,\n          0.4250011444091797,\n          0.1750011444091797\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb1_ULUaPRCl",
        "outputId": "03a4071b-70db-477b-89f2-59523c54cbd4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Close     0\n",
            "High      0\n",
            "Low       0\n",
            "Open      0\n",
            "Volume    0\n",
            "y         1\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(df['y'])"
      ],
      "metadata": {
        "id": "ZKd21BUT1fhx",
        "outputId": "316d5ff3-3b8b-451d-8fa8-e938c0d82825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7c63f4a28560>]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['y'].max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPUhjbBW7EyV",
        "outputId": "eeb1ddd1-40e1-46c4-ca60-d84f99cd2ce0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.775001525878906"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BihJxRNe7K6Q",
        "outputId": "214c50e2-59b8-4672-a584-69c7bf4fcef2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X_mean': array([[[2.4504503e+01, 2.4866903e+01, 2.4157646e+01, 2.4524424e+01,\n",
              "          5.0806104e+07]]], dtype=float32),\n",
              " 'X_std': array([[[8.5293894e+00, 8.6087313e+00, 8.4428740e+00, 8.5294952e+00,\n",
              "          7.2401784e+07]]], dtype=float32),\n",
              " 'y_mean': array([[[0.00207084]]], dtype=float32),\n",
              " 'y_std': array([[[0.6326609]]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Scaling and Deframing"
      ],
      "metadata": {
        "id": "XUiy7qLHT4P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9dAyzOD7msu",
        "outputId": "e993398a-f4b6-4a0a-eb9f-825fa2ad7a75"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(9.124842)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df)\n",
        "df_scaled = pd.DataFrame(df_scaled, columns=df.columns)\n",
        "df_scaled.head()"
      ],
      "metadata": {
        "id": "3iviz6TC893w",
        "outputId": "7e88247b-cb9d-4cd9-f53e-e60c570f42b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Close      High       Low      Open    Volume         y\n",
              "0  0.335748  0.331512  0.335773  0.333712  0.006573  0.542812\n",
              "1  0.336473  0.331006  0.336713  0.335986  0.010136  0.595380\n",
              "2  0.350859  0.343149  0.344658  0.339500  0.011315  0.553963\n",
              "3  0.354482  0.354685  0.351871  0.350248  0.012154  0.538032\n",
              "4  0.353964  0.351144  0.354589  0.356036  0.005937  0.573875"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f1b99ac4-5e49-4ff0-b138-d9c5292cbe9f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.335748</td>\n",
              "      <td>0.331512</td>\n",
              "      <td>0.335773</td>\n",
              "      <td>0.333712</td>\n",
              "      <td>0.006573</td>\n",
              "      <td>0.542812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.336473</td>\n",
              "      <td>0.331006</td>\n",
              "      <td>0.336713</td>\n",
              "      <td>0.335986</td>\n",
              "      <td>0.010136</td>\n",
              "      <td>0.595380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.350859</td>\n",
              "      <td>0.343149</td>\n",
              "      <td>0.344658</td>\n",
              "      <td>0.339500</td>\n",
              "      <td>0.011315</td>\n",
              "      <td>0.553963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.354482</td>\n",
              "      <td>0.354685</td>\n",
              "      <td>0.351871</td>\n",
              "      <td>0.350248</td>\n",
              "      <td>0.012154</td>\n",
              "      <td>0.538032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.353964</td>\n",
              "      <td>0.351144</td>\n",
              "      <td>0.354589</td>\n",
              "      <td>0.356036</td>\n",
              "      <td>0.005937</td>\n",
              "      <td>0.573875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1b99ac4-5e49-4ff0-b138-d9c5292cbe9f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f1b99ac4-5e49-4ff0-b138-d9c5292cbe9f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f1b99ac4-5e49-4ff0-b138-d9c5292cbe9f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-045a8e77-f629-4ea9-acbb-a9b3bdd80365\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-045a8e77-f629-4ea9-acbb-a9b3bdd80365')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-045a8e77-f629-4ea9-acbb-a9b3bdd80365 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_scaled",
              "summary": "{\n  \"name\": \"df_scaled\",\n  \"rows\": 4969,\n  \"fields\": [\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1765731186479757,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2574,\n        \"samples\": [\n          0.2887600960228266,\n          0.05361209340402222,\n          0.7362865192278101\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17424804969089755,\n        \"min\": 0.0,\n        \"max\": 0.9999999999999999,\n        \"num_unique_values\": 2628,\n        \"samples\": [\n          0.3436551286711913,\n          0.4662011773544602,\n          0.4379680170911716\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1765361202355002,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2608,\n        \"samples\": [\n          0.12523520094583435,\n          0.21910935194790718,\n          0.4038260344151007\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17631981156431864,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2504,\n        \"samples\": [\n          0.2412153697521131,\n          0.44016122654102713,\n          0.058288550099507966\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05419640956607797,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4870,\n        \"samples\": [\n          0.033498992108937024,\n          0.009481088312535376,\n          0.021417849752880948\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05039613343624363,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1104,\n        \"samples\": [\n          0.6742334184132717,\n          0.5463956586291674,\n          0.626841852037475\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_features = [col for col in df.columns if not col.startswith('y')]\n",
        "n_features = len(columns_features)\n",
        "n_samples = len(df)\n",
        "\n",
        "X = df_scaled[columns_features].values.astype(np.float32).reshape(1, -1, n_features)\n",
        "\n",
        "y = df_scaled['y'].values.astype(np.float32).reshape(1, -1, 1)\n",
        "\n",
        "\n",
        "print(f\"Features shape: {X.shape}, Targets shape: {y.shape}\")\n",
        "plt.plot(y[0,:,0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "nZ1A02j5CH1I",
        "outputId": "e97b1606-2320-44d4-c916-0aad3f731881"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (1, 4969, 5), Targets shape: (1, 4969, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7c63a46e6840>]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Criar janela de entrada e de previsão em um dataset"
      ],
      "metadata": {
        "id": "GTCkTS03fEb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowedTS(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for sliding window forecasting.\n",
        "\n",
        "    CONCEPT: Long time series → many overlapping windows\n",
        "\n",
        "    Example with history=3, forecast=2:\n",
        "      Series: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "      Window 1: history=[0,1,2], forecast=[3,4]\n",
        "      Window 2: history=[1,2,3], forecast=[4,5]\n",
        "      Window 3: history=[2,3,4], forecast=[5,6]\n",
        "      ... and so on\n",
        "\n",
        "    This creates many training examples from a single long series!\n",
        "\n",
        "    Returns (for each window):\n",
        "        Xw: Historical features [history_len, n_features]\n",
        "        y_in: Decoder inputs (teacher forcing) [forecast_len, 1]\n",
        "              First token = 0 (start token)\n",
        "              Remaining tokens = shifted ground-truth targets\n",
        "        y_true: Ground-truth future targets [forecast_len, 1]\n",
        "\n",
        "    Args:\n",
        "        X: All features [n_series, series_length, n_features]\n",
        "        y: All targets [n_series, series_length, 1]\n",
        "        T_x: History length (how much past to use)\n",
        "        T_y: Forecast horizon (how far ahead to predict)\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, T_x: int, T_y: int):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.T_x = T_x  # History window size\n",
        "        self.T_y = T_y  # Forecast horizon\n",
        "        self.length = X.shape[1]\n",
        "\n",
        "        # BUILD INDEX: list of all valid (series_id, start_time) pairs\n",
        "        self.index = []\n",
        "        for i in range(X.shape[0]):  # For each series\n",
        "            # For each valid starting position\n",
        "            for t in range(self.length - (T_x + T_y) + 1):\n",
        "                self.index.append((i, t))\n",
        "\n",
        "        # Total windows = n_series * (series_length - T_x - T_y + 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Extract one training window.\n",
        "\n",
        "        Returns:\n",
        "            Xw: Historical window [T_x, n_features]\n",
        "            y_in: Decoder inputs [T_y, 1] (shifted targets with start token)\n",
        "            y_future: Ground-truth targets [T_y, 1]\n",
        "        \"\"\"\n",
        "        i, t = self.index[idx]  # Which series, which starting time\n",
        "\n",
        "        # EXTRACT HISTORY\n",
        "        Xw = self.X[i, t:t+self.T_x, :].copy()  # [T_x, n_features]\n",
        "\n",
        "        # EXTRACT FUTURE TARGETS\n",
        "        y_future = self.y[i, t+self.T_x:t+self.T_x+self.T_y, :].copy()  # [T_y, 1]\n",
        "\n",
        "        # CREATE TEACHER FORCING INPUTS (shifted right by 1)\n",
        "        y_in = np.zeros((self.T_y, 1), dtype=np.float32)\n",
        "        y_in[1:, 0] = y_future[:-1, 0]  # Shift: positions 1..T_y get targets 0..T_y-1\n",
        "        # y_in[0] = 0 (start token)\n",
        "\n",
        "        return torch.from_numpy(Xw), torch.from_numpy(y_in), torch.from_numpy(y_future)"
      ],
      "metadata": {
        "id": "hQHxZnlHfDUp"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T_X = 32\n",
        "T_Y = 8\n",
        "ds = WindowedTS(X, y, T_x=T_X, T_y=T_Y)\n",
        "print(f\"Total windows: {len(ds)} (history={T_X}, forecast={T_Y})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwAfp7dcIkMw",
        "outputId": "e1c78452-e465-4ed8-d86d-3141d3f1bc66"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total windows: 4930 (history=32, forecast=8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Separa em treino, validação e test e Cria dataloader"
      ],
      "metadata": {
        "id": "hcq2giAdU7Wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH = 64\n",
        "\n",
        "train_ds, val_ds, test_ds = random_split(ds, [0.6, 0.2, 0.2], generator=torch.Generator().manual_seed(0))\n",
        "\n",
        "\n",
        "# DATALOADERS\n",
        "# pin_memory=True: faster host→GPU transfer\n",
        "# num_workers=0: single-threaded (portable, easier to debug)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, drop_last=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH, shuffle=False, drop_last=False)"
      ],
      "metadata": {
        "id": "39YuSOyrJSzP"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Aplicação dos modelos"
      ],
      "metadata": {
        "id": "8I8So47hVUod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pré-seting"
      ],
      "metadata": {
        "id": "4aEKZ87N389r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ts_transformer_cuda_demo.py\n",
        "# Educational Implementation: Transformer for Time Series Forecasting\n",
        "#\n",
        "# This code demonstrates a complete sequence-to-sequence Transformer architecture\n",
        "# for multi-step time series forecasting. It includes:\n",
        "#   - Encoder-Decoder architecture with attention mechanisms\n",
        "#   - Scheduled sampling (gradual reduction of teacher forcing)\n",
        "#   - Mixed precision training (AMP) for GPU acceleration\n",
        "#   - Both teacher-forced and free-running evaluation modes\n",
        "#\n",
        "# Key Concepts for Students:\n",
        "#   1. Teacher Forcing: During training, we feed ground-truth previous outputs\n",
        "#      to the decoder (even if the model would have predicted differently)\n",
        "#   2. Scheduled Sampling: Gradually transition from teacher forcing to using\n",
        "#      model predictions during training (bridges train-test gap)\n",
        "#   3. Autoregressive Generation: At inference, the model generates step-by-step,\n",
        "#      using its own predictions as inputs for the next step\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                       0) RUNTIME / DEVICE SETTINGS\n",
        "# =============================================================================\n",
        "# PEDAGOGICAL NOTE: Modern deep learning leverages GPU acceleration.\n",
        "# This section configures PyTorch to use CUDA (NVIDIA GPUs) if available.\n",
        "\n",
        "# Automatically detect and use GPU if available, otherwise fall back to CPU\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if DEVICE == \"cuda\":\n",
        "    # Enable cuDNN autotuner: finds the fastest convolution algorithms for your hardware\n",
        "    # (Not critical for Transformers, but good practice for CNNs)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # Use TensorFloat-32 (TF32) for faster matrix multiplications on Ampere+ GPUs\n",
        "    # TF32 trades a tiny bit of precision for ~8x speed improvement\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")  # options: \"highest\", \"high\", \"medium\"\n",
        "    except Exception:\n",
        "        pass  # Older PyTorch versions don't support this\n",
        "\n",
        "# MIXED PRECISION TRAINING SETUP\n",
        "# Why? Training in 16-bit (half precision) is ~2x faster and uses less memory\n",
        "# than 32-bit (full precision), with minimal accuracy loss.\n",
        "USE_AMP = (DEVICE == \"cuda\")                                 # Enable AMP only on GPU\n",
        "USE_BF16 = USE_AMP and torch.cuda.is_bf16_supported()        # BFloat16 is more stable than Float16\n",
        "AMP_DTYPE = torch.bfloat16 if USE_BF16 else torch.float16    # Choose precision for autocast\n",
        "SCALER_ENABLED = USE_AMP and (not USE_BF16)                  # GradScaler only needed for fp16\n",
        "\n",
        "# REPRODUCIBILITY: Set random seeds for consistent results across runs\n",
        "# Important for teaching: students can reproduce exact results\n",
        "torch.manual_seed(7)\n",
        "np.random.seed(7)"
      ],
      "metadata": {
        "id": "bMytgbWP3T48"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 LSTM"
      ],
      "metadata": {
        "id": "BPqj8f-3WQiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TS_LSTM(nn.Module):\n",
        "    def __init__(self, input_size=30, step_ahead=10, hidden_size=64, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, step_ahead)\n",
        "        self.to(DEVICE)\n",
        "        self.step_ahead = step_ahead\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out.view(batch_size, self.step_ahead, 1)\n",
        "\n",
        "    def fit(self, loader, n_epochs=20, lr=0.001):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        loss_fn = nn.MSELoss()\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            epoch_loss = 0.0\n",
        "            for X_batch, _, y_batch in loader:\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = self(X_batch.to(DEVICE))\n",
        "                loss = loss_fn(y_pred.squeeze(), y_batch.squeeze().to(DEVICE))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "            epoch_loss /= len(loader.dataset)\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    def predict_evaluation(self, test_loader):\n",
        "\n",
        "      DEVICE = next(self.parameters()).device\n",
        "      self.eval()  # Set to evaluation mode\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # Get one batch from validation set\n",
        "          Xb_cpu, _, ytrue_cpu = next(iter(test_loader))\n",
        "\n",
        "          # Use only first sample for visualization\n",
        "          Xb = Xb_cpu[:1].to(DEVICE).float()      # [1, T_x, features]\n",
        "          ytrue = ytrue_cpu[:1].to(DEVICE).float()  # [1, T_y, 1]\n",
        "\n",
        "          # Generate forecast\n",
        "          pred = self(Xb).float()\n",
        "\n",
        "      # Convert to numpy for plotting\n",
        "      gt = ytrue[0, :, 0].detach().cpu().numpy()  # Ground truth\n",
        "      pr = pred[0, :, 0].detach().cpu().numpy()   # Prediction\n",
        "\n",
        "      # Compute error metrics\n",
        "      mae = np.mean(np.abs(pr - gt))              # Mean Absolute Error\n",
        "      rmse = np.sqrt(np.mean((pr - gt) ** 2))     # Root Mean Squared Error\n",
        "\n",
        "      return mae, rmse"
      ],
      "metadata": {
        "id": "iIxTPyYfWTbj"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = TS_LSTM(input_size=5, step_ahead=T_Y, hidden_size=64, num_layers=2)\n",
        "model_lstm.fit(train_loader, n_epochs=30, lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07kvoNncWgp7",
        "outputId": "ff4d0158-10be-4acf-ce7f-5de4ca0ad27b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 0.000934\n",
            "Epoch 2/30, Loss: 0.000044\n",
            "Epoch 3/30, Loss: 0.000040\n",
            "Epoch 4/30, Loss: 0.000040\n",
            "Epoch 5/30, Loss: 0.000040\n",
            "Epoch 6/30, Loss: 0.000040\n",
            "Epoch 7/30, Loss: 0.000040\n",
            "Epoch 8/30, Loss: 0.000039\n",
            "Epoch 9/30, Loss: 0.000040\n",
            "Epoch 10/30, Loss: 0.000040\n",
            "Epoch 11/30, Loss: 0.000040\n",
            "Epoch 12/30, Loss: 0.000040\n",
            "Epoch 13/30, Loss: 0.000040\n",
            "Epoch 14/30, Loss: 0.000039\n",
            "Epoch 15/30, Loss: 0.000040\n",
            "Epoch 16/30, Loss: 0.000040\n",
            "Epoch 17/30, Loss: 0.000040\n",
            "Epoch 18/30, Loss: 0.000040\n",
            "Epoch 19/30, Loss: 0.000040\n",
            "Epoch 20/30, Loss: 0.000040\n",
            "Epoch 21/30, Loss: 0.000040\n",
            "Epoch 22/30, Loss: 0.000040\n",
            "Epoch 23/30, Loss: 0.000040\n",
            "Epoch 24/30, Loss: 0.000040\n",
            "Epoch 25/30, Loss: 0.000040\n",
            "Epoch 26/30, Loss: 0.000040\n",
            "Epoch 27/30, Loss: 0.000040\n",
            "Epoch 28/30, Loss: 0.000040\n",
            "Epoch 29/30, Loss: 0.000040\n",
            "Epoch 30/30, Loss: 0.000040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mae, rmse = model_lstm.predict_evaluation(val_loader)\n",
        "\n",
        "print(\"\\nForecast analysis:\")\n",
        "print(f\"  - MAE (Mean Absolute Error): {mae:.4f}\")\n",
        "print(f\"  - RMSE (Root Mean Squared Error): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drTELuDUGKur",
        "outputId": "ea30749c-4f8f-4cb6-d54f-42aac2aaa27f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forecast analysis:\n",
            "  - MAE (Mean Absolute Error): 0.0332\n",
            "  - RMSE (Root Mean Squared Error): 0.0410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Transformer Original"
      ],
      "metadata": {
        "id": "mmis8lvXRlV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funções Base"
      ],
      "metadata": {
        "id": "IRuLoQp5vBFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "#                       1) ATTENTION MASK UTILITIES\n",
        "# =============================================================================\n",
        "# PEDAGOGICAL NOTE: Attention masks control which positions in the sequence\n",
        "# can \"see\" each other. This is crucial for:\n",
        "#   1. Causal masking: prevent the decoder from \"cheating\" by looking at future tokens\n",
        "#   2. Padding masking: ignore padded positions in variable-length sequences\n",
        "\n",
        "def subsequent_mask(sz: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Create a causal (look-ahead) mask for decoder self-attention.\n",
        "\n",
        "    This mask ensures that position i can only attend to positions <= i,\n",
        "    preventing the model from \"seeing the future\" during training.\n",
        "\n",
        "    Visual example (sz=4):\n",
        "        Position:  0  1  2  3\n",
        "              0 [  F  T  T  T ]   Position 0 can only see itself\n",
        "              1 [  F  F  T  T ]   Position 1 can see 0,1\n",
        "              2 [  F  F  F  T ]   Position 2 can see 0,1,2\n",
        "              3 [  F  F  F  F ]   Position 3 can see all (0,1,2,3)\n",
        "\n",
        "    Args:\n",
        "        sz: Sequence length (number of time steps)\n",
        "\n",
        "    Returns:\n",
        "        Boolean mask of shape [1, 1, sz, sz] where:\n",
        "          - True  = BLOCK attention (can't see this position)\n",
        "          - False = ALLOW attention (can see this position)\n",
        "\n",
        "    Why this shape? [1, 1, sz, sz] broadcasts to [batch, heads, sz, sz]\n",
        "    \"\"\"\n",
        "    # torch.triu creates upper triangular matrix; diagonal=1 excludes the main diagonal\n",
        "    mask = torch.triu(torch.ones(sz, sz, dtype=torch.bool, device=\"cpu\"), diagonal=1)\n",
        "    return mask.view(1, 1, sz, sz)  # Add batch and head dimensions for broadcasting\n",
        "\n",
        "\n",
        "def merge_padding_mask(attn_shape: Tuple[int, int, int, int],\n",
        "                       key_padding_mask: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Convert a key padding mask to attention score shape for broadcasting.\n",
        "\n",
        "    Padding masks handle variable-length sequences by ignoring padding tokens.\n",
        "\n",
        "    Example scenario:\n",
        "        Batch of 2 sequences:\n",
        "          Seq 1: [tok1, tok2, tok3, PAD, PAD]  length=3, padded to 5\n",
        "          Seq 2: [tok1, tok2, tok3, tok4, tok5] length=5, no padding\n",
        "\n",
        "        key_padding_mask = [[F, F, F, T, T],    # T = ignore these positions\n",
        "                            [F, F, F, F, F]]\n",
        "\n",
        "    Args:\n",
        "        attn_shape: Target shape [batch, heads, query_len, key_len]\n",
        "        key_padding_mask: [batch, key_len] where True = ignore this key position\n",
        "\n",
        "    Returns:\n",
        "        Mask broadcasted to [batch, heads, query_len, key_len], or None if no mask provided\n",
        "\n",
        "    Why broadcast? Each query position needs to know which key positions are padding.\n",
        "    \"\"\"\n",
        "    if key_padding_mask is None:\n",
        "        return None\n",
        "\n",
        "    b, t_k = key_padding_mask.shape\n",
        "    _, h, t_q, _ = attn_shape\n",
        "\n",
        "    # Reshape: [B, T_k] -> [B, 1, 1, T_k] then expand to [B, H, T_q, T_k]\n",
        "    return key_padding_mask.view(b, 1, 1, t_k).expand(b, h, t_q, t_k)\n",
        "\n",
        "# =============================================================================\n",
        "#                       2) POSITIONAL ENCODINGS\n",
        "# =============================================================================\n",
        "# PEDAGOGICAL NOTE: Transformers have no inherent notion of position/order.\n",
        "# Without positional encodings, \"I love cats\" = \"cats love I\" to the model!\n",
        "# We add position information in two ways:\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Fixed sinusoidal positional encoding (original \"Attention Is All You Need\" paper).\n",
        "\n",
        "    Key idea: Use sine and cosine functions of different frequencies to encode position.\n",
        "\n",
        "    Why sinusoidal?\n",
        "      1. Smooth, continuous encoding (nearby positions have similar encodings)\n",
        "      2. Can extrapolate to longer sequences than seen in training\n",
        "      3. No parameters to learn (reduces model complexity)\n",
        "\n",
        "    Formula for position pos and dimension i:\n",
        "        PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
        "        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "\n",
        "    Intuition: Low dimensions oscillate fast (capture fine details),\n",
        "               high dimensions oscillate slowly (capture coarse structure)\n",
        "\n",
        "    Args:\n",
        "        d_model: Embedding dimension (must match model hidden size)\n",
        "        max_len: Maximum sequence length we might encounter\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 10000):\n",
        "        super().__init__()\n",
        "\n",
        "        # Pre-compute positional encodings for all possible positions\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Position indices: [0, 1, 2, ..., max_len-1]\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Compute division term: 10000^(2i/d_model) for each dimension pair\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2, dtype=torch.float) *\n",
        "            -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        # Apply sine to even dimensions (0, 2, 4, ...)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cosine to odd dimensions (1, 3, 5, ...)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register as buffer: saved with model but not trained (no gradients)\n",
        "        self.register_buffer(\"pe\", pe, persistent=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Add positional encoding to input embeddings.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch, seq_len, d_model]\n",
        "\n",
        "        Returns:\n",
        "            x + positional_encoding, same shape as input\n",
        "        \"\"\"\n",
        "        T = x.size(1)  # Sequence length\n",
        "        # Slice out the needed positions and add to input (broadcasting over batch)\n",
        "        return x + self.pe[:T, :]\n",
        "\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Learned positional embeddings (trainable alternative to sinusoidal).\n",
        "\n",
        "    Key idea: Let the model learn optimal position representations from data.\n",
        "\n",
        "    Pros:\n",
        "      - Can learn task-specific position patterns\n",
        "      - Often performs slightly better on the specific task\n",
        "\n",
        "    Cons:\n",
        "      - Cannot extrapolate beyond max_len seen in training\n",
        "      - Adds parameters to the model (max_len * d_model weights)\n",
        "\n",
        "    Args:\n",
        "        d_model: Embedding dimension\n",
        "        max_len: Maximum sequence length (model can't handle longer sequences)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 10000):\n",
        "        super().__init__()\n",
        "        # Create a lookup table: position ID -> embedding vector\n",
        "        self.embed = nn.Embedding(max_len, d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Add learned positional embedding to input.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch, seq_len, d_model]\n",
        "\n",
        "        Returns:\n",
        "            x + learned_position_embedding\n",
        "        \"\"\"\n",
        "        B, T, _ = x.shape\n",
        "        # Create position IDs: [0, 1, 2, ..., T-1] for each batch item\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
        "        return x + self.embed(pos)\n",
        "\n",
        "# =============================================================================\n",
        "#                       3) TRANSFORMER BUILDING BLOCKS\n",
        "# =============================================================================\n",
        "# PEDAGOGICAL NOTE: Transformers are built from repeated layers of:\n",
        "#   1. Multi-Head Attention (the \"magic sauce\" - learns relationships)\n",
        "#   2. Feed-Forward Networks (processes each position independently)\n",
        "#   3. Residual Connections + Layer Normalization (training stability)\n",
        "\n",
        "class PreNormResidual(nn.Module):\n",
        "    \"\"\"\n",
        "    Pre-normalization residual wrapper: a modern, stable Transformer design pattern.\n",
        "\n",
        "    Flow: x -> LayerNorm -> Block -> Dropout -> (+) x\n",
        "                                                  ↑\n",
        "                                            residual\n",
        "\n",
        "    Why Pre-Norm (vs. Post-Norm)?\n",
        "      - More stable gradients (especially for deep networks)\n",
        "      - Can often train without learning rate warmup\n",
        "      - Original Transformer used Post-Norm, but Pre-Norm is now standard\n",
        "\n",
        "    Why Residual Connections?\n",
        "      - Allow gradients to flow directly through the network\n",
        "      - Help train very deep networks (50+ layers)\n",
        "      - Let early layers pass information directly to later layers\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        block: The actual computation (attention or feedforward)\n",
        "        dropout: Probability of dropping activations (regularization)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, block: nn.Module, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(d_model)  # Normalize over feature dimension\n",
        "        self.block = block                  # Multi-head attention or feedforward\n",
        "        self.drop = nn.Dropout(dropout)     # Randomly zero some activations\n",
        "\n",
        "    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply: residual + dropout(block(norm(x)))\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch, seq_len, d_model]\n",
        "            *args, **kwargs: Passed to the block (e.g., attention masks)\n",
        "\n",
        "        Returns:\n",
        "            Output with same shape as input\n",
        "        \"\"\"\n",
        "        # Normalize first, compute block, dropout, then add residual\n",
        "        return x + self.drop(self.block(self.norm(x), *args, **kwargs))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise feedforward network: processes each position independently.\n",
        "\n",
        "    Architecture: Linear -> Activation -> Dropout -> Linear\n",
        "                   (expand)                        (compress)\n",
        "\n",
        "    Why this design?\n",
        "      - Expands to higher dimension (d_ff, typically 4*d_model) for expressiveness\n",
        "      - Non-linear activation allows learning complex patterns\n",
        "      - Compresses back to original dimension\n",
        "      - Applied identically to each position (no cross-position interaction)\n",
        "\n",
        "    Think of it as: each position gets passed through a small 2-layer MLP.\n",
        "\n",
        "    Args:\n",
        "        d_model: Input/output dimension\n",
        "        d_ff: Hidden dimension (typically 2048 or 4*d_model)\n",
        "        dropout: Dropout probability after activation\n",
        "        activation: \"gelu\" (smooth) or \"relu\" (traditional)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.0, activation: str = \"gelu\"):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d_model, d_ff)    # Expand dimension\n",
        "        self.lin2 = nn.Linear(d_ff, d_model)    # Compress back\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        # GELU is smoother than ReLU, often works better in Transformers\n",
        "        self.act = F.gelu if activation == \"gelu\" else F.relu\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch, seq_len, d_model]\n",
        "        Returns:\n",
        "            Same shape as input\n",
        "        \"\"\"\n",
        "        # Expand -> activate -> dropout -> compress\n",
        "        return self.lin2(self.drop(self.act(self.lin1(x))))\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention: the core mechanism that makes Transformers powerful.\n",
        "\n",
        "    KEY INTUITION:\n",
        "        Attention lets each position \"look at\" other positions and decide\n",
        "        what information to pull from them. Multi-head attention runs\n",
        "        several attention operations in parallel (different \"heads\"),\n",
        "        each potentially learning different relationships.\n",
        "\n",
        "    Example: In \"The cat sat on the mat\", when processing \"sat\":\n",
        "      - Head 1 might focus on \"cat\" (subject)\n",
        "      - Head 2 might focus on \"mat\" (object)\n",
        "      - Head 3 might focus on \"on\" (preposition)\n",
        "\n",
        "    MECHANISM:\n",
        "      1. Project inputs to Query (Q), Key (K), Value (V) spaces\n",
        "      2. Split into multiple heads (num_heads parallel attention operations)\n",
        "      3. Compute attention scores: how much should each position attend to others?\n",
        "         score(q, k) = (q · k) / sqrt(d_head)  [scaled dot-product]\n",
        "      4. Apply softmax to get attention weights (sum to 1)\n",
        "      5. Weighted sum of Values: output = Σ attention_weight * value\n",
        "      6. Concatenate heads and project back to d_model\n",
        "\n",
        "    Args:\n",
        "        d_model: Total model dimension (must be divisible by num_heads)\n",
        "        num_heads: Number of parallel attention operations\n",
        "        dropout: Dropout applied to attention weights\n",
        "        bias: Whether to use bias in linear projections\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0, bias: bool = True):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.h = num_heads\n",
        "        self.d_head = d_model // num_heads  # Dimension per head\n",
        "\n",
        "        # Linear projections for Q, K, V (one per input role)\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "\n",
        "        # Output projection (combine all heads)\n",
        "        self.o_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def _split_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Split the embedding dimension into multiple heads.\n",
        "\n",
        "        Reshape: [B, T, D] -> [B, T, H, D/H] -> [B, H, T, D/H]\n",
        "\n",
        "        Why? We want to compute H separate attention operations in parallel.\n",
        "        Moving H to dimension 1 allows efficient batched matrix operations.\n",
        "        \"\"\"\n",
        "        B, T, _ = x.shape\n",
        "        return x.view(B, T, self.h, self.d_head).permute(0, 2, 1, 3)\n",
        "\n",
        "    def _combine_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Merge multiple heads back into single embedding.\n",
        "\n",
        "        Reshape: [B, H, T, D/H] -> [B, T, H, D/H] -> [B, T, D]\n",
        "\n",
        "        Concatenates all head outputs into the original d_model dimension.\n",
        "        \"\"\"\n",
        "        B, H, T, Dh = x.shape\n",
        "        return x.permute(0, 2, 1, 3).contiguous().view(B, T, H * Dh)\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask=None, key_padding_mask=None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute multi-head attention.\n",
        "\n",
        "        Args:\n",
        "            q: Query tensor [batch, seq_len_q, d_model]\n",
        "            k: Key tensor [batch, seq_len_k, d_model]\n",
        "            v: Value tensor [batch, seq_len_k, d_model]\n",
        "            attn_mask: Attention mask [1, 1, seq_len_q, seq_len_k] (e.g., causal mask)\n",
        "            key_padding_mask: Padding mask [batch, seq_len_k] (True = ignore this position)\n",
        "\n",
        "        Returns:\n",
        "            Attention output [batch, seq_len_q, d_model]\n",
        "\n",
        "        Note: For self-attention, q=k=v (same sequence attends to itself)\n",
        "              For cross-attention, q comes from decoder, k=v from encoder\n",
        "        \"\"\"\n",
        "        # Project and split into heads\n",
        "        Q = self._split_heads(self.q_proj(q))  # [B, H, T_q, D/H]\n",
        "        K = self._split_heads(self.k_proj(k))  # [B, H, T_k, D/H]\n",
        "        V = self._split_heads(self.v_proj(v))  # [B, H, T_k, D/H]\n",
        "\n",
        "        # SCALED DOT-PRODUCT ATTENTION\n",
        "        # Compute attention scores: how much should each query attend to each key?\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        # Shape: [B, H, T_q, T_k]\n",
        "        # Scaling by sqrt(d_head) prevents softmax saturation (gradients vanish)\n",
        "\n",
        "        # Use finfo.min instead of -inf for numerical stability in half precision\n",
        "        neg_inf = torch.finfo(scores.dtype).min\n",
        "\n",
        "        # APPLY MASKS\n",
        "        # Key padding mask: ignore padded positions in the key sequence\n",
        "        if key_padding_mask is not None:\n",
        "            kpm = merge_padding_mask(scores.shape, key_padding_mask)\n",
        "            scores = scores.masked_fill(kpm, neg_inf)  # Set to very negative -> softmax ~0\n",
        "\n",
        "        # Attention mask: prevent attending to certain positions (e.g., future tokens)\n",
        "        if attn_mask is not None:\n",
        "            if attn_mask.device != scores.device:\n",
        "                attn_mask = attn_mask.to(scores.device)\n",
        "            scores = scores.masked_fill(attn_mask, neg_inf)\n",
        "\n",
        "        # Compute attention weights (softmax over key dimension)\n",
        "        attn = torch.softmax(scores, dim=-1)  # [B, H, T_q, T_k]\n",
        "        attn = self.drop(attn)  # Dropout for regularization\n",
        "\n",
        "        # Weighted sum of values\n",
        "        out = torch.matmul(attn, V)  # [B, H, T_q, D/H]\n",
        "\n",
        "        # Combine heads and project\n",
        "        out = self._combine_heads(out)  # [B, T_q, D]\n",
        "        return self.o_proj(out)\n",
        "\n",
        "# =============================================================================\n",
        "#                       6) SCHEDULED SAMPLING (TEACHER FORCING PROBABILITY)\n",
        "# =============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def _bernoulli_mask(batch: int, p_tf: float, device):\n",
        "    \"\"\"\n",
        "    Sample a random mask for scheduled sampling.\n",
        "\n",
        "    Creates a per-sample binary decision: use teacher forcing or model prediction?\n",
        "\n",
        "    Args:\n",
        "        batch: Batch size\n",
        "        p_tf: Probability of using teacher forcing (0.0 = never, 1.0 = always)\n",
        "        device: torch device\n",
        "\n",
        "    Returns:\n",
        "        Boolean mask [batch, 1, 1] where:\n",
        "          True  = use ground-truth (teacher forcing)\n",
        "          False = use model prediction\n",
        "\n",
        "    Example with p_tf=0.7, batch=4:\n",
        "      Output might be: [[True], [True], [False], [True]]\n",
        "      → 3 samples use teacher forcing, 1 uses its own prediction\n",
        "    \"\"\"\n",
        "    return (torch.rand(batch, 1, 1, device=device) < p_tf)\n",
        "\n",
        "\n",
        "def scheduled_sampling_step(\n",
        "    model: TimeSeriesTransformer,\n",
        "    Xb: torch.Tensor,           # [batch, history_len, n_features]\n",
        "    ytrue: torch.Tensor,        # [batch, forecast_len, target_dim]\n",
        "    p_tf: float,                # teacher forcing probability ∈ [0, 1]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Autoregressive training with scheduled sampling.\n",
        "\n",
        "    THE EXPOSURE BIAS PROBLEM:\n",
        "      - Training with teacher forcing: decoder always sees perfect inputs\n",
        "      - Testing (inference): decoder sees its own (imperfect) predictions\n",
        "      - Result: model performs worse at test time than training suggests!\n",
        "\n",
        "    SOLUTION: Scheduled Sampling\n",
        "      - During training, randomly mix ground-truth and model predictions\n",
        "      - Gradually decrease teacher forcing probability over training\n",
        "      - Decoder learns to handle its own mistakes\n",
        "\n",
        "    ALGORITHM:\n",
        "      For each forecast step t:\n",
        "        1. Decode current sequence → predict next value\n",
        "        2. Randomly choose (with probability p_tf):\n",
        "           - Use ground-truth target (teacher forcing)\n",
        "           - Use model's prediction (self-correction)\n",
        "        3. Append chosen value to decoder input\n",
        "        4. Repeat for next step\n",
        "\n",
        "    Args:\n",
        "        model: TimeSeriesTransformer instance\n",
        "        Xb: Historical features [batch, history_len, n_features]\n",
        "        ytrue: Ground-truth targets [batch, forecast_len, target_dim]\n",
        "        p_tf: Teacher forcing probability\n",
        "              p_tf=1.0 → always use ground truth (standard teacher forcing)\n",
        "              p_tf=0.5 → 50/50 mix of ground truth and predictions\n",
        "              p_tf=0.0 → always use predictions (free-running)\n",
        "              p_tf=0.0 → always use predictions (free-running)\n",
        "\n",
        "    Returns:\n",
        "        Predictions [batch, forecast_len, target_dim] in float32\n",
        "\n",
        "    Note: Assumes dec_input_dim == target_dim for simplicity\n",
        "    \"\"\"\n",
        "    assert model.cfg.dec_input_dim == model.cfg.target_dim == ytrue.size(-1), \\\n",
        "        \"scheduled_sampling_step assumes dec_input_dim == target_dim.\"\n",
        "    device = Xb.device\n",
        "    B, T_y, C = ytrue.shape\n",
        "\n",
        "    # ENCODE HISTORY (once, doesn't change during generation)\n",
        "    x = model.pos_enc(model.enc_in(Xb))         # [batch, history_len, d_model]\n",
        "    mem = model.enc(x, key_padding_mask=None)   # Encoder output (memory)\n",
        "\n",
        "    preds = []\n",
        "    # SEED: Start decoder with first ground-truth target\n",
        "    # (Alternative: use last observed value from history)\n",
        "    y_seq = ytrue[:, :1, :]                     # [batch, 1, target_dim]\n",
        "\n",
        "    # AUTOREGRESSIVE LOOP WITH SCHEDULED SAMPLING\n",
        "    # Mixed precision for speed (automatic on GPU)\n",
        "    with torch.amp.autocast(\"cuda\", enabled=USE_AMP, dtype=AMP_DTYPE):\n",
        "        for t in range(T_y):\n",
        "            # DECODE current prefix\n",
        "            y_dec = model.pos_dec(model.dec_in(y_seq))          # Add positional encoding\n",
        "            self_mask = subsequent_mask(y_dec.size(1)).to(device)  # Causal mask\n",
        "            out = model.dec(y_dec, mem, self_mask, None, None)  # Decoder forward\n",
        "\n",
        "            # PREDICT next value\n",
        "            step_pred = model.head(out[:, -1:, :])              # Project last position\n",
        "            if model.out_act is not None:\n",
        "                step_pred = model.out_act(step_pred)            # Apply activation\n",
        "            preds.append(step_pred)\n",
        "\n",
        "            # DONE if this was the last step\n",
        "            if t == T_y - 1:\n",
        "                break\n",
        "\n",
        "            # SCHEDULED SAMPLING: Choose next input\n",
        "            # Each sample independently decides: teacher forcing or self-prediction?\n",
        "            use_teacher = _bernoulli_mask(B, p_tf, device)      # [batch, 1, 1]\n",
        "            next_input = torch.where(\n",
        "                use_teacher,\n",
        "                ytrue[:, t:t+1, :],  # Ground truth\n",
        "                step_pred             # Model prediction\n",
        "            )\n",
        "\n",
        "            # GROW decoder input sequence\n",
        "            y_seq = torch.cat([y_seq, next_input], dim=1)\n",
        "\n",
        "    pred_seq = torch.cat(preds, dim=1)          # Concatenate all predictions\n",
        "    return pred_seq.float()                      # Ensure float32 for stable loss computation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6GeF7mpDvAUw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definição"
      ],
      "metadata": {
        "id": "NdNF2j6LX3kM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YnJ_UR5C32kr"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Transformer encoder layer.\n",
        "\n",
        "    Architecture:\n",
        "      1. Multi-Head Self-Attention (each position attends to all positions)\n",
        "      2. Feedforward Network (process each position independently)\n",
        "\n",
        "    Both wrapped in PreNormResidual blocks for stability.\n",
        "\n",
        "    Purpose: Build representations that incorporate context from the entire sequence.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        num_heads: Number of attention heads\n",
        "        d_ff: Feedforward hidden dimension\n",
        "        attn_dropout: Dropout for attention\n",
        "        ff_dropout: Dropout for feedforward\n",
        "        activation: Activation function type\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, attn_dropout=0.0, ff_dropout=0.0, activation=\"gelu\"):\n",
        "        super().__init__()\n",
        "        # Self-attention: sequence attends to itself\n",
        "        self.mha = PreNormResidual(\n",
        "            d_model,\n",
        "            MultiHeadAttention(d_model, num_heads, attn_dropout)\n",
        "        )\n",
        "        # Position-wise feedforward\n",
        "        self.ffn = PreNormResidual(\n",
        "            d_model,\n",
        "            FeedForward(d_model, d_ff, ff_dropout, activation)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input sequence [batch, seq_len, d_model]\n",
        "            key_padding_mask: Padding mask [batch, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            Encoded sequence [batch, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # Self-attention: q=k=v=x\n",
        "        x = self.mha(x, x, x, attn_mask=None, key_padding_mask=key_padding_mask)\n",
        "        # Feedforward\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Transformer decoder layer.\n",
        "\n",
        "    Architecture:\n",
        "      1. Masked Self-Attention (causal: can't see future)\n",
        "      2. Cross-Attention (attend to encoder output)\n",
        "      3. Feedforward Network\n",
        "\n",
        "    All wrapped in PreNormResidual blocks.\n",
        "\n",
        "    Purpose: Generate output sequence one step at a time, using:\n",
        "      - What we've generated so far (self-attention)\n",
        "      - Information from the input sequence (cross-attention)\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        num_heads: Number of attention heads\n",
        "        d_ff: Feedforward hidden dimension\n",
        "        attn_dropout: Dropout for attention\n",
        "        ff_dropout: Dropout for feedforward\n",
        "        activation: Activation function type\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, attn_dropout=0.0, ff_dropout=0.0, activation=\"gelu\"):\n",
        "        super().__init__()\n",
        "        # Causal self-attention: decoder attends to its own past\n",
        "        self.self_attn = PreNormResidual(\n",
        "            d_model,\n",
        "            MultiHeadAttention(d_model, num_heads, attn_dropout)\n",
        "        )\n",
        "        # Cross-attention: decoder attends to encoder output\n",
        "        self.cross_attn = PreNormResidual(\n",
        "            d_model,\n",
        "            MultiHeadAttention(d_model, num_heads, attn_dropout)\n",
        "        )\n",
        "        # Position-wise feedforward\n",
        "        self.ffn = PreNormResidual(\n",
        "            d_model,\n",
        "            FeedForward(d_model, d_ff, ff_dropout, activation)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mem, self_attn_mask, self_key_padding_mask, mem_key_padding_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Decoder input sequence [batch, seq_len, d_model]\n",
        "            mem: Encoder output (memory) [batch, src_len, d_model]\n",
        "            self_attn_mask: Causal mask for decoder self-attention\n",
        "            self_key_padding_mask: Padding mask for decoder sequence\n",
        "            mem_key_padding_mask: Padding mask for encoder sequence\n",
        "\n",
        "        Returns:\n",
        "            Decoded sequence [batch, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # 1. Causal self-attention: look at past decoder outputs\n",
        "        x = self.self_attn(x, x, x, attn_mask=self_attn_mask, key_padding_mask=self_key_padding_mask)\n",
        "\n",
        "        # 2. Cross-attention: look at encoder outputs\n",
        "        #    Query from decoder, Key/Value from encoder\n",
        "        x = self.cross_attn(x, mem, mem, attn_mask=None, key_padding_mask=mem_key_padding_mask)\n",
        "\n",
        "        # 3. Feedforward processing\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Stack of encoder layers + final layer normalization.\n",
        "\n",
        "    Purpose: Process the input sequence to build rich representations\n",
        "    that capture context and relationships.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        num_layers: Number of encoder layers to stack\n",
        "        num_heads: Number of attention heads per layer\n",
        "        d_ff: Feedforward hidden dimension\n",
        "        attn_dropout: Attention dropout\n",
        "        ff_dropout: Feedforward dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_layers, num_heads, d_ff, attn_dropout=0.0, ff_dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, attn_dropout, ff_dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)  # Final normalization\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input embeddings [batch, seq_len, d_model]\n",
        "            key_padding_mask: Padding mask [batch, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            Encoded representations [batch, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, key_padding_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Stack of decoder layers + final layer normalization.\n",
        "\n",
        "    Purpose: Generate output sequence autoregressively,\n",
        "    conditioned on encoder output.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        num_layers: Number of decoder layers to stack\n",
        "        num_heads: Number of attention heads per layer\n",
        "        d_ff: Feedforward hidden dimension\n",
        "        attn_dropout: Attention dropout\n",
        "        ff_dropout: Feedforward dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_layers, num_heads, d_ff, attn_dropout=0.0, ff_dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, attn_dropout, ff_dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)  # Final normalization\n",
        "\n",
        "    def forward(self, y, mem, self_attn_mask, self_key_padding_mask, mem_key_padding_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            y: Decoder input sequence [batch, seq_len, d_model]\n",
        "            mem: Encoder output [batch, src_len, d_model]\n",
        "            self_attn_mask: Causal mask\n",
        "            self_key_padding_mask: Decoder padding mask\n",
        "            mem_key_padding_mask: Encoder padding mask\n",
        "\n",
        "        Returns:\n",
        "            Decoded representations [batch, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            y = layer(y, mem, self_attn_mask, self_key_padding_mask, mem_key_padding_mask)\n",
        "        return self.norm(y)\n",
        "\n",
        "# =============================================================================\n",
        "#                       4) COMPLETE MODEL (ENC + DEC)\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Sequence-to-Sequence Transformer for Time Series Forecasting.\n",
        "\n",
        "    ARCHITECTURE OVERVIEW:\n",
        "      Input (history) → Encoder → Memory\n",
        "                                     ↓\n",
        "      Previous outputs → Decoder → Next prediction\n",
        "\n",
        "    KEY CONCEPTS:\n",
        "      1. ENCODER: Processes historical data (e.g., past 24 hours)\n",
        "         - Builds rich representations that capture patterns and context\n",
        "         - Output = \"memory\" that decoder can attend to\n",
        "\n",
        "      2. DECODER: Generates future predictions (e.g., next 12 hours)\n",
        "         - Autoregressively: generates one step at a time\n",
        "         - Uses both its own past predictions AND encoder memory\n",
        "\n",
        "      3. TEACHER FORCING (training): Feed ground-truth previous values to decoder\n",
        "         - Fast: can train all steps in parallel\n",
        "         - Problem: decoder never sees its own mistakes during training\n",
        "\n",
        "      4. AUTOREGRESSIVE GENERATION (inference): Feed model's own predictions\n",
        "         - Slow: must generate step-by-step\n",
        "         - Realistic: matches deployment scenario\n",
        "\n",
        "    EXAMPLE:\n",
        "      History: [hour 1, hour 2, ..., hour 24] → Encoder\n",
        "      Decoder generates: [hour 25, hour 26, ..., hour 36]\n",
        "        - To predict hour 26, decoder sees: hour 25 (either ground-truth or its prediction)\n",
        "        - To predict hour 27, decoder sees: hours 25-26\n",
        "        - And so on...\n",
        "\n",
        "    Args:\n",
        "        cfg: Configuration object with all hyperparameters\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: TSTConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # INPUT PROJECTIONS: Convert raw features to model dimension\n",
        "        self.enc_in = nn.Linear(cfg.input_dim, cfg.d_model)      # Encoder input projection\n",
        "        self.dec_in = nn.Linear(cfg.dec_input_dim, cfg.d_model)  # Decoder input projection\n",
        "\n",
        "        # POSITIONAL ENCODINGS: Add position information to embeddings\n",
        "        if cfg.pos_encoding == \"sin\":\n",
        "            self.pos_enc = SinusoidalPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "            self.pos_dec = SinusoidalPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "        else:\n",
        "            self.pos_enc = LearnedPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "            self.pos_dec = LearnedPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "\n",
        "        # ENCODER & DECODER STACKS\n",
        "        self.enc = Encoder(cfg.d_model, cfg.num_encoder_layers, cfg.num_heads, cfg.d_ff,\n",
        "                           attn_dropout=cfg.dropout, ff_dropout=cfg.dropout)\n",
        "        self.dec = Decoder(cfg.d_model, cfg.num_decoder_layers, cfg.num_heads, cfg.d_ff,\n",
        "                           attn_dropout=cfg.dropout, ff_dropout=cfg.dropout)\n",
        "\n",
        "        # OUTPUT HEAD: Project decoder output to prediction space\n",
        "        self.head = nn.Linear(cfg.d_model, cfg.target_dim)\n",
        "\n",
        "        # INPUT DROPOUT: Regularization on embeddings\n",
        "        self.drop_in = nn.Dropout(cfg.dropout)\n",
        "\n",
        "        # OPTIONAL OUTPUT ACTIVATION: Constrain predictions to a range\n",
        "        self.out_act = None\n",
        "        if cfg.output_activation == \"tanh\":\n",
        "            self.out_act = torch.tanh      # Range: [-1, 1]\n",
        "        elif cfg.output_activation == \"sigmoid\":\n",
        "            self.out_act = torch.sigmoid   # Range: [0, 1]\n",
        "\n",
        "    def forward(self, X, y_in, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Teacher-forced forward pass (for training).\n",
        "\n",
        "        TRAINING MODE: We give the decoder the ground-truth previous outputs.\n",
        "        This allows parallel computation (fast!) but creates train-test mismatch.\n",
        "\n",
        "        Flow:\n",
        "          1. Encode history: X → embeddings → encoder → memory\n",
        "          2. Decode with teacher forcing: y_in → embeddings → decoder → predictions\n",
        "          3. Project to output space: predictions → final forecasts\n",
        "\n",
        "        Args:\n",
        "            X: Historical features [batch, history_len, input_dim]\n",
        "               Example shape: [32, 24, 4] = 32 sequences, 24 timesteps, 4 features\n",
        "\n",
        "            y_in: Decoder inputs (shifted targets) [batch, forecast_len, dec_input_dim]\n",
        "                  IMPORTANT: y_in[0] = start token (often 0 or last observed value)\n",
        "                             y_in[1] = ground-truth target at step 0\n",
        "                             y_in[2] = ground-truth target at step 1\n",
        "                             ... (shifted right by 1)\n",
        "\n",
        "            src_key_padding_mask: Padding mask for encoder [batch, history_len]\n",
        "            tgt_key_padding_mask: Padding mask for decoder [batch, forecast_len]\n",
        "\n",
        "        Returns:\n",
        "            Predictions [batch, forecast_len, target_dim]\n",
        "\n",
        "        TEACHER FORCING VISUALIZATION:\n",
        "          Ground truth: [y₀, y₁, y₂, y₃]\n",
        "          Decoder input: [START, y₀, y₁, y₂]  (shifted right)\n",
        "          Predictions:   [ŷ₀, ŷ₁, ŷ₂, ŷ₃]\n",
        "          Loss: compare predictions to ground truth\n",
        "        \"\"\"\n",
        "        # ENCODE HISTORY\n",
        "        x = self.drop_in(self.pos_enc(self.enc_in(X)))  # Project → positional encoding → dropout\n",
        "        mem = self.enc(x, key_padding_mask=src_key_padding_mask)  # Run through encoder\n",
        "\n",
        "        # DECODE WITH TEACHER FORCING\n",
        "        y = self.drop_in(self.pos_dec(self.dec_in(y_in)))  # Process decoder inputs\n",
        "        self_mask = subsequent_mask(y.size(1)).to(y.device)  # Causal mask: can't see future\n",
        "        out = self.dec(y, mem, self_mask, tgt_key_padding_mask, src_key_padding_mask)\n",
        "\n",
        "        # PROJECT TO OUTPUT SPACE\n",
        "        logits = self.head(out)  # [batch, forecast_len, target_dim]\n",
        "\n",
        "        # Apply output activation if specified\n",
        "        return logits if self.out_act is None else self.out_act(logits)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, X, y_start, steps, src_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Autoregressive generation (for inference/deployment).\n",
        "\n",
        "        INFERENCE MODE: Decoder uses its own predictions (no ground truth available).\n",
        "        This is slow (sequential) but realistic for deployment.\n",
        "\n",
        "        Process:\n",
        "          1. Encode history once (doesn't change)\n",
        "          2. Start with initial token (y_start)\n",
        "          3. For each step:\n",
        "             a. Decode current sequence\n",
        "             b. Predict next value\n",
        "             c. Append prediction to sequence\n",
        "             d. Repeat\n",
        "\n",
        "        Args:\n",
        "            X: Historical features [batch, history_len, input_dim]\n",
        "            y_start: Initial decoder token [batch, 1, dec_input_dim]\n",
        "                     Often: last observed target value or a zero vector\n",
        "            steps: Number of future steps to predict\n",
        "            src_key_padding_mask: Padding mask for encoder\n",
        "\n",
        "        Returns:\n",
        "            Predictions [batch, steps, target_dim]\n",
        "\n",
        "        AUTOREGRESSIVE VISUALIZATION:\n",
        "          Step 1: [START]           → predict ŷ₀\n",
        "          Step 2: [START, ŷ₀]       → predict ŷ₁\n",
        "          Step 3: [START, ŷ₀, ŷ₁]   → predict ŷ₂\n",
        "          Step 4: [START, ŷ₀, ŷ₁, ŷ₂] → predict ŷ₃\n",
        "          ...\n",
        "\n",
        "        Note: Errors compound! If ŷ₀ is wrong, all future predictions are affected.\n",
        "        \"\"\"\n",
        "        device = X.device\n",
        "\n",
        "        # ENCODE HISTORY (once)\n",
        "        x = self.pos_enc(self.enc_in(X))\n",
        "        mem = self.enc(x, key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        preds = []\n",
        "        y_seq = y_start  # Start with initial token; will grow each step\n",
        "\n",
        "        # AUTOREGRESSIVE LOOP\n",
        "        for _ in range(steps):\n",
        "            # Decode current sequence\n",
        "            y_dec = self.pos_dec(self.dec_in(y_seq))\n",
        "            self_mask = subsequent_mask(y_dec.size(1)).to(device)\n",
        "            out = self.dec(y_dec, mem, self_mask, None, src_key_padding_mask)\n",
        "\n",
        "            # Predict next value (only use last position's output)\n",
        "            next_logits = self.head(out[:, -1:, :])  # [batch, 1, target_dim]\n",
        "            next_value = next_logits if self.out_act is None else self.out_act(next_logits)\n",
        "            preds.append(next_value)\n",
        "\n",
        "            # APPEND PREDICTION TO SEQUENCE (for next iteration)\n",
        "            # Handle dimension mismatch if dec_input_dim != target_dim\n",
        "            if self.cfg.dec_input_dim == self.cfg.target_dim:\n",
        "                y_seq = torch.cat([y_seq, next_value], dim=1)\n",
        "            else:\n",
        "                if self.cfg.dec_input_dim > self.cfg.target_dim:\n",
        "                    # Pad prediction to match decoder input size\n",
        "                    pad = torch.zeros(X.size(0), 1, self.cfg.dec_input_dim - self.cfg.target_dim, device=device)\n",
        "                    y_seq = torch.cat([y_seq, torch.cat([next_value, pad], dim=-1)], dim=1)\n",
        "                else:\n",
        "                    # Truncate prediction to match decoder input size\n",
        "                    y_seq = torch.cat([y_seq, next_value[..., :self.cfg.dec_input_dim]], dim=1)\n",
        "\n",
        "        return torch.cat(preds, dim=1)  # [batch, steps, target_dim]\n",
        "\n",
        "\n",
        "    def run_epoch(self, loader, train: bool, p_tf: float):\n",
        "      \"\"\"\n",
        "      Run one epoch of training or evaluation.\n",
        "\n",
        "      Args:\n",
        "          loader: DataLoader (train or validation)\n",
        "          train: If True, update weights; if False, just evaluate\n",
        "          p_tf: Teacher forcing probability for scheduled sampling\n",
        "\n",
        "      Returns:\n",
        "          Average loss over the epoch\n",
        "      \"\"\"\n",
        "      self.train(train)  # Set mode (affects dropout, batchnorm, etc.)\n",
        "      total_loss = 0.0\n",
        "      n_samples = 0\n",
        "\n",
        "      for Xb_cpu, _, ytrue_cpu in loader:\n",
        "          # MOVE TO DEVICE (non_blocking=True allows async transfer)\n",
        "          Xb = Xb_cpu.to(DEVICE, non_blocking=True).float()        # [batch, T_x, features]\n",
        "          ytrue = ytrue_cpu.to(DEVICE, non_blocking=True).float()  # [batch, T_y, 1]\n",
        "\n",
        "          # ZERO GRADIENTS (set_to_none=True is faster than zero_grad())\n",
        "          self.opt.zero_grad(set_to_none=True)\n",
        "\n",
        "          # FORWARD PASS: Autoregressive with scheduled sampling\n",
        "          pred = scheduled_sampling_step(self, Xb, ytrue, p_tf=p_tf)\n",
        "\n",
        "          # COMPUTE LOSS: Mean Squared Error over all predictions\n",
        "          loss = F.mse_loss(pred, ytrue)  # Average over batch, steps, dimensions\n",
        "\n",
        "          # BACKWARD PASS (only in training mode)\n",
        "          if train:\n",
        "              if self.scaler.is_enabled():\n",
        "                  # MIXED PRECISION TRAINING\n",
        "                  self.scaler.scale(loss).backward()           # Scale loss to prevent underflow\n",
        "                  nn.utils.clip_grad_norm_(self.parameters(), 1.0)  # Clip gradients\n",
        "                  self.scaler.step(self.opt)                        # Update weights\n",
        "                  self.scaler.update()                         # Update scale factor\n",
        "              else:\n",
        "                  # STANDARD TRAINING\n",
        "                  loss.backward()\n",
        "                  nn.utils.clip_grad_norm_(self.parameters(), 1.0)  # Prevent exploding gradients\n",
        "                  self.opt.step()\n",
        "\n",
        "          # ACCUMULATE LOSS (weighted by batch size for proper averaging)\n",
        "          total_loss += loss.item() * Xb.size(0)\n",
        "          n_samples += Xb.size(0)\n",
        "\n",
        "      return total_loss / max(1, n_samples)  # Average loss per sample\n",
        "\n",
        "\n",
        "\n",
        "    def train_transformer(self, train_loader, val_loader,\n",
        "      EPOCHS=8,\n",
        "      LR=1e-3,\n",
        "      TF_START=0.9,                 # Initial teacher forcing probability\n",
        "      TF_END=0.5                    # Final teacher forcing probability\n",
        "      ):\n",
        "      \"\"\"\n",
        "      Complete training pipeline with scheduled sampling.\n",
        "\n",
        "      TRAINING STRATEGY:\n",
        "        1. Start with high teacher forcing (p_tf=0.9): easy learning\n",
        "        2. Gradually reduce (linear schedule to p_tf=0.5): harder learning\n",
        "        3. Validate with both teacher-forced and free-running modes\n",
        "\n",
        "      WHY TWO VALIDATION METRICS?\n",
        "        - val_tf (teacher-forced): Fast, optimistic (what model could achieve)\n",
        "        - val_free (free-running): Slow, realistic (actual deployment performance)\n",
        "        - Gap between them shows exposure bias\n",
        "\n",
        "      OPTIMIZATION:\n",
        "        - AdamW optimizer: Adam + weight decay (prevents overfitting)\n",
        "        - Cosine annealing: learning rate decreases smoothly\n",
        "        - Gradient clipping: prevents exploding gradients\n",
        "        - Mixed precision (AMP): faster training on GPU\n",
        "\n",
        "      Args:\n",
        "          EPOCHS: Number of training epochs\n",
        "          N_SAMPLES: Number of time series to generate\n",
        "          SERIES_LENGTH: Length of each series\n",
        "          T_X: History window size\n",
        "          T_Y: Forecast horizon\n",
        "          BATCH: Batch size\n",
        "          LR: Initial learning rate\n",
        "          TF_START: Starting teacher forcing probability\n",
        "          TF_END: Ending teacher forcing probability\n",
        "\n",
        "      Returns:\n",
        "          model: Trained model\n",
        "          loaders: (train_loader, val_loader)\n",
        "          history: Training history dict\n",
        "          data: (X, y, stats) for visualization\n",
        "          windows: (T_X, T_Y) window sizes\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Count parameters\n",
        "      n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "      print(f\"Model parameters: {n_params:,}\")\n",
        "      print(f\"Device: {DEVICE}\")\n",
        "      print(f\"Mixed precision: {'BFloat16' if USE_BF16 else 'Float16' if USE_AMP else 'Float32'}\\n\")\n",
        "\n",
        "      # OPTIMIZER: AdamW with weight decay (L2 regularization)\n",
        "      self.opt = torch.optim.AdamW(self.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "      # SCHEDULER: Cosine annealing (smooth LR decay)\n",
        "      sched = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt, T_max=EPOCHS)\n",
        "\n",
        "      # GRADIENT SCALER: For mixed precision with float16 (not needed for bfloat16)\n",
        "      self.scaler = torch.amp.GradScaler(device=\"cuda\", enabled=SCALER_ENABLED)\n",
        "\n",
        "\n",
        "      print(\"=\" * 70)\n",
        "      print(\"TRAINING\")\n",
        "      print(\"=\" * 70)\n",
        "\n",
        "      # --- TRAINING LOOP ---\n",
        "      # Track three metrics:\n",
        "      #   1. train: training loss with scheduled sampling\n",
        "      #   2. val_tf: validation loss with teacher forcing (optimistic)\n",
        "      #   3. val_free: validation loss with free-running (realistic)\n",
        "      history = {\"train\": [], \"val_tf\": [], \"val_free\": []}\n",
        "\n",
        "      for ep in range(1, EPOCHS+1):\n",
        "          # TEACHER FORCING SCHEDULE: Linear decay from TF_START to TF_END\n",
        "          alpha = (ep - 1) / max(1, EPOCHS - 1)        # 0 → 1 progress\n",
        "          p_tf = TF_START * (1 - alpha) + TF_END * alpha\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # TRAIN: One epoch with scheduled sampling\n",
        "          train_loss = self.run_epoch(train_loader, train=True, p_tf=p_tf)\n",
        "\n",
        "          # VALIDATION MODE 1: Teacher-forced (parallel, fast)\n",
        "          # Gives us a sense of model capacity (if it had perfect inputs)\n",
        "          self.train(False)\n",
        "          val_tf_total, val_n = 0.0, 0\n",
        "\n",
        "          for Xb_cpu, y_in_cpu, ytrue_cpu in val_loader:\n",
        "              Xb = Xb_cpu.to(DEVICE, non_blocking=True).float()\n",
        "              y_in = y_in_cpu.to(DEVICE, non_blocking=True).float()  # Pre-shifted targets\n",
        "              ytrue = ytrue_cpu.to(DEVICE, non_blocking=True).float()\n",
        "\n",
        "              # Teacher-forced forward pass (all steps in parallel)\n",
        "              with torch.amp.autocast(\"cuda\", enabled=USE_AMP, dtype=AMP_DTYPE):\n",
        "                  pred_tf = self(Xb, y_in)           # [batch, T_y, 1]\n",
        "\n",
        "              loss_tf = F.mse_loss(pred_tf.float(), ytrue)\n",
        "              val_tf_total += loss_tf.item() * Xb.size(0)\n",
        "              val_n += Xb.size(0)\n",
        "\n",
        "          val_tf = val_tf_total / max(1, val_n)\n",
        "\n",
        "          # VALIDATION MODE 2: Free-running (autoregressive, slow)\n",
        "          # This is the REAL test: how well does model perform in deployment?\n",
        "          val_free_total, val_n = 0.0, 0\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for Xb_cpu, _, ytrue_cpu in val_loader:\n",
        "                  Xb = Xb_cpu.to(DEVICE, non_blocking=True).float()\n",
        "                  ytrue = ytrue_cpu.to(DEVICE, non_blocking=True).float()\n",
        "\n",
        "                  # Seed with first ground-truth value (in practice: last observed target)\n",
        "                  y_start = ytrue[:, :1, :]\n",
        "\n",
        "                  # Generate predictions autoregressively\n",
        "                  pred_free = self.generate(Xb, y_start=y_start, steps=T_Y).float()\n",
        "\n",
        "                  loss_free = F.mse_loss(pred_free, ytrue)\n",
        "                  val_free_total += loss_free.item() * Xb.size(0)\n",
        "                  val_n += Xb.size(0)\n",
        "\n",
        "          val_free = val_free_total / max(1, val_n)\n",
        "\n",
        "          # UPDATE LEARNING RATE\n",
        "          sched.step()\n",
        "\n",
        "          # PRINT PROGRESS\n",
        "          print(f\"Epoch {ep:02d} | p_tf={p_tf:.2f} | \"\n",
        "                f\"train={train_loss:.4f} | val_tf={val_tf:.4f} | val_free={val_free:.4f} | \"\n",
        "                f\"lr={sched.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "          # RECORD HISTORY\n",
        "          history[\"train\"].append(train_loss)\n",
        "          history[\"val_tf\"].append(val_tf)\n",
        "          history[\"val_free\"].append(val_free)\n",
        "\n",
        "      print(\"\\n\" + \"=\" * 70)\n",
        "      print(\"TRAINING COMPLETE\")\n",
        "      print(\"=\" * 70)\n",
        "      print(f\"Final train loss: {history['train'][-1]:.4f}\")\n",
        "      print(f\"Final val_tf loss: {history['val_tf'][-1]:.4f}\")\n",
        "      print(f\"Final val_free loss: {history['val_free'][-1]:.4f}\")\n",
        "      print(f\"Exposure bias gap: {history['val_free'][-1] - history['val_tf'][-1]:.4f}\")\n",
        "      print(\"(Gap = how much worse free-running is than teacher-forced)\\n\")\n",
        "\n",
        "      return history\n",
        "    def predict_evaluation(self, test_loader):\n",
        "\n",
        "      DEVICE = next(self.parameters()).device\n",
        "      self.eval()  # Set to evaluation mode\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # Get one batch from validation set\n",
        "          Xb_cpu, _, ytrue_cpu = next(iter(test_loader))\n",
        "\n",
        "          # Use only first sample for visualization\n",
        "          Xb = Xb_cpu[:1].to(DEVICE).float()      # [1, T_x, features]\n",
        "          ytrue = ytrue_cpu[:1].to(DEVICE).float()  # [1, T_y, 1]\n",
        "\n",
        "          # Generate forecast (autoregressive)\n",
        "          y_start = ytrue[:, :1, :]  # Seed with first ground-truth value\n",
        "          pred = self.generate(Xb, y_start=y_start, steps=T_Y).float()\n",
        "\n",
        "      # Convert to numpy for plotting\n",
        "      gt = ytrue[0, :, 0].detach().cpu().numpy()  # Ground truth\n",
        "      pr = pred[0, :, 0].detach().cpu().numpy()   # Prediction\n",
        "\n",
        "      # Compute error metrics\n",
        "      mae = np.mean(np.abs(pr - gt))              # Mean Absolute Error\n",
        "      rmse = np.sqrt(np.mean((pr - gt) ** 2))     # Root Mean Squared Error\n",
        "\n",
        "      return mae, rmse\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chamada"
      ],
      "metadata": {
        "id": "TmqzWQGlYFJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TSTConfig:\n",
        "    \"\"\"\n",
        "    Configuration for Time Series Transformer.\n",
        "\n",
        "    Architecture hyperparameters:\n",
        "        d_model: Hidden dimension throughout the model (like \"brain size\")\n",
        "        num_encoder_layers: Depth of encoder (more = more context integration)\n",
        "        num_decoder_layers: Depth of decoder (fewer than encoder is common)\n",
        "        num_heads: Parallel attention operations (more = capture more relationships)\n",
        "        d_ff: Feedforward expansion dimension (typically 4*d_model)\n",
        "        dropout: Regularization strength (0.0 = none, 0.3 = aggressive)\n",
        "\n",
        "    Input/output specification:\n",
        "        input_dim: Number of input features (e.g., temperature, pressure, etc.)\n",
        "        target_dim: Number of values to predict (often 1 for univariate forecasting)\n",
        "        dec_input_dim: Decoder input size (should match target_dim for simplicity)\n",
        "\n",
        "    Other settings:\n",
        "        pos_encoding: \"sin\" (fixed) or \"learned\" (trainable)\n",
        "        max_len: Maximum sequence length\n",
        "        output_activation: Optional final activation (\"tanh\", \"sigmoid\", or None)\n",
        "    \"\"\"\n",
        "    d_model: int = 128\n",
        "    num_encoder_layers: int = 3\n",
        "    num_decoder_layers: int = 2\n",
        "    num_heads: int = 8\n",
        "    d_ff: int = 256\n",
        "    dropout: float = 0.1\n",
        "    pos_encoding: str = \"sin\"  # \"sin\" or \"learned\"\n",
        "    max_len: int = 4096\n",
        "    input_dim: int = 2         # number of historical features\n",
        "    target_dim: int = 1        # we forecast a single target value\n",
        "    dec_input_dim: int = 1     # decoder input size (shifted target)\n",
        "    output_activation: Optional[str] = None  # e.g., \"tanh\" for bounded target\n",
        "\n",
        "# --- MODEL + OPTIMIZER ---\n",
        "cfg = TSTConfig()\n",
        "model_transformer = TimeSeriesTransformer(cfg).to(DEVICE)\n",
        "history = model_transformer.train_transformer(train_loader, val_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "1oSWp203cgot",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "409d82aa-7866-46f9-b126-8abd71a896a4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 796,289\n",
            "Device: cuda\n",
            "Mixed precision: BFloat16\n",
            "\n",
            "======================================================================\n",
            "TRAINING\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (2048x5 and 2x128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4185263489.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSTConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mmodel_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeriesTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4385781.py\u001b[0m in \u001b[0;36mtrain_transformer\u001b[0;34m(self, train_loader, val_loader, EPOCHS, LR, TF_START, TF_END)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m           \u001b[0;31m# TRAIN: One epoch with scheduled sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m           \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m           \u001b[0;31m# VALIDATION MODE 1: Teacher-forced (parallel, fast)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4385781.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, loader, train, p_tf)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m           \u001b[0;31m# FORWARD PASS: Autoregressive with scheduled sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m           \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduled_sampling_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m           \u001b[0;31m# COMPUTE LOSS: Mean Squared Error over all predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-678061439.py\u001b[0m in \u001b[0;36mscheduled_sampling_step\u001b[0;34m(model, Xb, ytrue, p_tf)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;31m# ENCODE HISTORY (once, doesn't change during generation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# [batch, history_len, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m     \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Encoder output (memory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2048x5 and 2x128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mae, rmse = model_transformer.predict_evaluation(test_loader)\n",
        "\n",
        "print(\"\\nForecast analysis:\")\n",
        "print(f\"  - MAE (Mean Absolute Error): {mae:.4f}\")\n",
        "print(f\"  - RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
        "print(f\"  - Forecast horizon: {T_Y} steps\")\n",
        "print(f\"  - History used: {T_X} steps\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBtpYUisWhQC",
        "outputId": "29354247-8553-42c8-f4ed-8ddf3f723282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forecast analysis:\n",
            "  - MAE (Mean Absolute Error): 0.0895\n",
            "  - RMSE (Root Mean Squared Error): 0.1097\n",
            "  - Forecast horizon: 8 steps\n",
            "  - History used: 32 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Transformer com Fourier"
      ],
      "metadata": {
        "id": "JlMaBKzWpiSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Camada de fourier"
      ],
      "metadata": {
        "id": "aDho-Dj73iYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FourierTransformLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Camada de Transformada de Fourier para análise de frequência temporal\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Camadas lineares para processar as partes real e imaginária separadamente\n",
        "        self.freq_linear_real = nn.Linear(d_model, d_model)\n",
        "        self.freq_linear_imag = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Parâmetros para normalização\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # Salvar o input original para resíduo\n",
        "        residual = x\n",
        "\n",
        "        # Realizar FFT ao longo da dimensão temporal (seq_len, dim=1)\n",
        "        # x_fft será [batch_size, seq_len_fft, d_model], complexo\n",
        "        # Onde seq_len_fft = seq_len // 2 + 1\n",
        "        x_fft = torch.fft.rfft(x, dim=1, norm='ortho')\n",
        "\n",
        "        # Separar parte real e imaginária\n",
        "        x_fft_real = x_fft.real  # [B, seq_len_fft, d_model]\n",
        "        x_fft_imag = x_fft.imag  # [B, seq_len_fft, d_model]\n",
        "\n",
        "        # Processar as partes real e imaginária com camadas lineares separadas\n",
        "        # As camadas lineares operam na última dimensão (d_model)\n",
        "        x_fft_real_processed = self.freq_linear_real(x_fft_real) # [B, seq_len_fft, d_model]\n",
        "        x_fft_imag_processed = self.freq_linear_imag(x_fft_imag) # [B, seq_len_fft, d_model]\n",
        "\n",
        "        # Reconstruir o sinal complexo processado\n",
        "        # Explicitly cast to float32 as torch.complex might not fully support bfloat16 inputs in some contexts.\n",
        "        x_fft_processed = torch.complex(x_fft_real_processed.float(), x_fft_imag_processed.float())\n",
        "\n",
        "        # Aplicar FFT inversa\n",
        "        # irfft espera a dimensão de frequência (dim=1 aqui) e reconstrói o seq_len original\n",
        "        x_reconstructed = torch.fft.irfft(x_fft_processed, n=seq_len, dim=1, norm='ortho')\n",
        "        # x_reconstructed será [B, seq_len, d_model]\n",
        "\n",
        "        # Aplicar dropout e adicionar resíduo\n",
        "        x_out = self.dropout(x_reconstructed)\n",
        "        x_out = self.norm(residual + x_out)\n",
        "\n",
        "        return x_out"
      ],
      "metadata": {
        "id": "ANnzgtOa3iKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definição"
      ],
      "metadata": {
        "id": "H1nByoDItH29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayerFourier(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Transformer encoder layer.\n",
        "\n",
        "    Architecture:\n",
        "      1. Multi-Head Self-Attention (each position attends to all positions)\n",
        "      2. Feedforward Network (process each position independently)\n",
        "\n",
        "    Both wrapped in PreNormResidual blocks for stability.\n",
        "\n",
        "    Purpose: Build representations that incorporate context from the entire sequence.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        num_heads: Number of attention heads\n",
        "        d_ff: Feedforward hidden dimension\n",
        "        attn_dropout: Dropout for attention\n",
        "        ff_dropout: Dropout for feedforward\n",
        "        activation: Activation function type\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, fft_drop_out=0.0, attn_dropout=0.0, ff_dropout=0.0, activation=\"gelu\"):\n",
        "        super().__init__()\n",
        "\n",
        "        #Fourier Layer\n",
        "        self.fft = PreNormResidual(\n",
        "            d_model,\n",
        "            FourierTransformLayer(d_model, fft_drop_out)\n",
        "        )\n",
        "\n",
        "\n",
        "        # Self-attention: sequence attends to itself\n",
        "        self.mha = PreNormResidual(\n",
        "            d_model,\n",
        "            MultiHeadAttention(d_model, num_heads, attn_dropout)\n",
        "        )\n",
        "        # Position-wise feedforward\n",
        "        self.ffn = PreNormResidual(\n",
        "            d_model,\n",
        "            FeedForward(d_model, d_ff, ff_dropout, activation)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input sequence [batch, seq_len, d_model]\n",
        "            key_padding_mask: Padding mask [batch, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            Encoded sequence [batch, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        #FFT\n",
        "        x = self.fft(x)\n",
        "        # Self-attention: q=k=v=x\n",
        "        x = self.mha(x, x, x, attn_mask=None, key_padding_mask=key_padding_mask)\n",
        "        # Feedforward\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderFourier(nn.Module):\n",
        "    \"\"\"\n",
        "    Stack of encoder layers + final layer normalization.\n",
        "\n",
        "    Purpose: Process the input sequence to build rich representations\n",
        "    that capture context and relationships.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        num_layers: Number of encoder layers to stack\n",
        "        num_heads: Number of attention heads per layer\n",
        "        d_ff: Feedforward hidden dimension\n",
        "        attn_dropout: Attention dropout\n",
        "        ff_dropout: Feedforward dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_layers, num_heads, d_ff, fft_dropout=0.0, attn_dropout=0.0, ff_dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayerFourier(d_model, num_heads, d_ff, fft_dropout, attn_dropout, ff_dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)  # Final normalization\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input embeddings [batch, seq_len, d_model]\n",
        "            key_padding_mask: Padding mask [batch, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            Encoded representations [batch, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, key_padding_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                       4) COMPLETE MODEL (ENC + DEC)\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TimeSeriesTransformerFourier(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Sequence-to-Sequence Transformer for Time Series Forecasting.\n",
        "\n",
        "    ARCHITECTURE OVERVIEW:\n",
        "      Input (history) → Encoder → Memory\n",
        "                                     ↓\n",
        "      Previous outputs → Decoder → Next prediction\n",
        "\n",
        "    KEY CONCEPTS:\n",
        "      1. ENCODER: Processes historical data (e.g., past 24 hours)\n",
        "         - Builds rich representations that capture patterns and context\n",
        "         - Output = \"memory\" that decoder can attend to\n",
        "\n",
        "      2. DECODER: Generates future predictions (e.g., next 12 hours)\n",
        "         - Autoregressively: generates one step at a time\n",
        "         - Uses both its own past predictions AND encoder memory\n",
        "\n",
        "      3. TEACHER FORCING (training): Feed ground-truth previous values to decoder\n",
        "         - Fast: can train all steps in parallel\n",
        "         - Problem: decoder never sees its own mistakes during training\n",
        "\n",
        "      4. AUTOREGRESSIVE GENERATION (inference): Feed model's own predictions\n",
        "         - Slow: must generate step-by-step\n",
        "         - Realistic: matches deployment scenario\n",
        "\n",
        "    EXAMPLE:\n",
        "      History: [hour 1, hour 2, ..., hour 24] → Encoder\n",
        "      Decoder generates: [hour 25, hour 26, ..., hour 36]\n",
        "        - To predict hour 26, decoder sees: hour 25 (either ground-truth or its prediction)\n",
        "        - To predict hour 27, decoder sees: hours 25-26\n",
        "        - And so on...\n",
        "\n",
        "    Args:\n",
        "        cfg: Configuration object with all hyperparameters\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: TSTConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # INPUT PROJECTIONS: Convert raw features to model dimension\n",
        "        self.enc_in = nn.Linear(cfg.input_dim, cfg.d_model)      # Encoder input projection\n",
        "        self.dec_in = nn.Linear(cfg.dec_input_dim, cfg.d_model)  # Decoder input projection\n",
        "\n",
        "        # POSITIONAL ENCODINGS: Add position information to embeddings\n",
        "        if cfg.pos_encoding == \"sin\":\n",
        "            self.pos_enc = SinusoidalPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "            self.pos_dec = SinusoidalPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "        else:\n",
        "            self.pos_enc = LearnedPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "            self.pos_dec = LearnedPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "\n",
        "        # ENCODER & DECODER STACKS\n",
        "        self.enc = EncoderFourier(cfg.d_model, cfg.num_encoder_layers, cfg.num_heads, cfg.d_ff,\n",
        "                           fft_dropout=cfg.dropout, attn_dropout=cfg.dropout, ff_dropout=cfg.dropout)\n",
        "        self.dec = Decoder(cfg.d_model, cfg.num_decoder_layers, cfg.num_heads, cfg.d_ff,\n",
        "                           attn_dropout=cfg.dropout, ff_dropout=cfg.dropout)\n",
        "\n",
        "        # OUTPUT HEAD: Project decoder output to prediction space\n",
        "        self.head = nn.Linear(cfg.d_model, cfg.target_dim)\n",
        "\n",
        "        # INPUT DROPOUT: Regularization on embeddings\n",
        "        self.drop_in = nn.Dropout(cfg.dropout)\n",
        "\n",
        "        # OPTIONAL OUTPUT ACTIVATION: Constrain predictions to a range\n",
        "        self.out_act = None\n",
        "        if cfg.output_activation == \"tanh\":\n",
        "            self.out_act = torch.tanh      # Range: [-1, 1]\n",
        "        elif cfg.output_activation == \"sigmoid\":\n",
        "            self.out_act = torch.sigmoid   # Range: [0, 1]\n",
        "\n",
        "    def forward(self, X, y_in, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Teacher-forced forward pass (for training).\n",
        "\n",
        "        TRAINING MODE: We give the decoder the ground-truth previous outputs.\n",
        "        This allows parallel computation (fast!) but creates train-test mismatch.\n",
        "\n",
        "        Flow:\n",
        "          1. Encode history: X → embeddings → encoder → memory\n",
        "          2. Decode with teacher forcing: y_in → embeddings → decoder → predictions\n",
        "          3. Project to output space: predictions → final forecasts\n",
        "\n",
        "        Args:\n",
        "            X: Historical features [batch, history_len, input_dim]\n",
        "               Example shape: [32, 24, 4] = 32 sequences, 24 timesteps, 4 features\n",
        "\n",
        "            y_in: Decoder inputs (shifted targets) [batch, forecast_len, dec_input_dim]\n",
        "                  IMPORTANT: y_in[0] = start token (often 0 or last observed value)\n",
        "                             y_in[1] = ground-truth target at step 0\n",
        "                             y_in[2] = ground-truth target at step 1\n",
        "                             ... (shifted right by 1)\n",
        "\n",
        "            src_key_padding_mask: Padding mask for encoder [batch, history_len]\n",
        "            tgt_key_padding_mask: Padding mask for decoder [batch, forecast_len]\n",
        "\n",
        "        Returns:\n",
        "            Predictions [batch, forecast_len, target_dim]\n",
        "\n",
        "        TEACHER FORCING VISUALIZATION:\n",
        "          Ground truth: [y₀, y₁, y₂, y₃]\n",
        "          Decoder input: [START, y₀, y₁, y₂]  (shifted right)\n",
        "          Predictions:   [ŷ₀, ŷ₁, ŷ₂, ŷ₃]\n",
        "          Loss: compare predictions to ground truth\n",
        "        \"\"\"\n",
        "        # ENCODE HISTORY\n",
        "        x = self.drop_in(self.pos_enc(self.enc_in(X)))  # Project → positional encoding → dropout\n",
        "        mem = self.enc(x, key_padding_mask=src_key_padding_mask)  # Run through encoder\n",
        "\n",
        "        # DECODE WITH TEACHER FORCING\n",
        "        y = self.drop_in(self.pos_dec(self.dec_in(y_in)))  # Process decoder inputs\n",
        "        self_mask = subsequent_mask(y.size(1)).to(y.device)  # Causal mask: can't see future\n",
        "        out = self.dec(y, mem, self_mask, tgt_key_padding_mask, src_key_padding_mask)\n",
        "\n",
        "        # PROJECT TO OUTPUT SPACE\n",
        "        logits = self.head(out)  # [batch, forecast_len, target_dim]\n",
        "\n",
        "        # Apply output activation if specified\n",
        "        return logits if self.out_act is None else self.out_act(logits)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, X, y_start, steps, src_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Autoregressive generation (for inference/deployment).\n",
        "\n",
        "        INFERENCE MODE: Decoder uses its own predictions (no ground truth available).\n",
        "        This is slow (sequential) but realistic for deployment.\n",
        "\n",
        "        Process:\n",
        "          1. Encode history once (doesn't change)\n",
        "          2. Start with initial token (y_start)\n",
        "          3. For each step:\n",
        "             a. Decode current sequence\n",
        "             b. Predict next value\n",
        "             c. Append prediction to sequence\n",
        "             d. Repeat\n",
        "\n",
        "        Args:\n",
        "            X: Historical features [batch, history_len, input_dim]\n",
        "            y_start: Initial decoder token [batch, 1, dec_input_dim]\n",
        "                     Often: last observed target value or a zero vector\n",
        "            steps: Number of future steps to predict\n",
        "            src_key_padding_mask: Padding mask for encoder\n",
        "\n",
        "        Returns:\n",
        "            Predictions [batch, steps, target_dim]\n",
        "\n",
        "        AUTOREGRESSIVE VISUALIZATION:\n",
        "          Step 1: [START]           → predict ŷ₀\n",
        "          Step 2: [START, ŷ₀]       → predict ŷ₁\n",
        "          Step 3: [START, ŷ₀, ŷ₁]   → predict ŷ₂\n",
        "          Step 4: [START, ŷ₀, ŷ₁, ŷ₂] → predict ŷ₃\n",
        "          ...\n",
        "\n",
        "        Note: Errors compound! If ŷ₀ is wrong, all future predictions are affected.\n",
        "        \"\"\"\n",
        "        device = X.device\n",
        "\n",
        "        # ENCODE HISTORY (once)\n",
        "        x = self.pos_enc(self.enc_in(X))\n",
        "        mem = self.enc(x, key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        preds = []\n",
        "        y_seq = y_start  # Start with initial token; will grow each step\n",
        "\n",
        "        # AUTOREGRESSIVE LOOP\n",
        "        for _ in range(steps):\n",
        "            # Decode current sequence\n",
        "            y_dec = self.pos_dec(self.dec_in(y_seq))\n",
        "            self_mask = subsequent_mask(y_dec.size(1)).to(device)\n",
        "            out = self.dec(y_dec, mem, self_mask, None, src_key_padding_mask)\n",
        "\n",
        "            # Predict next value (only use last position's output)\n",
        "            next_logits = self.head(out[:, -1:, :])  # [batch, 1, target_dim]\n",
        "            next_value = next_logits if self.out_act is None else self.out_act(next_logits)\n",
        "            preds.append(next_value)\n",
        "\n",
        "            # APPEND PREDICTION TO SEQUENCE (for next iteration)\n",
        "            # Handle dimension mismatch if dec_input_dim != target_dim\n",
        "            if self.cfg.dec_input_dim == self.cfg.target_dim:\n",
        "                y_seq = torch.cat([y_seq, next_value], dim=1)\n",
        "            else:\n",
        "                if self.cfg.dec_input_dim > self.cfg.target_dim:\n",
        "                    # Pad prediction to match decoder input size\n",
        "                    pad = torch.zeros(X.size(0), 1, self.cfg.dec_input_dim - self.cfg.target_dim, device=device)\n",
        "                    y_seq = torch.cat([y_seq, torch.cat([next_value, pad], dim=-1)], dim=1)\n",
        "                else:\n",
        "                    # Truncate prediction to match decoder input size\n",
        "                    y_seq = torch.cat([y_seq, next_value[..., :self.cfg.dec_input_dim]], dim=1)\n",
        "\n",
        "        return torch.cat(preds, dim=1)  # [batch, steps, target_dim]\n",
        "\n",
        "\n",
        "    def run_epoch(self, loader, train: bool, p_tf: float):\n",
        "      \"\"\"\n",
        "      Run one epoch of training or evaluation.\n",
        "\n",
        "      Args:\n",
        "          loader: DataLoader (train or validation)\n",
        "          train: If True, update weights; if False, just evaluate\n",
        "          p_tf: Teacher forcing probability for scheduled sampling\n",
        "\n",
        "      Returns:\n",
        "          Average loss over the epoch\n",
        "      \"\"\"\n",
        "      self.train(train)  # Set mode (affects dropout, batchnorm, etc.)\n",
        "      total_loss = 0.0\n",
        "      n_samples = 0\n",
        "\n",
        "      for Xb_cpu, _, ytrue_cpu in loader:\n",
        "          # MOVE TO DEVICE (non_blocking=True allows async transfer)\n",
        "          Xb = Xb_cpu.to(DEVICE, non_blocking=True).float()        # [batch, T_x, features]\n",
        "          ytrue = ytrue_cpu.to(DEVICE, non_blocking=True).float()  # [batch, T_y, 1]\n",
        "\n",
        "          # ZERO GRADIENTS (set_to_none=True is faster than zero_grad())\n",
        "          self.opt.zero_grad(set_to_none=True)\n",
        "\n",
        "          # FORWARD PASS: Autoregressive with scheduled sampling\n",
        "          pred = scheduled_sampling_step(self, Xb, ytrue, p_tf=p_tf)\n",
        "\n",
        "          # COMPUTE LOSS: Mean Squared Error over all predictions\n",
        "          loss = F.mse_loss(pred, ytrue)  # Average over batch, steps, dimensions\n",
        "\n",
        "          # BACKWARD PASS (only in training mode)\n",
        "          if train:\n",
        "              if self.scaler.is_enabled():\n",
        "                  # MIXED PRECISION TRAINING\n",
        "                  self.scaler.scale(loss).backward()           # Scale loss to prevent underflow\n",
        "                  nn.utils.clip_grad_norm_(self.parameters(), 1.0)  # Clip gradients\n",
        "                  self.scaler.step(self.opt)                        # Update weights\n",
        "                  self.scaler.update()                         # Update scale factor\n",
        "              else:\n",
        "                  # STANDARD TRAINING\n",
        "                  loss.backward()\n",
        "                  nn.utils.clip_grad_norm_(self.parameters(), 1.0)  # Prevent exploding gradients\n",
        "                  self.opt.step()\n",
        "\n",
        "          # ACCUMULATE LOSS (weighted by batch size for proper averaging)\n",
        "          total_loss += loss.item() * Xb.size(0)\n",
        "          n_samples += Xb.size(0)\n",
        "\n",
        "      return total_loss / max(1, n_samples)  # Average loss per sample\n",
        "\n",
        "\n",
        "\n",
        "    def train_transformer(self, train_loader, val_loader,\n",
        "      EPOCHS=8,\n",
        "      LR=1e-3,\n",
        "      TF_START=0.9,                 # Initial teacher forcing probability\n",
        "      TF_END=0.5                    # Final teacher forcing probability\n",
        "      ):\n",
        "      \"\"\"\n",
        "      Complete training pipeline with scheduled sampling.\n",
        "\n",
        "      TRAINING STRATEGY:\n",
        "        1. Start with high teacher forcing (p_tf=0.9): easy learning\n",
        "        2. Gradually reduce (linear schedule to p_tf=0.5): harder learning\n",
        "        3. Validate with both teacher-forced and free-running modes\n",
        "\n",
        "      WHY TWO VALIDATION METRICS?\n",
        "        - val_tf (teacher-forced): Fast, optimistic (what model could achieve)\n",
        "        - val_free (free-running): Slow, realistic (actual deployment performance)\n",
        "        - Gap between them shows exposure bias\n",
        "\n",
        "      OPTIMIZATION:\n",
        "        - AdamW optimizer: Adam + weight decay (prevents overfitting)\n",
        "        - Cosine annealing: learning rate decreases smoothly\n",
        "        - Gradient clipping: prevents exploding gradients\n",
        "        - Mixed precision (AMP): faster training on GPU\n",
        "\n",
        "      Args:\n",
        "          EPOCHS: Number of training epochs\n",
        "          N_SAMPLES: Number of time series to generate\n",
        "          SERIES_LENGTH: Length of each series\n",
        "          T_X: History window size\n",
        "          T_Y: Forecast horizon\n",
        "          BATCH: Batch size\n",
        "          LR: Initial learning rate\n",
        "          TF_START: Starting teacher forcing probability\n",
        "          TF_END: Ending teacher forcing probability\n",
        "\n",
        "      Returns:\n",
        "          model: Trained model\n",
        "          loaders: (train_loader, val_loader)\n",
        "          history: Training history dict\n",
        "          data: (X, y, stats) for visualization\n",
        "          windows: (T_X, T_Y) window sizes\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Count parameters\n",
        "      n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "      print(f\"Model parameters: {n_params:,}\")\n",
        "      print(f\"Device: {DEVICE}\")\n",
        "      print(f\"Mixed precision: {'BFloat16' if USE_BF16 else 'Float16' if USE_AMP else 'Float32'}\\n\")\n",
        "\n",
        "      # OPTIMIZER: AdamW with weight decay (L2 regularization)\n",
        "      self.opt = torch.optim.AdamW(self.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "      # SCHEDULER: Cosine annealing (smooth LR decay)\n",
        "      sched = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt, T_max=EPOCHS)\n",
        "\n",
        "      # GRADIENT SCALER: For mixed precision with float16 (not needed for bfloat16)\n",
        "      self.scaler = torch.amp.GradScaler(device=\"cuda\", enabled=SCALER_ENABLED)\n",
        "\n",
        "\n",
        "      print(\"=\" * 70)\n",
        "      print(\"TRAINING\")\n",
        "      print(\"=\" * 70)\n",
        "\n",
        "      # --- TRAINING LOOP ---\n",
        "      # Track three metrics:\n",
        "      #   1. train: training loss with scheduled sampling\n",
        "      #   2. val_tf: validation loss with teacher forcing (optimistic)\n",
        "      #   3. val_free: validation loss with free-running (realistic)\n",
        "      history = {\"train\": [], \"val_tf\": [], \"val_free\": []}\n",
        "\n",
        "      for ep in range(1, EPOCHS+1):\n",
        "          # TEACHER FORCING SCHEDULE: Linear decay from TF_START to TF_END\n",
        "          alpha = (ep - 1) / max(1, EPOCHS - 1)        # 0 → 1 progress\n",
        "          p_tf = TF_START * (1 - alpha) + TF_END * alpha\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # TRAIN: One epoch with scheduled sampling\n",
        "          train_loss = self.run_epoch(train_loader, train=True, p_tf=p_tf)\n",
        "\n",
        "          # VALIDATION MODE 1: Teacher-forced (parallel, fast)\n",
        "          # Gives us a sense of model capacity (if it had perfect inputs)\n",
        "          self.train(False)\n",
        "          val_tf_total, val_n = 0.0, 0\n",
        "\n",
        "          for Xb_cpu, y_in_cpu, ytrue_cpu in val_loader:\n",
        "              Xb = Xb_cpu.to(DEVICE, non_blocking=True).float()\n",
        "              y_in = y_in_cpu.to(DEVICE, non_blocking=True).float()  # Pre-shifted targets\n",
        "              ytrue = ytrue_cpu.to(DEVICE, non_blocking=True).float()\n",
        "\n",
        "              # Teacher-forced forward pass (all steps in parallel)\n",
        "              with torch.amp.autocast(\"cuda\", enabled=USE_AMP, dtype=AMP_DTYPE):\n",
        "                  pred_tf = self(Xb, y_in)           # [batch, T_y, 1]\n",
        "\n",
        "              loss_tf = F.mse_loss(pred_tf.float(), ytrue)\n",
        "              val_tf_total += loss_tf.item() * Xb.size(0)\n",
        "              val_n += Xb.size(0)\n",
        "\n",
        "          val_tf = val_tf_total / max(1, val_n)\n",
        "\n",
        "          # VALIDATION MODE 2: Free-running (autoregressive, slow)\n",
        "          # This is the REAL test: how well does model perform in deployment?\n",
        "          val_free_total, val_n = 0.0, 0\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for Xb_cpu, _, ytrue_cpu in val_loader:\n",
        "                  Xb = Xb_cpu.to(DEVICE, non_blocking=True).float()\n",
        "                  ytrue = ytrue_cpu.to(DEVICE, non_blocking=True).float()\n",
        "\n",
        "                  # Seed with first ground-truth value (in practice: last observed target)\n",
        "                  y_start = ytrue[:, :1, :]\n",
        "\n",
        "                  # Generate predictions autoregressively\n",
        "                  pred_free = self.generate(Xb, y_start=y_start, steps=T_Y).float()\n",
        "\n",
        "                  loss_free = F.mse_loss(pred_free, ytrue)\n",
        "                  val_free_total += loss_free.item() * Xb.size(0)\n",
        "                  val_n += Xb.size(0)\n",
        "\n",
        "          val_free = val_free_total / max(1, val_n)\n",
        "\n",
        "          # UPDATE LEARNING RATE\n",
        "          sched.step()\n",
        "\n",
        "          # PRINT PROGRESS\n",
        "          print(f\"Epoch {ep:02d} | p_tf={p_tf:.2f} | \"\n",
        "                f\"train={train_loss:.4f} | val_tf={val_tf:.4f} | val_free={val_free:.4f} | \"\n",
        "                f\"lr={sched.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "          # RECORD HISTORY\n",
        "          history[\"train\"].append(train_loss)\n",
        "          history[\"val_tf\"].append(val_tf)\n",
        "          history[\"val_free\"].append(val_free)\n",
        "\n",
        "      print(\"\\n\" + \"=\" * 70)\n",
        "      print(\"TRAINING COMPLETE\")\n",
        "      print(\"=\" * 70)\n",
        "      print(f\"Final train loss: {history['train'][-1]:.4f}\")\n",
        "      print(f\"Final val_tf loss: {history['val_tf'][-1]:.4f}\")\n",
        "      print(f\"Final val_free loss: {history['val_free'][-1]:.4f}\")\n",
        "      print(f\"Exposure bias gap: {history['val_free'][-1] - history['val_tf'][-1]:.4f}\")\n",
        "      print(\"(Gap = how much worse free-running is than teacher-forced)\\n\")\n",
        "\n",
        "      return history\n",
        "    def predict_evaluation(self, test_loader):\n",
        "\n",
        "      DEVICE = next(self.parameters()).device\n",
        "      self.eval()  # Set to evaluation mode\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # Get one batch from validation set\n",
        "          Xb_cpu, _, ytrue_cpu = next(iter(test_loader))\n",
        "\n",
        "          # Use only first sample for visualization\n",
        "          Xb = Xb_cpu[:1].to(DEVICE).float()      # [1, T_x, features]\n",
        "          ytrue = ytrue_cpu[:1].to(DEVICE).float()  # [1, T_y, 1]\n",
        "\n",
        "          # Generate forecast (autoregressive)\n",
        "          y_start = ytrue[:, :1, :]  # Seed with first ground-truth value\n",
        "          pred = self.generate(Xb, y_start=y_start, steps=T_Y).float()\n",
        "\n",
        "      # Convert to numpy for plotting\n",
        "      gt = ytrue[0, :, 0].detach().cpu().numpy()  # Ground truth\n",
        "      pr = pred[0, :, 0].detach().cpu().numpy()   # Prediction\n",
        "\n",
        "      # Compute error metrics\n",
        "      mae = np.mean(np.abs(pr - gt))              # Mean Absolute Error\n",
        "      rmse = np.sqrt(np.mean((pr - gt) ** 2))     # Root Mean Squared Error\n",
        "\n",
        "      return mae, rmse\n"
      ],
      "metadata": {
        "id": "fcQc3SNR3N0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chamada"
      ],
      "metadata": {
        "id": "3gbGPjRttTY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TSTConfig:\n",
        "    \"\"\"\n",
        "    Configuration for Time Series Transformer.\n",
        "\n",
        "    Architecture hyperparameters:\n",
        "        d_model: Hidden dimension throughout the model (like \"brain size\")\n",
        "        num_encoder_layers: Depth of encoder (more = more context integration)\n",
        "        num_decoder_layers: Depth of decoder (fewer than encoder is common)\n",
        "        num_heads: Parallel attention operations (more = capture more relationships)\n",
        "        d_ff: Feedforward expansion dimension (typically 4*d_model)\n",
        "        dropout: Regularization strength (0.0 = none, 0.3 = aggressive)\n",
        "\n",
        "    Input/output specification:\n",
        "        input_dim: Number of input features (e.g., temperature, pressure, etc.)\n",
        "        target_dim: Number of values to predict (often 1 for univariate forecasting)\n",
        "        dec_input_dim: Decoder input size (should match target_dim for simplicity)\n",
        "\n",
        "    Other settings:\n",
        "        pos_encoding: \"sin\" (fixed) or \"learned\" (trainable)\n",
        "        max_len: Maximum sequence length\n",
        "        output_activation: Optional final activation (\"tanh\", \"sigmoid\", or None)\n",
        "    \"\"\"\n",
        "    d_model: int = 128\n",
        "    num_encoder_layers: int = 3\n",
        "    num_decoder_layers: int = 2\n",
        "    num_heads: int = 8\n",
        "    d_ff: int = 256\n",
        "    dropout: float = 0.1\n",
        "    pos_encoding: str = \"sin\"  # \"sin\" or \"learned\"\n",
        "    max_len: int = 4096\n",
        "    input_dim: int = 2         # number of historical features\n",
        "    target_dim: int = 1        # we forecast a single target value\n",
        "    dec_input_dim: int = 1     # decoder input size (shifted target)\n",
        "    output_activation: Optional[str] = None  # e.g., \"tanh\" for bounded target\n",
        "\n",
        "# --- MODEL + OPTIMIZER ---\n",
        "cfg = TSTConfig()\n",
        "model_fft = TimeSeriesTransformerFourier(cfg).to(DEVICE)\n",
        "history = model_fft.train_transformer(train_loader, val_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doumdHVG9mMq",
        "outputId": "262b1780-2aab-48d0-e41b-bafd0cad49cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 896,897\n",
            "Device: cuda\n",
            "Mixed precision: BFloat16\n",
            "\n",
            "======================================================================\n",
            "TRAINING\n",
            "======================================================================\n",
            "Epoch 01 | p_tf=0.90 | train=0.3905 | val_tf=0.2054 | val_free=0.0807 | lr=9.62e-04\n",
            "Epoch 02 | p_tf=0.84 | train=0.0399 | val_tf=0.1697 | val_free=0.0892 | lr=8.54e-04\n",
            "Epoch 03 | p_tf=0.79 | train=0.0259 | val_tf=0.1622 | val_free=0.0456 | lr=6.91e-04\n",
            "Epoch 04 | p_tf=0.73 | train=0.0215 | val_tf=0.1535 | val_free=0.0775 | lr=5.00e-04\n",
            "Epoch 05 | p_tf=0.67 | train=0.0180 | val_tf=0.1598 | val_free=0.1282 | lr=3.09e-04\n",
            "Epoch 06 | p_tf=0.61 | train=0.0204 | val_tf=0.1443 | val_free=0.0364 | lr=1.46e-04\n",
            "Epoch 07 | p_tf=0.56 | train=0.0183 | val_tf=0.1475 | val_free=0.0394 | lr=3.81e-05\n",
            "Epoch 08 | p_tf=0.50 | train=0.0186 | val_tf=0.1445 | val_free=0.0340 | lr=0.00e+00\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE\n",
            "======================================================================\n",
            "Final train loss: 0.0186\n",
            "Final val_tf loss: 0.1445\n",
            "Final val_free loss: 0.0340\n",
            "Exposure bias gap: -0.1105\n",
            "(Gap = how much worse free-running is than teacher-forced)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mae, rmse = model_fft.predict_evaluation(test_loader)\n",
        "\n",
        "print(\"\\nForecast analysis:\")\n",
        "print(f\"  - MAE (Mean Absolute Error): {mae:.4f}\")\n",
        "print(f\"  - RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
        "print(f\"  - Forecast horizon: {T_Y} steps\")\n",
        "print(f\"  - History used: {T_X} steps\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iMOtP5oBzxu",
        "outputId": "c539900b-271d-42b2-8e35-00136edde53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forecast analysis:\n",
            "  - MAE (Mean Absolute Error): 0.1055\n",
            "  - RMSE (Root Mean Squared Error): 0.1246\n",
            "  - Forecast horizon: 8 steps\n",
            "  - History used: 32 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Sliding Window"
      ],
      "metadata": {
        "id": "2B6BNPwcjHx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definição da Atenção"
      ],
      "metadata": {
        "id": "kWFVQNSljV-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SlidingWindownMultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention: the core mechanism that makes Transformers powerful.\n",
        "\n",
        "    KEY INTUITION:\n",
        "        Attention lets each position \"look at\" other positions and decide\n",
        "        what information to pull from them. Multi-head attention runs\n",
        "        several attention operations in parallel (different \"heads\"),\n",
        "        each potentially learning different relationships.\n",
        "\n",
        "    Example: In \"The cat sat on the mat\", when processing \"sat\":\n",
        "      - Head 1 might focus on \"cat\" (subject)\n",
        "      - Head 2 might focus on \"mat\" (object)\n",
        "      - Head 3 might focus on \"on\" (preposition)\n",
        "\n",
        "    MECHANISM:\n",
        "      1. Project inputs to Query (Q), Key (K), Value (V) spaces\n",
        "      2. Split into multiple heads (num_heads parallel attention operations)\n",
        "      3. Compute attention scores: how much should each position attend to others?\n",
        "         score(q, k) = (q · k) / sqrt(d_head)  [scaled dot-product]\n",
        "      4. Apply softmax to get attention weights (sum to 1)\n",
        "      5. Weighted sum of Values: output = Σ attention_weight * value\n",
        "      6. Concatenate heads and project back to d_model\n",
        "\n",
        "    Args:\n",
        "        d_model: Total model dimension (must be divisible by num_heads)\n",
        "        num_heads: Number of parallel attention operations\n",
        "        dropout: Dropout applied to attention weights\n",
        "        bias: Whether to use bias in linear projections\n",
        "        window_size: Size of sliding window for attention (None = full attention)\n",
        "                    Each position can only attend to positions within ±window_size\n",
        "                    Reduces complexity from O(n²) to O(n × window_size)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0,\n",
        "                 bias: bool = True, window_size: int = None):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.h = num_heads\n",
        "        self.d_head = d_model // num_heads  # Dimension per head\n",
        "        self.window_size = window_size  # NEW: sliding window parameter\n",
        "\n",
        "        # Linear projections for Q, K, V (one per input role)\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "\n",
        "        # Output projection (combine all heads)\n",
        "        self.o_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def _create_window_mask(self, seq_len_q: int, seq_len_k: int, device: torch.device) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Create sliding window attention mask.\n",
        "\n",
        "        Each position i can only attend to positions in [i-window_size, i+window_size]\n",
        "        Positions outside this range are masked with True.\n",
        "\n",
        "        Args:\n",
        "            seq_len_q: Query sequence length\n",
        "            seq_len_k: Key sequence length\n",
        "            device: Device for tensor creation\n",
        "\n",
        "        Returns:\n",
        "            Window mask tensor [1, 1, seq_len_q, seq_len_k] with True for masked positions\n",
        "        \"\"\"\n",
        "        # Create a boolean mask initialized to True (mask all)\n",
        "        mask = torch.ones((seq_len_q, seq_len_k), dtype=torch.bool, device=device)\n",
        "\n",
        "        # For each query position, allow attention to positions within window\n",
        "        for i in range(seq_len_q):\n",
        "            start = max(0, i - self.window_size)\n",
        "            end = min(seq_len_k, i + self.window_size + 1)\n",
        "            # Set positions within the window to False (don't mask)\n",
        "            mask[i, start:end] = False\n",
        "\n",
        "        # Reshape to match attention scores shape\n",
        "        return mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    def _split_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Split the embedding dimension into multiple heads.\n",
        "\n",
        "        Reshape: [B, T, D] -> [B, T, H, D/H] -> [B, H, T, D/H]\n",
        "\n",
        "        Why? We want to compute H separate attention operations in parallel.\n",
        "        Moving H to dimension 1 allows efficient batched matrix operations.\n",
        "        \"\"\"\n",
        "        B, T, _ = x.shape\n",
        "        return x.view(B, T, self.h, self.d_head).permute(0, 2, 1, 3)\n",
        "\n",
        "    def _combine_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Merge multiple heads back into single embedding.\n",
        "\n",
        "        Reshape: [B, H, T, D/H] -> [B, T, H, D/H] -> [B, T, D]\n",
        "\n",
        "        Concatenates all head outputs into the original d_model dimension.\n",
        "        \"\"\"\n",
        "        B, H, T, Dh = x.shape\n",
        "        return x.permute(0, 2, 1, 3).contiguous().view(B, T, H * Dh)\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask=None, key_padding_mask=None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute multi-head attention.\n",
        "\n",
        "        Args:\n",
        "            q: Query tensor [batch, seq_len_q, d_model]\n",
        "            k: Key tensor [batch, seq_len_k, d_model]\n",
        "            v: Value tensor [batch, seq_len_k, d_model]\n",
        "            attn_mask: Attention mask [1, 1, seq_len_q, seq_len_k] (e.g., causal mask)\n",
        "            key_padding_mask: Padding mask [batch, seq_len_k] (True = ignore this position)\n",
        "\n",
        "        Returns:\n",
        "            Attention output [batch, seq_len_q, d_model]\n",
        "\n",
        "        Note: For self-attention, q=k=v (same sequence attends to itself)\n",
        "              For cross-attention, q comes from decoder, k=v from encoder\n",
        "        \"\"\"\n",
        "        # Project and split into heads\n",
        "        Q = self._split_heads(self.q_proj(q))  # [B, H, T_q, D/H]\n",
        "        K = self._split_heads(self.k_proj(k))  # [B, H, T_k, D/H]\n",
        "        V = self._split_heads(self.v_proj(v))  # [B, H, T_k, D/H]\n",
        "\n",
        "        # SCALED DOT-PRODUCT ATTENTION\n",
        "        # Compute attention scores: how much should each query attend to each key?\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        # Shape: [B, H, T_q, T_k]\n",
        "        # Scaling by sqrt(d_head) prevents softmax saturation (gradients vanish)\n",
        "\n",
        "        # Use finfo.min instead of -inf for numerical stability in half precision\n",
        "        neg_inf = torch.finfo(scores.dtype).min\n",
        "\n",
        "        # NEW: APPLY SLIDING WINDOW MASK (if window_size is specified)\n",
        "        # Applied BEFORE other masks to properly restrict attention window\n",
        "        if self.window_size is not None:\n",
        "            seq_len_q, seq_len_k = scores.shape[-2], scores.shape[-1]\n",
        "            # Only apply window mask if sequence is longer than 2*window_size+1\n",
        "            if seq_len_k > (self.window_size * 2 + 1):\n",
        "                window_mask = self._create_window_mask(seq_len_q, seq_len_k, scores.device)\n",
        "                scores = scores.masked_fill(window_mask, neg_inf)\n",
        "\n",
        "        # APPLY MASKS\n",
        "        # Key padding mask: ignore padded positions in the key sequence\n",
        "        if key_padding_mask is not None:\n",
        "            kpm = merge_padding_mask(scores.shape, key_padding_mask)\n",
        "            scores = scores.masked_fill(kpm, neg_inf)  # Set to very negative -> softmax ~0\n",
        "\n",
        "        # Attention mask: prevent attending to certain positions (e.g., future tokens)\n",
        "        if attn_mask is not None:\n",
        "            if attn_mask.device != scores.device:\n",
        "                attn_mask = attn_mask.to(scores.device)\n",
        "            scores = scores.masked_fill(attn_mask, neg_inf)\n",
        "\n",
        "        # Compute attention weights (softmax over key dimension)\n",
        "        attn = torch.softmax(scores, dim=-1)  # [B, H, T_q, T_k]\n",
        "        attn = self.drop(attn)  # Dropout for regularization\n",
        "\n",
        "        # Weighted sum of values\n",
        "        out = torch.matmul(attn, V)  # [B, H, T_q, D/H]\n",
        "\n",
        "        # Combine heads and project\n",
        "        out = self._combine_heads(out)  # [B, T_q, D]\n",
        "        return self.o_proj(out)"
      ],
      "metadata": {
        "id": "Q3YXaDIqjNLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definição"
      ],
      "metadata": {
        "id": "-MRjtpsykq8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayerSlidingWindown(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Transformer encoder layer.\n",
        "\n",
        "    Architecture:\n",
        "      1. Multi-Head Self-Attention (each position attends to all positions)\n",
        "      2. Feedforward Network (process each position independently)\n",
        "\n",
        "    Both wrapped in PreNormResidual blocks for stability.\n",
        "\n",
        "    Purpose: Build representations that incorporate context from the entire sequence.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        num_heads: Number of attention heads\n",
        "        d_ff: Feedforward hidden dimension\n",
        "        attn_dropout: Dropout for attention\n",
        "        ff_dropout: Dropout for feedforward\n",
        "        activation: Activation function type\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, attn_dropout=0.0, ff_dropout=0.0, activation=\"gelu\", window_size=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # Self-attention: sequence attends to itself\n",
        "        self.mha = PreNormResidual(\n",
        "            d_model,\n",
        "            SlidingWindownMultiHeadAttention(d_model, num_heads, attn_dropout, window_size=window_size)\n",
        "        )\n",
        "        # Position-wise feedforward\n",
        "        self.ffn = PreNormResidual(\n",
        "            d_model,\n",
        "            FeedForward(d_model, d_ff, ff_dropout, activation)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input sequence [batch, seq_len, d_model]\n",
        "            key_padding_mask: Padding mask [batch, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            Encoded sequence [batch, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # Self-attention: q=k=v=x\n",
        "        x = self.mha(x, x, x, attn_mask=None, key_padding_mask=key_padding_mask)\n",
        "        # Feedforward\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderSlidingWindown(nn.Module):\n",
        "    \"\"\"\n",
        "    Stack of encoder layers + final layer normalization.\n",
        "\n",
        "    Purpose: Process the input sequence to build rich representations\n",
        "    that capture context and relationships.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        num_layers: Number of encoder layers to stack\n",
        "        num_heads: Number of attention heads per layer\n",
        "        d_ff: Feedforward hidden dimension\n",
        "        attn_dropout: Attention dropout\n",
        "        ff_dropout: Feedforward dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_layers, num_heads, d_ff, attn_dropout=0.0, ff_dropout=0.0, sliding_window_size=None ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayerSlidingWindown(d_model, num_heads, d_ff, attn_dropout, ff_dropout, window_size=sliding_window_size)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)  # Final normalization\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input embeddings [batch, seq_len, d_model]\n",
        "            key_padding_mask: Padding mask [batch, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            Encoded representations [batch, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, key_padding_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                       4) COMPLETE MODEL (ENC + DEC)\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TimeSeriesTransformerSlidingWindown(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Sequence-to-Sequence Transformer for Time Series Forecasting.\n",
        "\n",
        "    ARCHITECTURE OVERVIEW:\n",
        "      Input (history) → Encoder → Memory\n",
        "                                     ↓\n",
        "      Previous outputs → Decoder → Next prediction\n",
        "\n",
        "    KEY CONCEPTS:\n",
        "      1. ENCODER: Processes historical data (e.g., past 24 hours)\n",
        "         - Builds rich representations that capture patterns and context\n",
        "         - Output = \"memory\" that decoder can attend to\n",
        "\n",
        "      2. DECODER: Generates future predictions (e.g., next 12 hours)\n",
        "         - Autoregressively: generates one step at a time\n",
        "         - Uses both its own past predictions AND encoder memory\n",
        "\n",
        "      3. TEACHER FORCING (training): Feed ground-truth previous values to decoder\n",
        "         - Fast: can train all steps in parallel\n",
        "         - Problem: decoder never sees its own mistakes during training\n",
        "\n",
        "      4. AUTOREGRESSIVE GENERATION (inference): Feed model's own predictions\n",
        "         - Slow: must generate step-by-step\n",
        "         - Realistic: matches deployment scenario\n",
        "\n",
        "    EXAMPLE:\n",
        "      History: [hour 1, hour 2, ..., hour 24] → Encoder\n",
        "      Decoder generates: [hour 25, hour 26, ..., hour 36]\n",
        "        - To predict hour 26, decoder sees: hour 25 (either ground-truth or its prediction)\n",
        "        - To predict hour 27, decoder sees: hours 25-26\n",
        "        - And so on...\n",
        "\n",
        "    Args:\n",
        "        cfg: Configuration object with all hyperparameters\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: TSTConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # INPUT PROJECTIONS: Convert raw features to model dimension\n",
        "        self.enc_in = nn.Linear(cfg.input_dim, cfg.d_model)      # Encoder input projection\n",
        "        self.dec_in = nn.Linear(cfg.dec_input_dim, cfg.d_model)  # Decoder input projection\n",
        "\n",
        "        # POSITIONAL ENCODINGS: Add position information to embeddings\n",
        "        if cfg.pos_encoding == \"sin\":\n",
        "            self.pos_enc = SinusoidalPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "            self.pos_dec = SinusoidalPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "        else:\n",
        "            self.pos_enc = LearnedPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "            self.pos_dec = LearnedPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "\n",
        "        # ENCODER & DECODER STACKS\n",
        "        self.enc = EncoderSlidingWindown(cfg.d_model, cfg.num_encoder_layers, cfg.num_heads, cfg.d_ff,\n",
        "                           attn_dropout=cfg.dropout, ff_dropout=cfg.dropout, sliding_window_size=cfg.sliding_window_size)\n",
        "        self.dec = Decoder(cfg.d_model, cfg.num_decoder_layers, cfg.num_heads, cfg.d_ff,\n",
        "                           attn_dropout=cfg.dropout, ff_dropout=cfg.dropout)\n",
        "\n",
        "        # OUTPUT HEAD: Project decoder output to prediction space\n",
        "        self.head = nn.Linear(cfg.d_model, cfg.target_dim)\n",
        "\n",
        "        # INPUT DROPOUT: Regularization on embeddings\n",
        "        self.drop_in = nn.Dropout(cfg.dropout)\n",
        "\n",
        "        # OPTIONAL OUTPUT ACTIVATION: Constrain predictions to a range\n",
        "        self.out_act = None\n",
        "        if cfg.output_activation == \"tanh\":\n",
        "            self.out_act = torch.tanh      # Range: [-1, 1]\n",
        "        elif cfg.output_activation == \"sigmoid\":\n",
        "            self.out_act = torch.sigmoid   # Range: [0, 1]\n",
        "\n",
        "    def forward(self, X, y_in, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Teacher-forced forward pass (for training).\n",
        "\n",
        "        TRAINING MODE: We give the decoder the ground-truth previous outputs.\n",
        "        This allows parallel computation (fast!) but creates train-test mismatch.\n",
        "\n",
        "        Flow:\n",
        "          1. Encode history: X → embeddings → encoder → memory\n",
        "          2. Decode with teacher forcing: y_in → embeddings → decoder → predictions\n",
        "          3. Project to output space: predictions → final forecasts\n",
        "\n",
        "        Args:\n",
        "            X: Historical features [batch, history_len, input_dim]\n",
        "               Example shape: [32, 24, 4] = 32 sequences, 24 timesteps, 4 features\n",
        "\n",
        "            y_in: Decoder inputs (shifted targets) [batch, forecast_len, dec_input_dim]\n",
        "                  IMPORTANT: y_in[0] = start token (often 0 or last observed value)\n",
        "                             y_in[1] = ground-truth target at step 0\n",
        "                             y_in[2] = ground-truth target at step 1\n",
        "                             ... (shifted right by 1)\n",
        "\n",
        "            src_key_padding_mask: Padding mask for encoder [batch, history_len]\n",
        "            tgt_key_padding_mask: Padding mask for decoder [batch, forecast_len]\n",
        "\n",
        "        Returns:\n",
        "            Predictions [batch, forecast_len, target_dim]\n",
        "\n",
        "        TEACHER FORCING VISUALIZATION:\n",
        "          Ground truth: [y₀, y₁, y₂, y₃]\n",
        "          Decoder input: [START, y₀, y₁, y₂]  (shifted right)\n",
        "          Predictions:   [ŷ₀, ŷ₁, ŷ₂, ŷ₃]\n",
        "          Loss: compare predictions to ground truth\n",
        "        \"\"\"\n",
        "        # ENCODE HISTORY\n",
        "        x = self.drop_in(self.pos_enc(self.enc_in(X)))  # Project → positional encoding → dropout\n",
        "        mem = self.enc(x, key_padding_mask=src_key_padding_mask)  # Run through encoder\n",
        "\n",
        "        # DECODE WITH TEACHER FORCING\n",
        "        y = self.drop_in(self.pos_dec(self.dec_in(y_in)))  # Process decoder inputs\n",
        "        self_mask = subsequent_mask(y.size(1)).to(y.device)  # Causal mask: can't see future\n",
        "        out = self.dec(y, mem, self_mask, tgt_key_padding_mask, src_key_padding_mask)\n",
        "\n",
        "        # PROJECT TO OUTPUT SPACE\n",
        "        logits = self.head(out)  # [batch, forecast_len, target_dim]\n",
        "\n",
        "        # Apply output activation if specified\n",
        "        return logits if self.out_act is None else self.out_act(logits)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, X, y_start, steps, src_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Autoregressive generation (for inference/deployment).\n",
        "\n",
        "        INFERENCE MODE: Decoder uses its own predictions (no ground truth available).\n",
        "        This is slow (sequential) but realistic for deployment.\n",
        "\n",
        "        Process:\n",
        "          1. Encode history once (doesn't change)\n",
        "          2. Start with initial token (y_start)\n",
        "          3. For each step:\n",
        "             a. Decode current sequence\n",
        "             b. Predict next value\n",
        "             c. Append prediction to sequence\n",
        "             d. Repeat\n",
        "\n",
        "        Args:\n",
        "            X: Historical features [batch, history_len, input_dim]\n",
        "            y_start: Initial decoder token [batch, 1, dec_input_dim]\n",
        "                     Often: last observed target value or a zero vector\n",
        "            steps: Number of future steps to predict\n",
        "            src_key_padding_mask: Padding mask for encoder\n",
        "\n",
        "        Returns:\n",
        "            Predictions [batch, steps, target_dim]\n",
        "\n",
        "        AUTOREGRESSIVE VISUALIZATION:\n",
        "          Step 1: [START]           → predict ŷ₀\n",
        "          Step 2: [START, ŷ₀]       → predict ŷ₁\n",
        "          Step 3: [START, ŷ₀, ŷ₁]   → predict ŷ₂\n",
        "          Step 4: [START, ŷ₀, ŷ₁, ŷ₂] → predict ŷ₃\n",
        "          ...\n",
        "\n",
        "        Note: Errors compound! If ŷ₀ is wrong, all future predictions are affected.\n",
        "        \"\"\"\n",
        "        device = X.device\n",
        "\n",
        "        # ENCODE HISTORY (once)\n",
        "        x = self.pos_enc(self.enc_in(X))\n",
        "        mem = self.enc(x, key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        preds = []\n",
        "        y_seq = y_start  # Start with initial token; will grow each step\n",
        "\n",
        "        # AUTOREGRESSIVE LOOP\n",
        "        for _ in range(steps):\n",
        "            # Decode current sequence\n",
        "            y_dec = self.pos_dec(self.dec_in(y_seq))\n",
        "            self_mask = subsequent_mask(y_dec.size(1)).to(device)\n",
        "            out = self.dec(y_dec, mem, self_mask, None, src_key_padding_mask)\n",
        "\n",
        "            # Predict next value (only use last position's output)\n",
        "            next_logits = self.head(out[:, -1:, :])  # [batch, 1, target_dim]\n",
        "            next_value = next_logits if self.out_act is None else self.out_act(next_logits)\n",
        "            preds.append(next_value)\n",
        "\n",
        "            # APPEND PREDICTION TO SEQUENCE (for next iteration)\n",
        "            # Handle dimension mismatch if dec_input_dim != target_dim\n",
        "            if self.cfg.dec_input_dim == self.cfg.target_dim:\n",
        "                y_seq = torch.cat([y_seq, next_value], dim=1)\n",
        "            else:\n",
        "                if self.cfg.dec_input_dim > self.cfg.target_dim:\n",
        "                    # Pad prediction to match decoder input size\n",
        "                    pad = torch.zeros(X.size(0), 1, self.cfg.dec_input_dim - self.cfg.target_dim, device=device)\n",
        "                    y_seq = torch.cat([y_seq, torch.cat([next_value, pad], dim=-1)], dim=1)\n",
        "                else:\n",
        "                    # Truncate prediction to match decoder input size\n",
        "                    y_seq = torch.cat([y_seq, next_value[..., :self.cfg.dec_input_dim]], dim=1)\n",
        "\n",
        "        return torch.cat(preds, dim=1)  # [batch, steps, target_dim]\n",
        "\n",
        "\n",
        "    def run_epoch(self, loader, train: bool, p_tf: float):\n",
        "      \"\"\"\n",
        "      Run one epoch of training or evaluation.\n",
        "\n",
        "      Args:\n",
        "          loader: DataLoader (train or validation)\n",
        "          train: If True, update weights; if False, just evaluate\n",
        "          p_tf: Teacher forcing probability for scheduled sampling\n",
        "\n",
        "      Returns:\n",
        "          Average loss over the epoch\n",
        "      \"\"\"\n",
        "      self.train(train)  # Set mode (affects dropout, batchnorm, etc.)\n",
        "      total_loss = 0.0\n",
        "      n_samples = 0\n",
        "\n",
        "      for Xb_cpu, _, ytrue_cpu in loader:\n",
        "          # MOVE TO DEVICE (non_blocking=True allows async transfer)\n",
        "          Xb = Xb_cpu.to(DEVICE, non_blocking=True).float()        # [batch, T_x, features]\n",
        "          ytrue = ytrue_cpu.to(DEVICE, non_blocking=True).float()  # [batch, T_y, 1]\n",
        "\n",
        "          # ZERO GRADIENTS (set_to_none=True is faster than zero_grad())\n",
        "          self.opt.zero_grad(set_to_none=True)\n",
        "\n",
        "          # FORWARD PASS: Autoregressive with scheduled sampling\n",
        "          pred = scheduled_sampling_step(self, Xb, ytrue, p_tf=p_tf)\n",
        "\n",
        "          # COMPUTE LOSS: Mean Squared Error over all predictions\n",
        "          loss = F.mse_loss(pred, ytrue)  # Average over batch, steps, dimensions\n",
        "\n",
        "          # BACKWARD PASS (only in training mode)\n",
        "          if train:\n",
        "              if self.scaler.is_enabled():\n",
        "                  # MIXED PRECISION TRAINING\n",
        "                  self.scaler.scale(loss).backward()           # Scale loss to prevent underflow\n",
        "                  nn.utils.clip_grad_norm_(self.parameters(), 1.0)  # Clip gradients\n",
        "                  self.scaler.step(self.opt)                        # Update weights\n",
        "                  self.scaler.update()                         # Update scale factor\n",
        "              else:\n",
        "                  # STANDARD TRAINING\n",
        "                  loss.backward()\n",
        "                  nn.utils.clip_grad_norm_(self.parameters(), 1.0)  # Prevent exploding gradients\n",
        "                  self.opt.step()\n",
        "\n",
        "          # ACCUMULATE LOSS (weighted by batch size for proper averaging)\n",
        "          total_loss += loss.item() * Xb.size(0)\n",
        "          n_samples += Xb.size(0)\n",
        "\n",
        "      return total_loss / max(1, n_samples)  # Average loss per sample\n",
        "\n",
        "\n",
        "\n",
        "    def train_transformer(self, train_loader, val_loader,\n",
        "      EPOCHS=8,\n",
        "      LR=1e-3,\n",
        "      TF_START=0.9,                 # Initial teacher forcing probability\n",
        "      TF_END=0.5                    # Final teacher forcing probability\n",
        "      ):\n",
        "      \"\"\"\n",
        "      Complete training pipeline with scheduled sampling.\n",
        "\n",
        "      TRAINING STRATEGY:\n",
        "        1. Start with high teacher forcing (p_tf=0.9): easy learning\n",
        "        2. Gradually reduce (linear schedule to p_tf=0.5): harder learning\n",
        "        3. Validate with both teacher-forced and free-running modes\n",
        "\n",
        "      WHY TWO VALIDATION METRICS?\n",
        "        - val_tf (teacher-forced): Fast, optimistic (what model could achieve)\n",
        "        - val_free (free-running): Slow, realistic (actual deployment performance)\n",
        "        - Gap between them shows exposure bias\n",
        "\n",
        "      OPTIMIZATION:\n",
        "        - AdamW optimizer: Adam + weight decay (prevents overfitting)\n",
        "        - Cosine annealing: learning rate decreases smoothly\n",
        "        - Gradient clipping: prevents exploding gradients\n",
        "        - Mixed precision (AMP): faster training on GPU\n",
        "\n",
        "      Args:\n",
        "          EPOCHS: Number of training epochs\n",
        "          N_SAMPLES: Number of time series to generate\n",
        "          SERIES_LENGTH: Length of each series\n",
        "          T_X: History window size\n",
        "          T_Y: Forecast horizon\n",
        "          BATCH: Batch size\n",
        "          LR: Initial learning rate\n",
        "          TF_START: Starting teacher forcing probability\n",
        "          TF_END: Ending teacher forcing probability\n",
        "\n",
        "      Returns:\n",
        "          model: Trained model\n",
        "          loaders: (train_loader, val_loader)\n",
        "          history: Training history dict\n",
        "          data: (X, y, stats) for visualization\n",
        "          windows: (T_X, T_Y) window sizes\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Count parameters\n",
        "      n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "      print(f\"Model parameters: {n_params:,}\")\n",
        "      print(f\"Device: {DEVICE}\")\n",
        "      print(f\"Mixed precision: {'BFloat16' if USE_BF16 else 'Float16' if USE_AMP else 'Float32'}\\n\")\n",
        "\n",
        "      # OPTIMIZER: AdamW with weight decay (L2 regularization)\n",
        "      self.opt = torch.optim.AdamW(self.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "      # SCHEDULER: Cosine annealing (smooth LR decay)\n",
        "      sched = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt, T_max=EPOCHS)\n",
        "\n",
        "      # GRADIENT SCALER: For mixed precision with float16 (not needed for bfloat16)\n",
        "      self.scaler = torch.amp.GradScaler(device=\"cuda\", enabled=SCALER_ENABLED)\n",
        "\n",
        "\n",
        "      print(\"=\" * 70)\n",
        "      print(\"TRAINING\")\n",
        "      print(\"=\" * 70)\n",
        "\n",
        "      # --- TRAINING LOOP ---\n",
        "      # Track three metrics:\n",
        "      #   1. train: training loss with scheduled sampling\n",
        "      #   2. val_tf: validation loss with teacher forcing (optimistic)\n",
        "      #   3. val_free: validation loss with free-running (realistic)\n",
        "      history = {\"train\": [], \"val_tf\": [], \"val_free\": []}\n",
        "\n",
        "      for ep in range(1, EPOCHS+1):\n",
        "          # TEACHER FORCING SCHEDULE: Linear decay from TF_START to TF_END\n",
        "          alpha = (ep - 1) / max(1, EPOCHS - 1)        # 0 → 1 progress\n",
        "          p_tf = TF_START * (1 - alpha) + TF_END * alpha\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # TRAIN: One epoch with scheduled sampling\n",
        "          train_loss = self.run_epoch(train_loader, train=True, p_tf=p_tf)\n",
        "\n",
        "          # VALIDATION MODE 1: Teacher-forced (parallel, fast)\n",
        "          # Gives us a sense of model capacity (if it had perfect inputs)\n",
        "          self.train(False)\n",
        "          val_tf_total, val_n = 0.0, 0\n",
        "\n",
        "          for Xb_cpu, y_in_cpu, ytrue_cpu in val_loader:\n",
        "              Xb = Xb_cpu.to(DEVICE, non_blocking=True).float()\n",
        "              y_in = y_in_cpu.to(DEVICE, non_blocking=True).float()  # Pre-shifted targets\n",
        "              ytrue = ytrue_cpu.to(DEVICE, non_blocking=True).float()\n",
        "\n",
        "              # Teacher-forced forward pass (all steps in parallel)\n",
        "              with torch.amp.autocast(\"cuda\", enabled=USE_AMP, dtype=AMP_DTYPE):\n",
        "                  pred_tf = self(Xb, y_in)           # [batch, T_y, 1]\n",
        "\n",
        "              loss_tf = F.mse_loss(pred_tf.float(), ytrue)\n",
        "              val_tf_total += loss_tf.item() * Xb.size(0)\n",
        "              val_n += Xb.size(0)\n",
        "\n",
        "          val_tf = val_tf_total / max(1, val_n)\n",
        "\n",
        "          # VALIDATION MODE 2: Free-running (autoregressive, slow)\n",
        "          # This is the REAL test: how well does model perform in deployment?\n",
        "          val_free_total, val_n = 0.0, 0\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for Xb_cpu, _, ytrue_cpu in val_loader:\n",
        "                  Xb = Xb_cpu.to(DEVICE, non_blocking=True).float()\n",
        "                  ytrue = ytrue_cpu.to(DEVICE, non_blocking=True).float()\n",
        "\n",
        "                  # Seed with first ground-truth value (in practice: last observed target)\n",
        "                  y_start = ytrue[:, :1, :]\n",
        "\n",
        "                  # Generate predictions autoregressively\n",
        "                  pred_free = self.generate(Xb, y_start=y_start, steps=T_Y).float()\n",
        "\n",
        "                  loss_free = F.mse_loss(pred_free, ytrue)\n",
        "                  val_free_total += loss_free.item() * Xb.size(0)\n",
        "                  val_n += Xb.size(0)\n",
        "\n",
        "          val_free = val_free_total / max(1, val_n)\n",
        "\n",
        "          # UPDATE LEARNING RATE\n",
        "          sched.step()\n",
        "\n",
        "          # PRINT PROGRESS\n",
        "          print(f\"Epoch {ep:02d} | p_tf={p_tf:.2f} | \"\n",
        "                f\"train={train_loss:.4f} | val_tf={val_tf:.4f} | val_free={val_free:.4f} | \"\n",
        "                f\"lr={sched.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "          # RECORD HISTORY\n",
        "          history[\"train\"].append(train_loss)\n",
        "          history[\"val_tf\"].append(val_tf)\n",
        "          history[\"val_free\"].append(val_free)\n",
        "\n",
        "      print(\"\\n\" + \"=\" * 70)\n",
        "      print(\"TRAINING COMPLETE\")\n",
        "      print(\"=\" * 70)\n",
        "      print(f\"Final train loss: {history['train'][-1]:.4f}\")\n",
        "      print(f\"Final val_tf loss: {history['val_tf'][-1]:.4f}\")\n",
        "      print(f\"Final val_free loss: {history['val_free'][-1]:.4f}\")\n",
        "      print(f\"Exposure bias gap: {history['val_free'][-1] - history['val_tf'][-1]:.4f}\")\n",
        "      print(\"(Gap = how much worse free-running is than teacher-forced)\\n\")\n",
        "\n",
        "      return history\n",
        "    def predict_evaluation(self, test_loader):\n",
        "\n",
        "      DEVICE = next(self.parameters()).device\n",
        "      self.eval()  # Set to evaluation mode\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # Get one batch from validation set\n",
        "          Xb_cpu, _, ytrue_cpu = next(iter(test_loader))\n",
        "\n",
        "          # Use only first sample for visualization\n",
        "          Xb = Xb_cpu[:1].to(DEVICE).float()      # [1, T_x, features]\n",
        "          ytrue = ytrue_cpu[:1].to(DEVICE).float()  # [1, T_y, 1]\n",
        "\n",
        "          # Generate forecast (autoregressive)\n",
        "          y_start = ytrue[:, :1, :]  # Seed with first ground-truth value\n",
        "          pred = self.generate(Xb, y_start=y_start, steps=T_Y).float()\n",
        "\n",
        "      # Convert to numpy for plotting\n",
        "      gt = ytrue[0, :, 0].detach().cpu().numpy()  # Ground truth\n",
        "      pr = pred[0, :, 0].detach().cpu().numpy()   # Prediction\n",
        "\n",
        "      # Compute error metrics\n",
        "      mae = np.mean(np.abs(pr - gt))              # Mean Absolute Error\n",
        "      rmse = np.sqrt(np.mean((pr - gt) ** 2))     # Root Mean Squared Error\n",
        "\n",
        "      return mae, rmse\n"
      ],
      "metadata": {
        "id": "7AL7DfuTkqzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chamada"
      ],
      "metadata": {
        "id": "lNtm-fImlTdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TSTConfig:\n",
        "    \"\"\"\n",
        "    Configuration for Time Series Transformer.\n",
        "\n",
        "    Architecture hyperparameters:\n",
        "        d_model: Hidden dimension throughout the model (like \"brain size\")\n",
        "        num_encoder_layers: Depth of encoder (more = more context integration)\n",
        "        num_decoder_layers: Depth of decoder (fewer than encoder is common)\n",
        "        num_heads: Parallel attention operations (more = capture more relationships)\n",
        "        d_ff: Feedforward expansion dimension (typically 4*d_model)\n",
        "        dropout: Regularization strength (0.0 = none, 0.3 = aggressive)\n",
        "\n",
        "    Input/output specification:\n",
        "        input_dim: Number of input features (e.g., temperature, pressure, etc.)\n",
        "        target_dim: Number of values to predict (often 1 for univariate forecasting)\n",
        "        dec_input_dim: Decoder input size (should match target_dim for simplicity)\n",
        "\n",
        "    Other settings:\n",
        "        pos_encoding: \"sin\" (fixed) or \"learned\" (trainable)\n",
        "        max_len: Maximum sequence length\n",
        "        output_activation: Optional final activation (\"tanh\", \"sigmoid\", or None)\n",
        "    \"\"\"\n",
        "    d_model: int = 128\n",
        "    num_encoder_layers: int = 3\n",
        "    num_decoder_layers: int = 2\n",
        "    num_heads: int = 8\n",
        "    d_ff: int = 256\n",
        "    dropout: float = 0.1\n",
        "    pos_encoding: str = \"sin\"  # \"sin\" or \"learned\"\n",
        "    max_len: int = 4096\n",
        "    sliding_window_size: int = 4\n",
        "    input_dim: int = 2         # number of historical features\n",
        "    target_dim: int = 1        # we forecast a single target value\n",
        "    dec_input_dim: int = 1     # decoder input size (shifted target)\n",
        "    output_activation: Optional[str] = None  # e.g., \"tanh\" for bounded target\n",
        "\n",
        "# --- MODEL + OPTIMIZER ---\n",
        "cfg = TSTConfig()\n",
        "model_slideWin = TimeSeriesTransformerSlidingWindown(cfg).to(DEVICE)\n",
        "history = model_slideWin.train_transformer(train_loader, val_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaifoyA5lTSj",
        "outputId": "1f4abbe9-3b2e-42a3-d1a6-ec1dad6014f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 796,289\n",
            "Device: cuda\n",
            "Mixed precision: BFloat16\n",
            "\n",
            "======================================================================\n",
            "TRAINING\n",
            "======================================================================\n",
            "Epoch 01 | p_tf=0.90 | train=0.5164 | val_tf=0.2366 | val_free=0.1929 | lr=9.62e-04\n",
            "Epoch 02 | p_tf=0.84 | train=0.0495 | val_tf=0.2066 | val_free=0.0731 | lr=8.54e-04\n",
            "Epoch 03 | p_tf=0.79 | train=0.0234 | val_tf=0.1588 | val_free=0.0529 | lr=6.91e-04\n",
            "Epoch 04 | p_tf=0.73 | train=0.0168 | val_tf=0.1597 | val_free=0.0470 | lr=5.00e-04\n",
            "Epoch 05 | p_tf=0.67 | train=0.0175 | val_tf=0.1460 | val_free=0.0408 | lr=3.09e-04\n",
            "Epoch 06 | p_tf=0.61 | train=0.0170 | val_tf=0.1425 | val_free=0.0455 | lr=1.46e-04\n",
            "Epoch 07 | p_tf=0.56 | train=0.0178 | val_tf=0.1476 | val_free=0.0436 | lr=3.81e-05\n",
            "Epoch 08 | p_tf=0.50 | train=0.0193 | val_tf=0.1434 | val_free=0.0337 | lr=0.00e+00\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE\n",
            "======================================================================\n",
            "Final train loss: 0.0193\n",
            "Final val_tf loss: 0.1434\n",
            "Final val_free loss: 0.0337\n",
            "Exposure bias gap: -0.1097\n",
            "(Gap = how much worse free-running is than teacher-forced)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mae, rmse = model_slideWin.predict_evaluation(test_loader)\n",
        "\n",
        "print(\"\\nForecast analysis:\")\n",
        "print(f\"  - MAE (Mean Absolute Error): {mae:.4f}\")\n",
        "print(f\"  - RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
        "print(f\"  - Forecast horizon: {T_Y} steps\")\n",
        "print(f\"  - History used: {T_X} steps\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiL9Biq6llYk",
        "outputId": "8f35ff0c-6e66-430e-9c92-06947dd4ab74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forecast analysis:\n",
            "  - MAE (Mean Absolute Error): 0.0890\n",
            "  - RMSE (Root Mean Squared Error): 0.1069\n",
            "  - Forecast horizon: 8 steps\n",
            "  - History used: 32 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Probsparse"
      ],
      "metadata": {
        "id": "fQEMs_YyI1LK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definição da Atenção"
      ],
      "metadata": {
        "id": "JhmqK0Xgb2hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProbSparseAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    ProbSparse Multi-Head Attention: efficient attention for long sequences.\n",
        "\n",
        "    KEY INNOVATION:\n",
        "        Instead of computing attention for all queries (O(n²)),\n",
        "        selects only the most \"informative\" queries based on sparsity measure.\n",
        "    MECHANISM:\n",
        "      1. Project inputs to Query (Q), Key (K), Value (V) spaces\n",
        "      2. For each query, measure sparsity: M(q_i, K) = max_j(q_i·k_j) - mean_j(q_i·k_j)\n",
        "      3. Select top-u queries (u = factor * log(n)) with highest sparsity\n",
        "      4. Compute full attention only for selected queries\n",
        "      5. For non-selected queries, use mean of values\n",
        "\n",
        "      6. Concatenate heads and project back to d_model\n",
        "\n",
        "    COMPLEXITY: O(n log n) instead of O(n²)\n",
        "\n",
        "    Args:\n",
        "        d_model: Total model dimension (must be divisible by num_heads)\n",
        "        num_heads: Number of parallel attention operations\n",
        "        factor: Controls sparsity level - u = factor * log(seq_len)\n",
        "        dropout: Dropout applied to attention weights\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0, bias: bool = True, factor: int = 5):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.h = num_heads\n",
        "        self.d_head = d_model // num_heads  # Dimension per head\n",
        "\n",
        "        # Linear projections for Q, K, V (one per input role)\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "\n",
        "        # Output projection (combine all heads)\n",
        "        self.o_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.factor = factor\n",
        "\n",
        "    def _measure_query_sparsity(self, Q: torch.Tensor, K: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Measure sparsity of each query using max-mean difference.\n",
        "\n",
        "        M(q_i, K) = max_j(q_i·k_j) - mean_j(q_i·k_j)\n",
        "\n",
        "        Higher value = more informative query (attention less uniform)\n",
        "        \"\"\"\n",
        "        # Sample subset of keys for efficiency\n",
        "        seq_len = K.size(2)\n",
        "        sample_size = min(seq_len, self.factor * 4)\n",
        "        sample_indices = torch.randperm(seq_len)[:sample_size].to(K.device)\n",
        "        K_sample = K[:, :, sample_indices, :]\n",
        "\n",
        "        # Compute dot products with sampled keys\n",
        "        dot_products = torch.matmul(Q, K_sample.transpose(-2, -1))\n",
        "\n",
        "        # Sparsity measure: max - mean\n",
        "        return dot_products.max(dim=-1)[0] - dot_products.mean(dim=-1)\n",
        "\n",
        "    def _select_informative_queries(self, Q: torch.Tensor, K: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Select top-u most informative queries for full attention computation.\n",
        "        \"\"\"\n",
        "        batch_size, num_heads, seq_len, _ = Q.shape\n",
        "\n",
        "        # Number of queries to select: u = factor * math.log(seq_len)\n",
        "        u = int(self.factor * math.log(seq_len)) if seq_len > 1 else 1 # Ensure u is at least 1\n",
        "        u = max(1, min(u, seq_len))  # Ensure valid range\n",
        "\n",
        "        # Measure sparsity for all queries\n",
        "        sparsity_scores = self._measure_query_sparsity(Q, K)\n",
        "\n",
        "        # Select top-u queries per batch and head\n",
        "        _, top_indices = torch.topk(sparsity_scores, k=u, dim=-1)\n",
        "\n",
        "        # Create selection mask\n",
        "        mask = torch.zeros_like(sparsity_scores, dtype=torch.bool)\n",
        "        for b in range(batch_size):\n",
        "            for h in range(num_heads):\n",
        "                mask[b, h, top_indices[b, h]] = True\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def _split_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Split the embedding dimension into multiple heads.\n",
        "\n",
        "        Reshape: [B, T, D] -> [B, T, H, D/H] -> [B, H, T, D/H]\n",
        "\n",
        "        Why? We want to compute H separate attention operations in parallel.\n",
        "        Moving H to dimension 1 allows efficient batched matrix operations.\n",
        "        \"\"\"\n",
        "        B, T, _ = x.shape\n",
        "        return x.view(B, T, self.h, self.d_head).permute(0, 2, 1, 3)\n",
        "\n",
        "    def _combine_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Merge multiple heads back into single embedding.\n",
        "\n",
        "        Reshape: [B, H, T, D/H] -> [B, T, H, D/H] -> [B, T, D]\n",
        "\n",
        "        Concatenates all head outputs into the original d_model dimension.\n",
        "        \"\"\"\n",
        "        B, H, T, Dh = x.shape\n",
        "        return x.permute(0, 2, 1, 3).contiguous().view(B, T, H * Dh)\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask=None, key_padding_mask=None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute ProbSparse multi-head attention.\n",
        "\n",
        "        Args:\n",
        "            q: Query tensor [batch, seq_len_q, d_model]\n",
        "            k: Key tensor [batch, seq_len_k, d_model]\n",
        "            v: Value tensor [batch, seq_len_k, d_model]\n",
        "            attn_mask: Attention mask [1, 1, seq_len_q, seq_len_k] (e.g., causal mask)\n",
        "            key_padding_mask: Padding mask [batch, seq_len_k] (True = ignore this position)\n",
        "\n",
        "\n",
        "        Returns:\n",
        "            Attention output [batch, seq_len_q, d_model]\n",
        "\n",
        "        Note: For self-attention, q=k=v (same sequence attends to itself)\n",
        "              For cross-attention, q comes from decoder, k=v from encoder\n",
        "        \"\"\"\n",
        "        # Project and split into heads\n",
        "        Q = self._split_heads(self.q_proj(q))  # [B, H, T_q, D/H]\n",
        "        K = self._split_heads(self.k_proj(k))  # [B, H, T_k, D/H]\n",
        "        V = self._split_heads(self.v_proj(v))  # [B, H, T_k, D/H]\n",
        "\n",
        "        # SELECT INFORMATIVE QUERIES\n",
        "        query_mask = self._select_informative_queries(Q, K)\n",
        "\n",
        "        # Use finfo.min instead of -inf for numerical stability in half precision\n",
        "        neg_inf = torch.finfo(Q.dtype).min\n",
        "\n",
        "        # Initialize scores with neg_inf\n",
        "        batch_size, _, seq_len_q, seq_len_k = Q.size(0), Q.size(1), Q.size(2), K.size(2)\n",
        "        scores = torch.full((batch_size, self.h, seq_len_q, seq_len_k),\n",
        "                           neg_inf, device=Q.device)\n",
        "\n",
        "        # Compute attention scores only for selected queries\n",
        "        expanded_mask = query_mask.unsqueeze(-1).expand(-1, -1, -1, seq_len_k)\n",
        "        selected_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        scores = torch.where(expanded_mask, selected_scores, scores)\n",
        "\n",
        "\n",
        "        # Use finfo.min instead of -inf for numerical stability in half precision\n",
        "        neg_inf = torch.finfo(scores.dtype).min\n",
        "\n",
        "        # APPLY MASK\n",
        "        # Key padding mask: ignore padded positions in the key sequence\n",
        "        if key_padding_mask is not None:\n",
        "            kpm = merge_padding_mask(scores.shape, key_padding_mask)\n",
        "            scores = scores.masked_fill(kpm, neg_inf)  # Set to very negative -> softmax ~0\n",
        "\n",
        "        # Attention mask: prevent attending to certain positions (e.g., future tokens)\n",
        "        if attn_mask is not None:\n",
        "            if attn_mask.device != scores.device:\n",
        "                attn_mask = attn_mask.to(scores.device)\n",
        "            scores = scores.masked_fill(attn_mask, neg_inf)\n",
        "\n",
        "        # Compute attention weights (softmax over key dimension)\n",
        "        attn = torch.softmax(scores, dim=-1)  # [B, H, T_q, T_k]\n",
        "        attn = self.drop(attn)  # Dropout for regularization\n",
        "\n",
        "        # Weighted sum of values\n",
        "        out = torch.matmul(attn, V)  # [B, H, T_q, D/H]\n",
        "\n",
        "        # For non-selected queries, use mean of values\n",
        "        query_mask_expanded = query_mask.unsqueeze(-1).expand(-1, -1, -1, V.size(-1))\n",
        "        value_mean = V.mean(dim=2, keepdim=True).expand(-1, -1, seq_len_q, -1)\n",
        "        out = torch.where(query_mask_expanded, out, value_mean)\n",
        "\n",
        "        # Combine heads and project\n",
        "        out = self._combine_heads(out)  # [B, T_q, D]\n",
        "        return self.o_proj(out)\n"
      ],
      "metadata": {
        "id": "jdFOYmdpI9Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definição"
      ],
      "metadata": {
        "id": "BQlwoAtOcs7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayerProbSparse(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Transformer encoder layer.\n",
        "\n",
        "    Architecture:\n",
        "      1. Multi-Head Self-Attention (each position attends to all positions)\n",
        "      2. Feedforward Network (process each position independently)\n",
        "\n",
        "    Both wrapped in PreNormResidual blocks for stability.\n",
        "\n",
        "    Purpose: Build representations that incorporate context from the entire sequence.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        num_heads: Number of attention heads\n",
        "        d_ff: Feedforward hidden dimension\n",
        "        attn_dropout: Dropout for attention\n",
        "        ff_dropout: Dropout for feedforward\n",
        "        activation: Activation function type\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, attn_dropout=0.0, ff_dropout=0.0, activation=\"gelu\", factor: int = 5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Self-attention: sequence attends to itself\n",
        "        self.mha = PreNormResidual(\n",
        "            d_model,\n",
        "            ProbSparseAttention(d_model, num_heads, attn_dropout, factor=factor)\n",
        "        )\n",
        "        # Position-wise feedforward\n",
        "        self.ffn = PreNormResidual(\n",
        "            d_model,\n",
        "            FeedForward(d_model, d_ff, ff_dropout, activation)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input sequence [batch, seq_len, d_model]\n",
        "            key_padding_mask: Padding mask [batch, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            Encoded sequence [batch, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # Self-attention: q=k=v=x\n",
        "        x = self.mha(x, x, x, attn_mask=None, key_padding_mask=key_padding_mask)\n",
        "        # Feedforward\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderProbSparse(nn.Module):\n",
        "    \"\"\"\n",
        "    Stack of encoder layers + final layer normalization.\n",
        "\n",
        "    Purpose: Process the input sequence to build rich representations\n",
        "    that capture context and relationships.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        num_layers: Number of encoder layers to stack\n",
        "        num_heads: Number of attention heads per layer\n",
        "        d_ff: Feedforward hidden dimension\n",
        "        attn_dropout: Attention dropout\n",
        "        ff_dropout: Feedforward dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_layers, num_heads, d_ff, attn_dropout=0.0, ff_dropout=0.0, factor: int = 5 ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayerProbSparse(d_model, num_heads, d_ff, attn_dropout, ff_dropout, factor=factor)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)  # Final normalization\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input embeddings [batch, seq_len, d_model]\n",
        "            key_padding_mask: Padding mask [batch, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            Encoded representations [batch, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, key_padding_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                       4) COMPLETE MODEL (ENC + DEC)\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TimeSeriesTransformerProbSparse(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Sequence-to-Sequence Transformer for Time Series Forecasting.\n",
        "\n",
        "    ARCHITECTURE OVERVIEW:\n",
        "      Input (history) → Encoder → Memory\n",
        "                                     ↓\n",
        "      Previous outputs → Decoder → Next prediction\n",
        "\n",
        "    KEY CONCEPTS:\n",
        "      1. ENCODER: Processes historical data (e.g., past 24 hours)\n",
        "         - Builds rich representations that capture patterns and context\n",
        "         - Output = \"memory\" that decoder can attend to\n",
        "\n",
        "      2. DECODER: Generates future predictions (e.g., next 12 hours)\n",
        "         - Autoregressively: generates one step at a time\n",
        "         - Uses both its own past predictions AND encoder memory\n",
        "\n",
        "      3. TEACHER FORCING (training): Feed ground-truth previous values to decoder\n",
        "         - Fast: can train all steps in parallel\n",
        "         - Problem: decoder never sees its own mistakes during training\n",
        "\n",
        "      4. AUTOREGRESSIVE GENERATION (inference): Feed model's own predictions\n",
        "         - Slow: must generate step-by-step\n",
        "         - Realistic: matches deployment scenario\n",
        "\n",
        "    EXAMPLE:\n",
        "      History: [hour 1, hour 2, ..., hour 24] → Encoder\n",
        "      Decoder generates: [hour 25, hour 26, ..., hour 36]\n",
        "        - To predict hour 26, decoder sees: hour 25 (either ground-truth or its prediction)\n",
        "        - To predict hour 27, decoder sees: hours 25-26\n",
        "        - And so on...\n",
        "\n",
        "    Args:\n",
        "        cfg: Configuration object with all hyperparameters\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: TSTConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # INPUT PROJECTIONS: Convert raw features to model dimension\n",
        "        self.enc_in = nn.Linear(cfg.input_dim, cfg.d_model)      # Encoder input projection\n",
        "        self.dec_in = nn.Linear(cfg.dec_input_dim, cfg.d_model)  # Decoder input projection\n",
        "\n",
        "        # POSITIONAL ENCODINGS: Add position information to embeddings\n",
        "        if cfg.pos_encoding == \"sin\":\n",
        "            self.pos_enc = SinusoidalPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "            self.pos_dec = SinusoidalPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "        else:\n",
        "            self.pos_enc = LearnedPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "            self.pos_dec = LearnedPositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "\n",
        "        # ENCODER & DECODER STACKS\n",
        "        self.enc = EncoderProbSparse(cfg.d_model, cfg.num_encoder_layers, cfg.num_heads, cfg.d_ff,\n",
        "                           attn_dropout=cfg.dropout, ff_dropout=cfg.dropout, factor=cfg.factor)\n",
        "        self.dec = Decoder(cfg.d_model, cfg.num_decoder_layers, cfg.num_heads, cfg.d_ff,\n",
        "                           attn_dropout=cfg.dropout, ff_dropout=cfg.dropout)\n",
        "\n",
        "        # OUTPUT HEAD: Project decoder output to prediction space\n",
        "        self.head = nn.Linear(cfg.d_model, cfg.target_dim)\n",
        "\n",
        "        # INPUT DROPOUT: Regularization on embeddings\n",
        "        self.drop_in = nn.Dropout(cfg.dropout)\n",
        "\n",
        "        # OPTIONAL OUTPUT ACTIVATION: Constrain predictions to a range\n",
        "        self.out_act = None\n",
        "        if cfg.output_activation == \"tanh\":\n",
        "            self.out_act = torch.tanh      # Range: [-1, 1]\n",
        "        elif cfg.output_activation == \"sigmoid\":\n",
        "            self.out_act = torch.sigmoid   # Range: [0, 1]\n",
        "\n",
        "    def forward(self, X, y_in, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Teacher-forced forward pass (for training).\n",
        "\n",
        "        TRAINING MODE: We give the decoder the ground-truth previous outputs.\n",
        "        This allows parallel computation (fast!) but creates train-test mismatch.\n",
        "\n",
        "        Flow:\n",
        "          1. Encode history: X → embeddings → encoder → memory\n",
        "          2. Decode with teacher forcing: y_in → embeddings → decoder → predictions\n",
        "          3. Project to output space: predictions → final forecasts\n",
        "\n",
        "        Args:\n",
        "            X: Historical features [batch, history_len, input_dim]\n",
        "               Example shape: [32, 24, 4] = 32 sequences, 24 timesteps, 4 features\n",
        "\n",
        "            y_in: Decoder inputs (shifted targets) [batch, forecast_len, dec_input_dim]\n",
        "                  IMPORTANT: y_in[0] = start token (often 0 or last observed value)\n",
        "                             y_in[1] = ground-truth target at step 0\n",
        "                             y_in[2] = ground-truth target at step 1\n",
        "                             ... (shifted right by 1)\n",
        "\n",
        "            src_key_padding_mask: Padding mask for encoder [batch, history_len]\n",
        "            tgt_key_padding_mask: Padding mask for decoder [batch, forecast_len]\n",
        "\n",
        "        Returns:\n",
        "            Predictions [batch, forecast_len, target_dim]\n",
        "\n",
        "        TEACHER FORCING VISUALIZATION:\n",
        "          Ground truth: [y₀, y₁, y₂, y₃]\n",
        "          Decoder input: [START, y₀, y₁, y₂]  (shifted right)\n",
        "          Predictions:   [ŷ₀, ŷ₁, ŷ₂, ŷ₃]\n",
        "          Loss: compare predictions to ground truth\n",
        "        \"\"\"\n",
        "        # ENCODE HISTORY\n",
        "        x = self.drop_in(self.pos_enc(self.enc_in(X)))  # Project → positional encoding → dropout\n",
        "        mem = self.enc(x, key_padding_mask=None)  # Run through encoder\n",
        "\n",
        "        # DECODE WITH TEACHER FORCING\n",
        "        y = self.drop_in(self.pos_dec(self.dec_in(y_in)))  # Process decoder inputs\n",
        "        self_mask = subsequent_mask(y.size(1)).to(y.device)  # Causal mask: can't see future\n",
        "        out = self.dec(y, mem, self_mask, tgt_key_padding_mask, src_key_padding_mask)\n",
        "\n",
        "        # PROJECT TO OUTPUT SPACE\n",
        "        logits = self.head(out)  # [batch, forecast_len, target_dim]\n",
        "\n",
        "        # Apply output activation if specified\n",
        "        return logits if self.out_act is None else self.out_act(logits)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, X, y_start, steps, src_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Autoregressive generation (for inference/deployment).\n",
        "\n",
        "        INFERENCE MODE: Decoder uses its own predictions (no ground truth available).\n",
        "        This is slow (sequential) but realistic for deployment.\n",
        "\n",
        "        Process:\n",
        "          1. Encode history once (doesn't change)\n",
        "          2. Start with initial token (y_start)\n",
        "          3. For each step:\n",
        "             a. Decode current sequence\n",
        "             b. Predict next value\n",
        "             c. Append prediction to sequence\n",
        "             d. Repeat\n",
        "\n",
        "        Args:\n",
        "            X: Historical features [batch, history_len, input_dim]\n",
        "            y_start: Initial decoder token [batch, 1, dec_input_dim]\n",
        "                     Often: last observed target value or a zero vector\n",
        "            steps: Number of future steps to predict\n",
        "            src_key_padding_mask: Padding mask for encoder\n",
        "\n",
        "        Returns:\n",
        "            Predictions [batch, steps, target_dim]\n",
        "\n",
        "        AUTOREGRESSIVE VISUALIZATION:\n",
        "          Step 1: [START]           → predict ŷ₀\n",
        "          Step 2: [START, ŷ₀]       → predict ŷ₁\n",
        "          Step 3: [START, ŷ₀, ŷ₁]   → predict ŷ₂\n",
        "          Step 4: [START, ŷ₀, ŷ₁, ŷ₂] → predict ŷ₃\n",
        "          ...\n",
        "\n",
        "        Note: Errors compound! If ŷ₀ is wrong, all future predictions are affected.\n",
        "        \"\"\"\n",
        "        device = X.device\n",
        "\n",
        "        # ENCODE HISTORY (once)\n",
        "        x = self.pos_enc(self.enc_in(X))\n",
        "        mem = self.enc(x, key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        preds = []\n",
        "        y_seq = y_start  # Start with initial token; will grow each step\n",
        "\n",
        "        # AUTOREGRESSIVE LOOP\n",
        "        for _ in range(steps):\n",
        "            # Decode current sequence\n",
        "            y_dec = self.pos_dec(self.dec_in(y_seq))\n",
        "            self_mask = subsequent_mask(y_dec.size(1)).to(device)\n",
        "            out = self.dec(y_dec, mem, self_mask, None, src_key_padding_mask)\n",
        "\n",
        "            # Predict next value (only use last position's output)\n",
        "            next_logits = self.head(out[:, -1:, :])  # [batch, 1, target_dim]\n",
        "            next_value = next_logits if self.out_act is None else self.out_act(next_logits)\n",
        "            preds.append(next_value)\n",
        "\n",
        "            # APPEND PREDICTION TO SEQUENCE (for next iteration)\n",
        "            # Handle dimension mismatch if dec_input_dim != target_dim\n",
        "            if self.cfg.dec_input_dim == self.cfg.target_dim:\n",
        "                y_seq = torch.cat([y_seq, next_value], dim=1)\n",
        "            else:\n",
        "                if self.cfg.dec_input_dim > self.cfg.target_dim:\n",
        "                    # Pad prediction to match decoder input size\n",
        "                    pad = torch.zeros(X.size(0), 1, self.cfg.dec_input_dim - self.cfg.target_dim, device=device)\n",
        "                    y_seq = torch.cat([y_seq, torch.cat([next_value, pad], dim=-1)], dim=1)\n",
        "                else:\n",
        "                    # Truncate prediction to match decoder input size\n",
        "                    y_seq = torch.cat([y_seq, next_value[..., :self.cfg.dec_input_dim]], dim=1)\n",
        "\n",
        "        return torch.cat(preds, dim=1)  # [batch, steps, target_dim]\n",
        "\n",
        "\n",
        "    def run_epoch(self, loader, train: bool, p_tf: float):\n",
        "      \"\"\"\n",
        "      Run one epoch of training or evaluation.\n",
        "\n",
        "      Args:\n",
        "          loader: DataLoader (train or validation)\n",
        "          train: If True, update weights; if False, just evaluate\n",
        "          p_tf: Teacher forcing probability for scheduled sampling\n",
        "\n",
        "      Returns:\n",
        "          Average loss over the epoch\n",
        "      \"\"\"\n",
        "      self.train(train)  # Set mode (affects dropout, batchnorm, etc.)\n",
        "      total_loss = 0.0\n",
        "      n_samples = 0\n",
        "\n",
        "      for Xb_cpu, _, ytrue_cpu in loader:\n",
        "          # MOVE TO DEVICE (non_blocking=True allows async transfer)\n",
        "          Xb = Xb_cpu.to(DEVICE, non_blocking=True).float()        # [batch, T_x, features]\n",
        "          ytrue = ytrue_cpu.to(DEVICE, non_blocking=True).float()  # [batch, T_y, 1]\n",
        "\n",
        "          # ZERO GRADIENTS (set_to_none=True is faster than zero_grad())\n",
        "          self.opt.zero_grad(set_to_none=True)\n",
        "\n",
        "          # FORWARD PASS: Autoregressive with scheduled sampling\n",
        "          pred = scheduled_sampling_step(self, Xb, ytrue, p_tf=p_tf)\n",
        "\n",
        "          # COMPUTE LOSS: Mean Squared Error over all predictions\n",
        "          loss = F.mse_loss(pred, ytrue)  # Average over batch, steps, dimensions\n",
        "\n",
        "          # BACKWARD PASS (only in training mode)\n",
        "          if train:\n",
        "              if self.scaler.is_enabled():\n",
        "                  # MIXED PRECISION TRAINING\n",
        "                  self.scaler.scale(loss).backward()           # Scale loss to prevent underflow\n",
        "                  nn.utils.clip_grad_norm_(self.parameters(), 1.0)  # Clip gradients\n",
        "                  self.scaler.step(self.opt)                        # Update weights\n",
        "                  self.scaler.update()                         # Update scale factor\n",
        "              else:\n",
        "                  # STANDARD TRAINING\n",
        "                  loss.backward()\n",
        "                  nn.utils.clip_grad_norm_(self.parameters(), 1.0)  # Prevent exploding gradients\n",
        "                  self.opt.step()\n",
        "\n",
        "          # ACCUMULATE LOSS (weighted by batch size for proper averaging)\n",
        "          total_loss += loss.item() * Xb.size(0)\n",
        "          n_samples += Xb.size(0)\n",
        "\n",
        "      return total_loss / max(1, n_samples)  # Average loss per sample\n",
        "\n",
        "\n",
        "\n",
        "    def train_transformer(self, train_loader, val_loader,\n",
        "      EPOCHS=8,\n",
        "      LR=1e-3,\n",
        "      TF_START=0.9,                 # Initial teacher forcing probability\n",
        "      TF_END=0.5                    # Final teacher forcing probability\n",
        "      ):\n",
        "      \"\"\"\n",
        "      Complete training pipeline with scheduled sampling.\n",
        "\n",
        "      TRAINING STRATEGY:\n",
        "        1. Start with high teacher forcing (p_tf=0.9): easy learning\n",
        "        2. Gradually reduce (linear schedule to p_tf=0.5): harder learning\n",
        "        3. Validate with both teacher-forced and free-running modes\n",
        "\n",
        "      WHY TWO VALIDATION METRICS?\n",
        "        - val_tf (teacher-forced): Fast, optimistic (what model could achieve)\n",
        "        - val_free (free-running): Slow, realistic (actual deployment performance)\n",
        "        - Gap between them shows exposure bias\n",
        "\n",
        "      OPTIMIZATION:\n",
        "        - AdamW optimizer: Adam + weight decay (prevents overfitting)\n",
        "        - Cosine annealing: learning rate decreases smoothly\n",
        "        - Gradient clipping: prevents exploding gradients\n",
        "        - Mixed precision (AMP): faster training on GPU\n",
        "\n",
        "      Args:\n",
        "          EPOCHS: Number of training epochs\n",
        "          N_SAMPLES: Number of time series to generate\n",
        "          SERIES_LENGTH: Length of each series\n",
        "          T_X: History window size\n",
        "          T_Y: Forecast horizon\n",
        "          BATCH: Batch size\n",
        "          LR: Initial learning rate\n",
        "          TF_START: Starting teacher forcing probability\n",
        "          TF_END: Ending teacher forcing probability\n",
        "\n",
        "      Returns:\n",
        "          model: Trained model\n",
        "          loaders: (train_loader, val_loader)\n",
        "          history: Training history dict\n",
        "          data: (X, y, stats) for visualization\n",
        "          windows: (T_X, T_Y) window sizes\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Count parameters\n",
        "      n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "      print(f\"Model parameters: {n_params:,}\")\n",
        "      print(f\"Device: {DEVICE}\")\n",
        "      print(f\"Mixed precision: {'BFloat16' if USE_BF16 else 'Float16' if USE_AMP else 'Float32'}\\n\")\n",
        "\n",
        "      # OPTIMIZER: AdamW with weight decay (L2 regularization)\n",
        "      self.opt = torch.optim.AdamW(self.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "      # SCHEDULER: Cosine annealing (smooth LR decay)\n",
        "      sched = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt, T_max=EPOCHS)\n",
        "\n",
        "      # GRADIENT SCALER: For mixed precision with float16 (not needed for bfloat16)\n",
        "      self.scaler = torch.amp.GradScaler(device=\"cuda\", enabled=SCALER_ENABLED)\n",
        "\n",
        "\n",
        "      print(\"=\" * 70)\n",
        "      print(\"TRAINING\")\n",
        "      print(\"=\" * 70)\n",
        "\n",
        "      # --- TRAINING LOOP ---\n",
        "      # Track three metrics:\n",
        "      #   1. train: training loss with scheduled sampling\n",
        "      #   2. val_tf: validation loss with teacher forcing (optimistic)\n",
        "      #   3. val_free: validation loss with free-running (realistic)\n",
        "      history = {\"train\": [], \"val_tf\": [], \"val_free\": []}\n",
        "\n",
        "      for ep in range(1, EPOCHS+1):\n",
        "          # TEACHER FORCING SCHEDULE: Linear decay from TF_START to TF_END\n",
        "          alpha = (ep - 1) / max(1, EPOCHS - 1)        # 0 → 1 progress\n",
        "          p_tf = TF_START * (1 - alpha) + TF_END * alpha\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # TRAIN: One epoch with scheduled sampling\n",
        "          train_loss = self.run_epoch(train_loader, train=True, p_tf=p_tf)\n",
        "\n",
        "          # VALIDATION MODE 1: Teacher-forced (parallel, fast)\n",
        "          # Gives us a sense of model capacity (if it had perfect inputs)\n",
        "          self.train(False)\n",
        "          val_tf_total, val_n = 0.0, 0\n",
        "\n",
        "          for Xb_cpu, y_in_cpu, ytrue_cpu in val_loader:\n",
        "              Xb = Xb_cpu.to(DEVICE, non_blocking=True).float()\n",
        "              y_in = y_in_cpu.to(DEVICE, non_blocking=True).float()  # Pre-shifted targets\n",
        "              ytrue = ytrue_cpu.to(DEVICE, non_blocking=True).float()\n",
        "\n",
        "              # Teacher-forced forward pass (all steps in parallel)\n",
        "              with torch.amp.autocast(\"cuda\", enabled=USE_AMP, dtype=AMP_DTYPE):\n",
        "                  pred_tf = self(Xb, y_in)           # [batch, T_y, 1]\n",
        "\n",
        "              loss_tf = F.mse_loss(pred_tf.float(), ytrue)\n",
        "              val_tf_total += loss_tf.item() * Xb.size(0)\n",
        "              val_n += Xb.size(0)\n",
        "\n",
        "          val_tf = val_tf_total / max(1, val_n)\n",
        "\n",
        "          # VALIDATION MODE 2: Free-running (autoregressive, slow)\n",
        "          # This is the REAL test: how well does model perform in deployment?\n",
        "          val_free_total, val_n = 0.0, 0\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for Xb_cpu, _, ytrue_cpu in val_loader:\n",
        "                  Xb = Xb_cpu.to(DEVICE, non_blocking=True).float()\n",
        "                  ytrue = ytrue_cpu.to(DEVICE, non_blocking=True).float()\n",
        "\n",
        "                  # Seed with first ground-truth value (in practice: last observed target)\n",
        "                  y_start = ytrue[:, :1, :]\n",
        "\n",
        "                  # Generate predictions autoregressively\n",
        "                  pred_free = self.generate(Xb, y_start=y_start, steps=T_Y).float()\n",
        "\n",
        "                  loss_free = F.mse_loss(pred_free, ytrue)\n",
        "                  val_free_total += loss_free.item() * Xb.size(0)\n",
        "                  val_n += Xb.size(0)\n",
        "\n",
        "          val_free = val_free_total / max(1, val_n)\n",
        "\n",
        "          # UPDATE LEARNING RATE\n",
        "          sched.step()\n",
        "\n",
        "          # PRINT PROGRESS\n",
        "          print(f\"Epoch {ep:02d} | p_tf={p_tf:.2f} | \"\n",
        "                f\"train={train_loss:.4f} | val_tf={val_tf:.4f} | val_free={val_free:.4f} | \"\n",
        "                f\"lr={sched.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "          # RECORD HISTORY\n",
        "          history[\"train\"].append(train_loss)\n",
        "          history[\"val_tf\"].append(val_tf)\n",
        "          history[\"val_free\"].append(val_free)\n",
        "\n",
        "      print(\"\\n\" + \"=\" * 70)\n",
        "      print(\"TRAINING COMPLETE\")\n",
        "      print(\"=\" * 70)\n",
        "      print(f\"Final train loss: {history['train'][-1]:.4f}\")\n",
        "      print(f\"Final val_tf loss: {history['val_tf'][-1]:.4f}\")\n",
        "      print(f\"Final val_free loss: {history['val_free'][-1]:.4f}\")\n",
        "      print(f\"Exposure bias gap: {history['val_free'][-1] - history['val_tf'][-1]:.4f}\")\n",
        "      print(\"(Gap = how much worse free-running is than teacher-forced)\\n\")\n",
        "\n",
        "      return history\n",
        "    def predict_evaluation(self, test_loader):\n",
        "\n",
        "      DEVICE = next(self.parameters()).device\n",
        "      self.eval()  # Set to evaluation mode\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # Get one batch from validation set\n",
        "          Xb_cpu, _, ytrue_cpu = next(iter(test_loader))\n",
        "\n",
        "          # Use only first sample for visualization\n",
        "          Xb = Xb_cpu[:1].to(DEVICE).float()      # [1, T_x, features]\n",
        "          ytrue = ytrue_cpu[:1].to(DEVICE).float()  # [1, T_y, 1]\n",
        "\n",
        "          # Generate forecast (autoregressive)\n",
        "          y_start = ytrue[:, :1, :]  # Seed with first ground-truth value\n",
        "          pred = self.generate(Xb, y_start=y_start, steps=T_Y).float()\n",
        "\n",
        "      # Convert to numpy for plotting\n",
        "      gt = ytrue[0, :, 0].detach().cpu().numpy()  # Ground truth\n",
        "      pr = pred[0, :, 0].detach().cpu().numpy()   # Prediction\n",
        "\n",
        "      # Compute error metrics\n",
        "      mae = np.mean(np.abs(pr - gt))              # Mean Absolute Error\n",
        "      rmse = np.sqrt(np.mean((pr - gt) ** 2))     # Root Mean Squared Error\n",
        "\n",
        "      return mae, rmse"
      ],
      "metadata": {
        "id": "npUHrHcLcsxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chamada"
      ],
      "metadata": {
        "id": "oaXuKTaNgR8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TSTConfig:\n",
        "    \"\"\"\n",
        "    Configuration for Time Series Transformer.\n",
        "\n",
        "    Architecture hyperparameters:\n",
        "        d_model: Hidden dimension throughout the model (like \"brain size\")\n",
        "        num_encoder_layers: Depth of encoder (more = more context integration)\n",
        "        num_decoder_layers: Depth of decoder (fewer than encoder is common)\n",
        "        num_heads: Parallel attention operations (more = capture more relationships)\n",
        "        d_ff: Feedforward expansion dimension (typically 4*d_model)\n",
        "        dropout: Regularization strength (0.0 = none, 0.3 = aggressive)\n",
        "\n",
        "    Input/output specification:\n",
        "        input_dim: Number of input features (e.g., temperature, pressure, etc.)\n",
        "        target_dim: Number of values to predict (often 1 for univariate forecasting)\n",
        "        dec_input_dim: Decoder input size (should match target_dim for simplicity)\n",
        "\n",
        "    Other settings:\n",
        "        pos_encoding: \"sin\" (fixed) or \"learned\" (trainable)\n",
        "        max_len: Maximum sequence length\n",
        "        output_activation: Optional final activation (\"tanh\", \"sigmoid\", or None)\n",
        "    \"\"\"\n",
        "    d_model: int = 128\n",
        "    num_encoder_layers: int = 3\n",
        "    num_decoder_layers: int = 2\n",
        "    num_heads: int = 8\n",
        "    d_ff: int = 256\n",
        "    dropout: float = 0.1\n",
        "    pos_encoding: str = \"sin\"  # \"sin\" or \"learned\"\n",
        "    max_len: int = 4096\n",
        "    factor: int = 5\n",
        "    input_dim: int = 2         # number of historical features\n",
        "    target_dim: int = 1        # we forecast a single target value\n",
        "    dec_input_dim: int = 1     # decoder input size (shifted target)\n",
        "    output_activation: Optional[str] = None  # e.g., \"tanh\" for bounded target\n",
        "\n",
        "# --- MODEL + OPTIMIZER ---\n",
        "cfg = TSTConfig()\n",
        "model_Probsparse = TimeSeriesTransformerProbSparse(cfg).to(DEVICE)\n",
        "history = model_Probsparse.train_transformer(train_loader, val_loader)"
      ],
      "metadata": {
        "id": "mF_T6kD0gRyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e666b67-ea41-45d0-c521-b4e3ce878cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 796,289\n",
            "Device: cuda\n",
            "Mixed precision: BFloat16\n",
            "\n",
            "======================================================================\n",
            "TRAINING\n",
            "======================================================================\n",
            "Epoch 01 | p_tf=0.90 | train=0.3353 | val_tf=0.1517 | val_free=0.1636 | lr=9.62e-04\n",
            "Epoch 02 | p_tf=0.84 | train=0.0369 | val_tf=0.1811 | val_free=0.0396 | lr=8.54e-04\n",
            "Epoch 03 | p_tf=0.79 | train=0.0178 | val_tf=0.1602 | val_free=0.0649 | lr=6.91e-04\n",
            "Epoch 04 | p_tf=0.73 | train=0.0178 | val_tf=0.1658 | val_free=0.0587 | lr=5.00e-04\n",
            "Epoch 05 | p_tf=0.67 | train=0.0168 | val_tf=0.1562 | val_free=0.0439 | lr=3.09e-04\n",
            "Epoch 06 | p_tf=0.61 | train=0.0164 | val_tf=0.1547 | val_free=0.0681 | lr=1.46e-04\n",
            "Epoch 07 | p_tf=0.56 | train=0.0177 | val_tf=0.1494 | val_free=0.0339 | lr=3.81e-05\n",
            "Epoch 08 | p_tf=0.50 | train=0.0172 | val_tf=0.1497 | val_free=0.0351 | lr=0.00e+00\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE\n",
            "======================================================================\n",
            "Final train loss: 0.0172\n",
            "Final val_tf loss: 0.1497\n",
            "Final val_free loss: 0.0351\n",
            "Exposure bias gap: -0.1145\n",
            "(Gap = how much worse free-running is than teacher-forced)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mae, rmse = model_Probsparse.predict_evaluation(test_loader)\n",
        "\n",
        "print(\"\\nForecast analysis:\")\n",
        "print(f\"  - MAE (Mean Absolute Error): {mae:.4f}\")\n",
        "print(f\"  - RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
        "print(f\"  - Forecast horizon: {T_Y} steps\")\n",
        "print(f\"  - History used: {T_X} steps\")"
      ],
      "metadata": {
        "id": "2uRqDyipgaOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c223486-30cf-423c-b37c-844f86a59851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forecast analysis:\n",
            "  - MAE (Mean Absolute Error): 0.0807\n",
            "  - RMSE (Root Mean Squared Error): 0.1003\n",
            "  - Forecast horizon: 8 steps\n",
            "  - History used: 32 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Comparação"
      ],
      "metadata": {
        "id": "ZkCsOky22bSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_list = [model_lstm, model_transformer, model_fft, model_slideWin, model_Probsparse]\n",
        "model_names = [\"LSTM\", \"Transformer(mha)\", \"Transf. + FFT\", \"Sliding Window\", \"ProbSparse\"]\n",
        "\n",
        "\n",
        "print(f'Model           |   MAE  |  RMSE')\n",
        "print(f'----------------|--------|--------')\n",
        "for model, name in zip(model_list, model_names):\n",
        "    mae, rmse = model.predict_evaluation(test_loader)\n",
        "    print(f\"{name.ljust(16)}| {mae:.4f} | {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BopAnULE2bJb",
        "outputId": "5aa1da50-c542-4c18-b8f5-d7ef4111db99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model           |   MAE  |  RMSE\n",
            "----------------|--------|--------\n",
            "LSTM            | 0.3828 | 0.3884\n",
            "Transformer(mha)| 0.0895 | 0.1097\n",
            "Transf. + FFT   | 0.1055 | 0.1246\n",
            "Sliding Window  | 0.0890 | 0.1069\n",
            "ProbSparse      | 0.0807 | 0.1003\n"
          ]
        }
      ]
    }
  ]
}