{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88961296",
   "metadata": {},
   "source": [
    "# Trabalho 2: Implementação de Árvore de Decisão + Floresta Aleatória\n",
    "\n",
    "Disciplina: Algoritmos Baseados em Dados para Problemas de Ciência e Engenharia\n",
    "\n",
    "Estudante: Carlos Monteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e5dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import random\n",
    "from sklearn.linear_model import ridge_regression\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import torch as tc\n",
    "\n",
    "import bisect\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b7bff",
   "metadata": {},
   "source": [
    "# 1. Geração de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0315fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_amostras = 5000\n",
    "\n",
    "dados = pd.DataFrame()\n",
    "dados['x1'] = 6 + tc.randn(num_amostras)\n",
    "dados['x2'] = [1.5*np.sin(0.002*idx) + n for idx, n in enumerate(dados['x1'])]\n",
    "dados['x3'] = [0.001*idx+n for idx, n in enumerate(dados['x1'])]\n",
    "dados['x4'] = [abs(n*np.sin(0.0025*idx))  for idx, n in enumerate(dados['x1'])]\n",
    "dados['x5'] = [n + float(tc.randn(1)) for idx, n in enumerate(dados['x1'])]\n",
    "dados['x6'] = [0.0003*idx + 0.8*np.sin(0.0008*idx) + n + float(tc.randn(1)) for idx, n in enumerate(dados['x1'])]\n",
    "dados['x7'] = tc.rand(num_amostras)\n",
    "dados['x8'] = [-np.log(n) for idx, n in enumerate(dados['x7'])]\n",
    "#dados['y'] = [10 + n*np.sin(0.0025*idx) + dados['x7'][idx]*np.sin(0.001*idx) + float(tc.randn(1)) for idx, n in enumerate(dados['x1'])]\n",
    "dados['y'] = [n + dados['x2'][idx] + dados['x3'][idx] + dados['x4'][idx] + dados['x5'][idx] + dados['x6'][idx] + dados['x7'][idx] + dados['x8'][idx] for idx, n in enumerate(dados['x1'])]\n",
    "\n",
    "df = dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c72318",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=['y']), df['y'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2723190",
   "metadata": {},
   "source": [
    "# 2. Implementação do algoritmo de Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54b2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    count=0\n",
    "    def __init__(self, left, right, data=None, w=None, \n",
    "                 b=None, criterion_value=None, _result=None):\n",
    "        self.id=Node.count\n",
    "        Node.count += 1\n",
    "    \n",
    "        self.left: Node = left\n",
    "        self.right: Node = right\n",
    "\n",
    "        self.data = data\n",
    "        self.w: int = w\n",
    "        self.b: float = b\n",
    "        self.criterion_value: float = criterion_value\n",
    "        self.n_sample: int = len(data) if data is not None else 0\n",
    "        self._result = _result\n",
    "\n",
    "    def predict(self, x):\n",
    "        z = x @ self.w - self.b\n",
    "        if z < 0:\n",
    "            return self.left.predict(x)\n",
    "        \n",
    "        elif z >= 0:\n",
    "            return self.right.predict(x)\n",
    "\n",
    "class LeafNode(Node):\n",
    "    def __init__(self, data, criterion_value, _result):\n",
    "        super().__init__(None, None, data=data, \n",
    "                         criterion_value=criterion_value, \n",
    "                         _result=_result)\n",
    "\n",
    "    def predict(self, X=None):\n",
    "        return self._result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1ac594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_to_split=4, min_samples_leaf=2,\n",
    "                 n_feature_split=2, n_feature_comb_split=10, min_cost_decrease=0.1,\n",
    "                 prob_oblique_split=0.7, hist=True):\n",
    "        self.root: Node = Node(None, None)\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_to_split = min_samples_to_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.n_feature_split = n_feature_split\n",
    "        self.n_feature_comb_split = n_feature_comb_split\n",
    "        self.min_cost_decrease = min_cost_decrease\n",
    "        self.prob_oblique_split = prob_oblique_split\n",
    "        self.hist = hist\n",
    "\n",
    "        \n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.n_sample, self.n_feature = X.shape\n",
    "\n",
    "        if self.hist:\n",
    "            #Converte y em bins usando a regra de Freedman-Diaconis\n",
    "            bins = int((np.max(y) - np.min(y)) / (2 * (np.percentile(y, 75) - np.percentile(y, 25)) * len(y)**(-1/3)))\n",
    "            edge = np.linspace(np.min(y), np.max(y), bins)\n",
    "            y_bins = []\n",
    "            for yi in y:\n",
    "                bin = bisect.bisect_left(edge, yi)\n",
    "                y_bins.append(edge[bin-1] + (edge[bin] - edge[bin-1]) / 2)\n",
    "            y = y_bins       \n",
    "\n",
    "        #Gera todas as combinações de features\n",
    "        if self.n_feature > self.n_feature_split:\n",
    "            self.feature_combination = list(combinations(np.arange(self.n_feature), self.n_feature_split))\n",
    "        else:\n",
    "            self.feature_combination = [np.arange(self.n_feature + 1)]\n",
    "\n",
    "        self.root = self._grow(np.hstack((X.to_numpy(), np.array(y).reshape(-1,1))))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X.to_numpy():\n",
    "            y_pred.append(self.root.predict(x))\n",
    "        return y_pred\n",
    "\n",
    "   \n",
    "    def _split(self, feature_data, labels):\n",
    "    \n",
    "        #Binariza as labels\n",
    "        if len(np.unique(labels)) == 2:\n",
    "            binary_labels = labels\n",
    "        else: \n",
    "            unique, counts = np.unique(labels, return_counts=True)\n",
    "            most_frequent_class = unique[np.argmax(counts)]\n",
    "            binary_labels = (labels == most_frequent_class).astype(int)\n",
    "\n",
    "        binary_labels = (labels >= np.median(labels)).astype(int)\n",
    "\n",
    "        if feature_data.ndim != 1:\n",
    "\n",
    "            #Aplica regressão ridge para encontrar o hiperplano de separação\n",
    "            if (Counter(binary_labels)[0] > self.min_samples_leaf and \n",
    "                Counter(binary_labels)[1] > self.min_samples_leaf):\n",
    "                w = ridge_regression(feature_data, binary_labels, alpha=0.9)\n",
    "            else:\n",
    "                return None, None, np.inf\n",
    "            \n",
    "            #Soft Theresholding\n",
    "            th = 0.1*np.max(np.abs(w))\n",
    "            for i, weight in enumerate(w):\n",
    "                if weight > th:\n",
    "                    w[i] = weight - th\n",
    "                elif weight < -th:\n",
    "                    w[i] = weight + th\n",
    "                else:\n",
    "                    w[i] = 0.0\n",
    "\n",
    "            #Define o b\n",
    "            z = feature_data @ w\n",
    "           \n",
    "        else:\n",
    "            w = np.array([1.0])\n",
    "            z = feature_data\n",
    "\n",
    "        \n",
    "\n",
    "        sorted_z_idx = np.argsort(z)\n",
    "        sorted_z = z[sorted_z_idx]\n",
    "        sorted_y = labels[sorted_z_idx]\n",
    "\n",
    "        best_b = None\n",
    "        best_cost = np.inf\n",
    "\n",
    "        for i in range(1, len(sorted_y)):\n",
    "            if sorted_z[i] != sorted_z[i-1]:\n",
    "                b_candidate = (sorted_z[i] + sorted_z[i-1]) / 2.0\n",
    "\n",
    "                left_labels = labels[z > b_candidate]\n",
    "                right_labels = labels[z <= b_candidate]\n",
    "\n",
    "                m_left, m_right = len(left_labels), len(right_labels)\n",
    "                m = m_left + m_right\n",
    "\n",
    "                s_left = np.sum((left_labels - np.mean(left_labels))**2)/m_left\n",
    "                s_right = np.sum((right_labels - np.mean(right_labels))**2)/m_right\n",
    "\n",
    "                cost_value = s_left*(m_left/m) + s_right*(m_right/m)\n",
    "\n",
    "                if cost_value < best_cost:\n",
    "                    best_cost = cost_value\n",
    "                    best_b = b_candidate\n",
    "        \n",
    "       \n",
    "        return w, best_b, best_cost\n",
    "    \n",
    "    \n",
    "    def _best_feature(self, data):\n",
    "\n",
    "        min_feature_cost = np.inf\n",
    "        min_w = None\n",
    "        min_b = None\n",
    "        selected_feature = []\n",
    "\n",
    "              \n",
    "        if random.uniform(0, 1) > self.prob_oblique_split:\n",
    "            sampled_feature_list = random.sample(self.feature_combination, \n",
    "                                             self.n_feature_comb_split)\n",
    "        else:\n",
    "            sampled_feature_list = np.arange(self.n_feature)\n",
    "\n",
    "        for feature_idxs in sampled_feature_list:\n",
    "\n",
    "            w, b, split_cost = self._split(data[:, feature_idxs], data[:, -1])\n",
    "\n",
    "            if split_cost < min_feature_cost:\n",
    "                min_feature_cost = split_cost\n",
    "                min_w = w\n",
    "                min_b = b\n",
    "                selected_feature = feature_idxs\n",
    "\n",
    "        if min_feature_cost != np.inf:\n",
    "            w_out = np.zeros(self.n_feature)\n",
    "            if isinstance(selected_feature, tuple):\n",
    "                for idx, f_idx in enumerate(selected_feature):\n",
    "                    w_out[f_idx] = min_w[idx]\n",
    "            else:\n",
    "                w_out[selected_feature] = 1\n",
    "        else:\n",
    "            w_out = None\n",
    "\n",
    "\n",
    "        return w_out, min_b, min_feature_cost\n",
    "        \n",
    "\n",
    "    def _grow(self, data, depth=1):\n",
    "\n",
    "        y = data[:, -1]\n",
    "\n",
    "        criterion_value = np.sum((y - np.mean(y))**2)\n",
    "        result = np.mean(y)\n",
    "\n",
    "        if ((depth >= self.max_depth) or\n",
    "            (criterion_value < np.finfo('float32').eps)):\n",
    "            return LeafNode(data, criterion_value=criterion_value, \n",
    "                            _result = result)\n",
    "       \n",
    "        w_out, min_b, min_feature_cost = self._best_feature(data)\n",
    "\n",
    "        if min_feature_cost == np.inf:\n",
    "            return LeafNode(data, criterion_value=criterion_value, \n",
    "                            _result=result)\n",
    "        \n",
    "        z = np.dot(data[:, :-1], w_out) - min_b\n",
    "    \n",
    "        left_data = data[z < 0]\n",
    "        right_data = data[z >= 0]\n",
    "\n",
    "        if ((len(left_data) < self.min_samples_leaf) or\n",
    "            (len(right_data) < self.min_samples_leaf) or\n",
    "            ((criterion_value/len(y)) - min_feature_cost <= self.min_cost_decrease)):           \n",
    "            return LeafNode(data, criterion_value=criterion_value, \n",
    "                            _result=result)\n",
    "        \n",
    "        if len(left_data) < self.min_samples_to_split:\n",
    "            left_y = left_data[:, -1]\n",
    "\n",
    "            left_criterion_value = np.sum((left_y - np.mean(left_y))**2)\n",
    "            left_result = np.mean(left_y)\n",
    "\n",
    "            left_node = LeafNode(left_data, criterion_value=left_criterion_value, \n",
    "                                 _result = left_result)\n",
    "        else:\n",
    "            left_node = self._grow(left_data, depth=depth+1)\n",
    "\n",
    "\n",
    "        if len(right_data) < self.min_samples_to_split:\n",
    "            right_y = right_data[:, -1]\n",
    "\n",
    "            right_criterion_value = np.sum((right_y - np.mean(right_y))**2)\n",
    "            right_result = np.mean(right_y)\n",
    "\n",
    "            right_node = LeafNode(right_data, criterion_value=right_criterion_value, \n",
    "                                 _result = right_result)\n",
    "        else:\n",
    "            right_node = self._grow(right_data, depth=depth+1)\n",
    "\n",
    "        return Node(left_node, \n",
    "                    right_node,\n",
    "                    data, \n",
    "                    w_out, \n",
    "                    min_b, \n",
    "                    criterion_value = criterion_value,\n",
    "                    _result=result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed81a9",
   "metadata": {},
   "source": [
    "# 3. Implementação do algoritmo de Floresta Aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a518ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=5, min_samples_to_split=4, \n",
    "                 min_samples_leaf=2, n_feature_split=2, n_feature_comb_split=10,\n",
    "                 min_cost_decrease=0.1, prob_oblique_split=0.7, hist=True,\n",
    "                 bootstrap=True):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.trees = []\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_to_split = min_samples_to_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.n_feature_split = n_feature_split\n",
    "        self.n_feature_comb_split = n_feature_comb_split\n",
    "        self.min_cost_decrease = min_cost_decrease\n",
    "        self.prob_oblique_split = prob_oblique_split\n",
    "        self.hist = hist\n",
    "        self.bootstrap = bootstrap\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.trees = []\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            if self.bootstrap:\n",
    "                sample_indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "                X_sample = X.iloc[sample_indices]\n",
    "                y_sample = y.iloc[sample_indices]\n",
    "            else:\n",
    "                X_sample = X\n",
    "                y_sample = y\n",
    "\n",
    "            tree = DecisionTree(max_depth=self.max_depth,\n",
    "                                min_samples_to_split=self.min_samples_to_split,\n",
    "                                min_samples_leaf=self.min_samples_leaf,\n",
    "                                n_feature_split=self.n_feature_split,\n",
    "                                n_feature_comb_split=self.n_feature_comb_split,\n",
    "                                min_cost_decrease=self.min_cost_decrease,\n",
    "                                prob_oblique_split=self.prob_oblique_split,\n",
    "                                hist=self.hist)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(tree_preds, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7edf5",
   "metadata": {},
   "source": [
    "# 4. Comparação dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b3c7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_depth': 100,\n",
    "    'min_samples_to_split': 10,\n",
    "    'min_samples_leaf': 3,\n",
    "    'n_feature_split': 2,\n",
    "    'n_feature_comb_split': 10,\n",
    "    'min_cost_decrease': 0.1,\n",
    "    'prob_oblique_split': 0.7,\n",
    "    'hist': True\n",
    "}\n",
    "\n",
    "model_list = [DecisionTree(max_depth=parameters['max_depth'], min_samples_to_split=parameters['min_samples_to_split'], min_samples_leaf=parameters['min_samples_leaf'], \n",
    "                        n_feature_split=parameters['n_feature_split'], n_feature_comb_split=parameters['n_feature_comb_split'], min_cost_decrease=parameters['min_cost_decrease'],\n",
    "                        prob_oblique_split=parameters['prob_oblique_split'], hist=parameters['hist']),\n",
    "              DecisionTreeRegressor(),\n",
    "              RandomForest(n_estimators=100, max_depth=parameters['max_depth'], min_samples_to_split=parameters['min_samples_to_split'], \n",
    "                           min_samples_leaf=parameters['min_samples_leaf'], n_feature_split=parameters['n_feature_split'], n_feature_comb_split=parameters['n_feature_comb_split'],\n",
    "                           min_cost_decrease=parameters['min_cost_decrease'],prob_oblique_split=parameters['prob_oblique_split'], hist=parameters['hist'],\n",
    "                           bootstrap=True),\n",
    "              RandomForestRegressor()]\n",
    "\n",
    "model_name_list = ['Decision Tree', \n",
    "                   'Sklearn Decision Tree', \n",
    "                   'Random Forest',\n",
    "                   'Sklearn Random Forest']\n",
    "\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "for model in model_list:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse_list.append(mean_squared_error(y_test, y_pred))\n",
    "    mae_list.append(mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d724b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo                 |     MSE    |   MAE   \n",
      "-----------------------|------------|----------\n",
      "Decision Tree          |     2.4785 |   1.2291 \n",
      "Sklearn Decision Tree  |     2.6059 |   1.2418 \n",
      "Random Forest          |     0.7399 |   0.6133 \n",
      "Sklearn Random Forest  |     0.7923 |   0.6449 \n"
     ]
    }
   ],
   "source": [
    "print(f'Modelo                 |     MSE    |   MAE   ')\n",
    "print(f'-----------------------|------------|----------')\n",
    "for i, model_name in enumerate(model_name_list):\n",
    "    print(f'{model_name:22s} | {mse_list[i]:10.4f} | {mae_list[i]:8.4f} ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
